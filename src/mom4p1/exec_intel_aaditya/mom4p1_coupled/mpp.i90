!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!                                                                   !!
!!                   GNU General Public License                      !!
!!                                                                   !!
!! This file is part of the Flexible Modeling System (FMS).          !!
!!                                                                   !!
!! FMS is free software; you can redistribute it and/or modify       !!
!! it and are expected to follow the terms of the GNU General Public !!
!! License as published by the Free Software Foundation.             !!
!!                                                                   !!
!! FMS is distributed in the hope that it will be useful,            !!
!! but WITHOUT ANY WARRANTY; without even the implied warranty of    !!
!! MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the     !!
!! GNU General Public License for more details.                      !!
!!                                                                   !!
!! You should have received a copy of the GNU General Public License !!
!! along with FMS; if not, write to:                                 !!
!!          Free Software Foundation, Inc.                           !!
!!          59 Temple Place, Suite 330                               !!
!!          Boston, MA  02111-1307  USA                              !!
!! or see:                                                           !!
!!          http://www.gnu.org/licenses/gpl.txt                      !!
!!                                                                   !!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!-----------------------------------------------------------------------
!                 Communication for message-passing codes
!
! AUTHOR: V. Balaji (V.Balaji@noaa.gov)
!         SGI/GFDL Princeton University
!
! This program is free software; you can redistribute it and/or modify
! it under the terms of the GNU General Public License as published by
! the Free Software Foundation; either version 2 of the License, or
! (at your option) any later version.
!
! This program is distributed in the hope that it will be useful,
! but WITHOUT ANY WARRANTY; without even the implied warranty of
! MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
! GNU General Public License for more details.
!
! For the full text of the GNU General Public License,
! write to: Free Software Foundation, Inc.,
!           675 Mass Ave, Cambridge, MA 02139, USA.
!-----------------------------------------------------------------------
module mpp_mod
!a generalized communication package for use with shmem and MPI
!will add: co_array_fortran, MPI2
!Balaji (V.Balaji@noaa.gov) 11 May 1998

! <CONTACT EMAIL="V.Balaji@noaa.gov">
!   V. Balaji
! </CONTACT>

! <HISTORY SRC="http://www.gfdl.noaa.gov/fms-cgi-bin/cvsweb.cgi/FMS/"/>
! <RCSLOG SRC="http://www.gfdl.noaa.gov/~vb/changes_mpp.html"/>

! <OVERVIEW>
!   <TT>mpp_mod</TT>, is a set of simple calls to provide a uniform interface
!   to different message-passing libraries. It currently can be
!   implemented either in the SGI/Cray native SHMEM library or in the MPI
!   standard. Other libraries (e.g MPI-2, Co-Array Fortran) can be
!   incorporated as the need arises.
! </OVERVIEW>

! <DESCRIPTION>
!   The data transfer between a processor and its own memory is based
!   on <TT>load</TT> and <TT>store</TT> operations upon
!   memory. Shared-memory systems (including distributed shared memory
!   systems) have a single address space and any processor can acquire any
!   data within the memory by <TT>load</TT> and
!   <TT>store</TT>. The situation is different for distributed
!   parallel systems. Specialized MPP systems such as the T3E can simulate
!   shared-memory by direct data acquisition from remote memory. But if
!   the parallel code is distributed across a cluster, or across the Net,
!   messages must be sent and received using the protocols for
!   long-distance communication, such as TCP/IP. This requires a
!   ``handshaking'' between nodes of the distributed system. One can think
!   of the two different methods as involving <TT>put</TT>s or
!   <TT>get</TT>s (e.g the SHMEM library), or in the case of
!   negotiated communication (e.g MPI), <TT>send</TT>s and
!   <TT>recv</TT>s.
!
!   The difference between SHMEM and MPI is that SHMEM uses one-sided
!   communication, which can have very low-latency high-bandwidth
!   implementations on tightly coupled systems. MPI is a standard
!   developed for distributed computing across loosely-coupled systems,
!   and therefore incurs a software penalty for negotiating the
!   communication. It is however an open industry standard whereas SHMEM
!   is a proprietary interface. Besides, the <TT>put</TT>s or
!   <TT>get</TT>s on which it is based cannot currently be implemented in
!   a cluster environment (there are recent announcements from Compaq that
!   occasion hope).
!
!   The message-passing requirements of climate and weather codes can be
!   reduced to a fairly simple minimal set, which is easily implemented in
!   any message-passing API. <TT>mpp_mod</TT> provides this API.
!
!    Features of <TT>mpp_mod</TT> include:
!
!    1) Simple, minimal API, with free access to underlying API for
!       more complicated stuff.<BR/>
!    2) Design toward typical use in climate/weather CFD codes.<BR/>
!    3) Performance to be not significantly lower than any native API.
!
!   This module is used to develop higher-level calls for <LINK
!   SRC="mpp_domains.html">domain decomposition</LINK> and <LINK
!   SRC="mpp_io.html">parallel I/O</LINK>.
!
!   Parallel computing is initially daunting, but it soon becomes
!   second nature, much the way many of us can now write vector code
!   without much effort. The key insight required while reading and
!   writing parallel code is in arriving at a mental grasp of several
!   independent parallel execution streams through the same code (the SPMD
!   model). Each variable you examine may have different values for each
!   stream, the processor ID being an obvious example. Subroutines and
!   function calls are particularly subtle, since it is not always obvious
!   from looking at a call what synchronization between execution streams
!   it implies. An example of erroneous code would be a global barrier
!   call (see <LINK SRC="#mpp_sync">mpp_sync</LINK> below) placed
!   within a code block that not all PEs will execute, e.g:
!
!   <PRE>
!   if( pe.EQ.0 )call mpp_sync()
!   </PRE>
!
!   Here only PE 0 reaches the barrier, where it will wait
!   indefinitely. While this is a particularly egregious example to
!   illustrate the coding flaw, more subtle versions of the same are
!   among the most common errors in parallel code.
!
!   It is therefore important to be conscious of the context of a
!   subroutine or function call, and the implied synchronization. There
!   are certain calls here (e.g <TT>mpp_declare_pelist, mpp_init,
!   mpp_malloc, mpp_set_stack_size</TT>) which must be called by all
!   PEs. There are others which must be called by a subset of PEs (here
!   called a <TT>pelist</TT>) which must be called by all the PEs in the
!   <TT>pelist</TT> (e.g <TT>mpp_max, mpp_sum, mpp_sync</TT>). Still
!   others imply no synchronization at all. I will make every effort to
!   highlight the context of each call in the MPP modules, so that the
!   implicit synchronization is spelt out.
!
!   For performance it is necessary to keep synchronization as limited
!   as the algorithm being implemented will allow. For instance, a single
!   message between two PEs should only imply synchronization across the
!   PEs in question. A <I>global</I> synchronization (or <I>barrier</I>)
!   is likely to be slow, and is best avoided. But codes first
!   parallelized on a Cray T3E tend to have many global syncs, as very
!   fast barriers were implemented there in hardware.
!
!   Another reason to use pelists is to run a single program in MPMD
!   mode, where different PE subsets work on different portions of the
!   code. A typical example is to assign an ocean model and atmosphere
!   model to different PE subsets, and couple them concurrently instead of
!   running them serially. The MPP module provides the notion of a
!   <I>current pelist</I>, which is set when a group of PEs branch off
!   into a subset. Subsequent calls that omit the <TT>pelist</TT> optional
!   argument (seen below in many of the individual calls) assume that the
!   implied synchronization is across the current pelist. The calls
!   <TT>mpp_root_pe</TT> and <TT>mpp_npes</TT> also return the values
!   appropriate to the current pelist. The <TT>mpp_set_current_pelist</TT>
!   call is provided to set the current pelist.

! </DESCRIPTION>
! <PUBLIC>
!  F90 is a strictly-typed language, and the syntax pass of the
!  compiler requires matching of type, kind and rank (TKR). Most calls
!  listed here use a generic type, shown here as <TT>MPP_TYPE_</TT>. This
!  is resolved in the pre-processor stage to any of a variety of
!  types. In general the MPP operations work on 4-byte and 8-byte
!  variants of <TT>integer, real, complex, logical</TT> variables, of
!  rank 0 to 5, leading to 48 specific module procedures under the same
!  generic interface. Any of the variables below shown as
!  <TT>MPP_TYPE_</TT> is treated in this way.
! </PUBLIC>




! -*-f90-*-*
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!                                                                   !!
!!                   GNU General Public License                      !!
!!                                                                   !!
!! This file is part of the Flexible Modeling System (FMS).          !!
!!                                                                   !!
!! FMS is free software; you can redistribute it and/or modify       !!
!! it and are expected to follow the terms of the GNU General Public !!
!! License as published by the Free Software Foundation.             !!
!!                                                                   !!
!! FMS is distributed in the hope that it will be useful,            !!
!! but WITHOUT ANY WARRANTY; without even the implied warranty of    !!
!! MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the     !!
!! GNU General Public License for more details.                      !!
!!                                                                   !!
!! You should have received a copy of the GNU General Public License !!
!! along with FMS; if not, write to:                                 !!
!!          Free Software Foundation, Inc.                           !!
!!          59 Temple Place, Suite 330                               !!
!!          Boston, MA  02111-1307  USA                              !!
!! or see:                                                           !!
!!          http://www.gnu.org/licenses/gpl.txt                      !!
!!                                                                   !!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!




!parallel machine types




!most compilers support Cray pointers
!if you find a compiler that doesn't, #undef this inside a suitable #ifdef


!values of kind: double and long are 8-byte, float and int are 4-byte
!pointer_kind is used for storing addresses as integers

!these might be different on non-SGICRAY, I believe
! Warning: these numbers may not map to byte sizes for all compilers







!DEC$ MESSAGE:'Using 8-byte addressing'



















!DEC$ MESSAGE:'Using PURE'




!DEC$ MESSAGE:'Converting pointers to allocatable components'







! 






  use mpp_parameter_mod, only : MPP_VERBOSE, MPP_DEBUG, ALL_PES, ANY_PE, NULL_PE
  use mpp_parameter_mod, only : NOTE, WARNING, FATAL, MPP_CLOCK_DETAILED,MPP_CLOCK_SYNC
  use mpp_parameter_mod, only : CLOCK_COMPONENT, CLOCK_SUBCOMPONENT, CLOCK_MODULE_DRIVER
  use mpp_parameter_mod, only : CLOCK_MODULE, CLOCK_ROUTINE, CLOCK_LOOP, CLOCK_INFRA
  use mpp_parameter_mod, only : MAX_EVENTS, MAX_BINS, MAX_EVENT_TYPES, PESET_MAX, MAX_CLOCKS
  use mpp_parameter_mod, only : MAXPES, EVENT_WAIT, EVENT_ALLREDUCE, EVENT_BROADCAST
  use mpp_parameter_mod, only : EVENT_RECV, EVENT_SEND, MPP_READY, MPP_WAIT
  use mpp_parameter_mod, only : mpp_parameter_version=>version, mpp_parameter_tagname=>tagname
  use mpp_data_mod,      only : stat, mpp_stack, ptr_stack, status, ptr_status, sync, ptr_sync  
  use mpp_data_mod,      only : mpp_from_pe, ptr_from, remote_data_loc, ptr_remote
  use mpp_data_mod,      only : mpp_data_version=>version, mpp_data_tagname=>tagname

implicit none
private




!
! Copyright (C) 2003-2014 Intel Corporation.  All Rights Reserved.
!
! The source code contained or described herein and all documents
! related to the source code ("Material") are owned by Intel Corporation
! or its suppliers or licensors.  Title to the Material remains with
! Intel Corporation or its suppliers and licensors.  The Material is
! protected by worldwide copyright and trade secret laws and treaty
! provisions.  No part of the Material may be used, copied, reproduced,
! modified, published, uploaded, posted, transmitted, distributed, or
! disclosed in any way without Intel's prior express written permission.
!
! No license under any patent, copyright, trade secret or other
! intellectual property right is granted to or conferred upon you by
! disclosure or delivery of the Materials, either expressly, by
! implication, inducement, estoppel or otherwise.  Any license under
! such intellectual property rights must be express and approved by
! Intel in writing.
!      /* -*- Mode: Fortran; -*- */
!
!      (C) 2001 by Argonne National Laboratory.
!
! 				  MPICH2 COPYRIGHT
!
! The following is a notice of limited availability of the code, and disclaimer
! which must be included in the prologue of the code and in all source listings
! of the code.
!
! Copyright Notice
!  + 2002 University of Chicago
!
! Permission is hereby granted to use, reproduce, prepare derivative works, and
! to redistribute to others.  This software was authored by:
!
! Mathematics and Computer Science Division
! Argonne National Laboratory, Argonne IL 60439
!
! (and)
!
! Department of Computer Science
! University of Illinois at Urbana-Champaign
!
!
! 			      GOVERNMENT LICENSE
!
! Portions of this material resulted from work developed under a U.S.
! Government Contract and are subject to the following license: the Government
! is granted for itself and others acting on its behalf a paid-up, nonexclusive,
! irrevocable worldwide license in this computer software to reproduce, prepare
! derivative works, and perform publicly and display publicly.
!
! 				  DISCLAIMER
!
! This computer code material was prepared, in part, as an account of work
! sponsored by an agency of the United States Government.  Neither the United
! States, nor the University of Chicago, nor any of their employees, makes any
! warranty express or implied, or assumes any legal liability or responsibility
! for the accuracy, completeness, or usefulness of any information, apparatus,
! product, or process disclosed, or represents that its use would not infringe
! privately owned rights.
!
! Portions of this code were written by Microsoft. Those portions are
! Copyright (c) 2007 Microsoft Corporation. Microsoft grants permission to
! use, reproduce, prepare derivative works, and to redistribute to
! others. The code is licensed "as is." The User bears the risk of using
! it. Microsoft gives no express warranties, guarantees or
! conditions. To the extent permitted by law, Microsoft excludes the
! implied warranties of merchantability, fitness for a particular
! purpose and non-infringement.
!
!
!
!
!
!      DO NOT EDIT
!      This file created by buildiface
!
       INTEGER MPI_SOURCE, MPI_TAG, MPI_ERROR
       PARAMETER (MPI_SOURCE=3,MPI_TAG=4,MPI_ERROR=5)
       INTEGER MPI_STATUS_SIZE
       PARAMETER (MPI_STATUS_SIZE=5)
       INTEGER*4 MPI_STATUS_IGNORE(MPI_STATUS_SIZE)
       INTEGER*4 MPI_STATUSES_IGNORE(MPI_STATUS_SIZE,1)
       INTEGER*4 MPI_ERRCODES_IGNORE(1)
       CHARACTER*1 MPI_ARGVS_NULL(1,1)
       CHARACTER*1 MPI_ARGV_NULL(1)
       INTEGER MPI_SUCCESS
       PARAMETER (MPI_SUCCESS=0)
       INTEGER MPI_ERR_OTHER
       PARAMETER (MPI_ERR_OTHER=15)
       INTEGER MPI_ERR_WIN
       PARAMETER (MPI_ERR_WIN=45)
       INTEGER MPI_ERR_FILE
       PARAMETER (MPI_ERR_FILE=27)
       INTEGER MPI_ERR_COUNT
       PARAMETER (MPI_ERR_COUNT=2)
       INTEGER MPI_ERR_SPAWN
       PARAMETER (MPI_ERR_SPAWN=42)
       INTEGER MPI_ERR_BASE
       PARAMETER (MPI_ERR_BASE=46)
       INTEGER MPI_ERR_RMA_CONFLICT
       PARAMETER (MPI_ERR_RMA_CONFLICT=49)
       INTEGER MPI_ERR_IN_STATUS
       PARAMETER (MPI_ERR_IN_STATUS=17)
       INTEGER MPI_ERR_INFO_KEY
       PARAMETER (MPI_ERR_INFO_KEY=29)
       INTEGER MPI_ERR_LOCKTYPE
       PARAMETER (MPI_ERR_LOCKTYPE=47)
       INTEGER MPI_ERR_OP
       PARAMETER (MPI_ERR_OP=9)
       INTEGER MPI_ERR_ARG
       PARAMETER (MPI_ERR_ARG=12)
       INTEGER MPI_ERR_READ_ONLY
       PARAMETER (MPI_ERR_READ_ONLY=40)
       INTEGER MPI_ERR_SIZE
       PARAMETER (MPI_ERR_SIZE=51)
       INTEGER MPI_ERR_BUFFER
       PARAMETER (MPI_ERR_BUFFER=1)
       INTEGER MPI_ERR_DUP_DATAREP
       PARAMETER (MPI_ERR_DUP_DATAREP=24)
       INTEGER MPI_ERR_UNSUPPORTED_DATAREP
       PARAMETER (MPI_ERR_UNSUPPORTED_DATAREP=43)
       INTEGER MPI_ERR_LASTCODE
       PARAMETER (MPI_ERR_LASTCODE=1073741823)
       INTEGER MPI_ERR_TRUNCATE
       PARAMETER (MPI_ERR_TRUNCATE=14)
       INTEGER MPI_ERR_DISP
       PARAMETER (MPI_ERR_DISP=52)
       INTEGER MPI_ERR_PORT
       PARAMETER (MPI_ERR_PORT=38)
       INTEGER MPI_ERR_INFO_NOKEY
       PARAMETER (MPI_ERR_INFO_NOKEY=31)
       INTEGER MPI_ERR_ASSERT
       PARAMETER (MPI_ERR_ASSERT=53)
       INTEGER MPI_ERR_FILE_EXISTS
       PARAMETER (MPI_ERR_FILE_EXISTS=25)
       INTEGER MPI_ERR_PENDING
       PARAMETER (MPI_ERR_PENDING=18)
       INTEGER MPI_ERR_COMM
       PARAMETER (MPI_ERR_COMM=5)
       INTEGER MPI_ERR_KEYVAL
       PARAMETER (MPI_ERR_KEYVAL=48)
       INTEGER MPI_ERR_NAME
       PARAMETER (MPI_ERR_NAME=33)
       INTEGER MPI_ERR_REQUEST
       PARAMETER (MPI_ERR_REQUEST=19)
       INTEGER MPI_ERR_GROUP
       PARAMETER (MPI_ERR_GROUP=8)
       INTEGER MPI_ERR_TOPOLOGY
       PARAMETER (MPI_ERR_TOPOLOGY=10)
       INTEGER MPI_ERR_TYPE
       PARAMETER (MPI_ERR_TYPE=3)
       INTEGER MPI_ERR_TAG
       PARAMETER (MPI_ERR_TAG=4)
       INTEGER MPI_ERR_INFO_VALUE
       PARAMETER (MPI_ERR_INFO_VALUE=30)
       INTEGER MPI_ERR_NOT_SAME
       PARAMETER (MPI_ERR_NOT_SAME=35)
       INTEGER MPI_ERR_RMA_SYNC
       PARAMETER (MPI_ERR_RMA_SYNC=50)
       INTEGER MPI_ERR_INFO
       PARAMETER (MPI_ERR_INFO=28)
       INTEGER MPI_ERR_NO_MEM
       PARAMETER (MPI_ERR_NO_MEM=34)
       INTEGER MPI_ERR_BAD_FILE
       PARAMETER (MPI_ERR_BAD_FILE=22)
       INTEGER MPI_ERR_FILE_IN_USE
       PARAMETER (MPI_ERR_FILE_IN_USE=26)
       INTEGER MPI_ERR_UNKNOWN
       PARAMETER (MPI_ERR_UNKNOWN=13)
       INTEGER MPI_ERR_UNSUPPORTED_OPERATION
       PARAMETER (MPI_ERR_UNSUPPORTED_OPERATION=44)
       INTEGER MPI_ERR_QUOTA
       PARAMETER (MPI_ERR_QUOTA=39)
       INTEGER MPI_ERR_AMODE
       PARAMETER (MPI_ERR_AMODE=21)
       INTEGER MPI_ERR_ROOT
       PARAMETER (MPI_ERR_ROOT=7)
       INTEGER MPI_ERR_RANK
       PARAMETER (MPI_ERR_RANK=6)
       INTEGER MPI_ERR_DIMS
       PARAMETER (MPI_ERR_DIMS=11)
       INTEGER MPI_ERR_NO_SUCH_FILE
       PARAMETER (MPI_ERR_NO_SUCH_FILE=37)
       INTEGER MPI_ERR_SERVICE
       PARAMETER (MPI_ERR_SERVICE=41)
       INTEGER MPI_ERR_INTERN
       PARAMETER (MPI_ERR_INTERN=16)
       INTEGER MPI_ERR_IO
       PARAMETER (MPI_ERR_IO=32)
       INTEGER MPI_ERR_ACCESS
       PARAMETER (MPI_ERR_ACCESS=20)
       INTEGER MPI_ERR_NO_SPACE
       PARAMETER (MPI_ERR_NO_SPACE=36)
       INTEGER MPI_ERR_CONVERSION
       PARAMETER (MPI_ERR_CONVERSION=23)
       INTEGER MPI_ERRORS_ARE_FATAL
       PARAMETER (MPI_ERRORS_ARE_FATAL=1409286144)
       INTEGER MPI_ERRORS_RETURN
       PARAMETER (MPI_ERRORS_RETURN=1409286145)
       INTEGER MPI_IDENT
       PARAMETER (MPI_IDENT=0)
       INTEGER MPI_CONGRUENT
       PARAMETER (MPI_CONGRUENT=1)
       INTEGER MPI_SIMILAR
       PARAMETER (MPI_SIMILAR=2)
       INTEGER MPI_UNEQUAL
       PARAMETER (MPI_UNEQUAL=3)
       INTEGER MPI_MAX
       PARAMETER (MPI_MAX=1476395009)
       INTEGER MPI_MIN
       PARAMETER (MPI_MIN=1476395010)
       INTEGER MPI_SUM
       PARAMETER (MPI_SUM=1476395011)
       INTEGER MPI_PROD
       PARAMETER (MPI_PROD=1476395012)
       INTEGER MPI_LAND
       PARAMETER (MPI_LAND=1476395013)
       INTEGER MPI_BAND
       PARAMETER (MPI_BAND=1476395014)
       INTEGER MPI_LOR
       PARAMETER (MPI_LOR=1476395015)
       INTEGER MPI_BOR
       PARAMETER (MPI_BOR=1476395016)
       INTEGER MPI_LXOR
       PARAMETER (MPI_LXOR=1476395017)
       INTEGER MPI_BXOR
       PARAMETER (MPI_BXOR=1476395018)
       INTEGER MPI_MINLOC
       PARAMETER (MPI_MINLOC=1476395019)
       INTEGER MPI_MAXLOC
       PARAMETER (MPI_MAXLOC=1476395020)
       INTEGER MPI_REPLACE
       PARAMETER (MPI_REPLACE=1476395021)
       INTEGER MPI_COMM_WORLD
       PARAMETER (MPI_COMM_WORLD=1140850688)
       INTEGER MPI_COMM_SELF
       PARAMETER (MPI_COMM_SELF=1140850689)
       INTEGER MPI_GROUP_EMPTY
       PARAMETER (MPI_GROUP_EMPTY=1207959552)
       INTEGER MPI_COMM_NULL
       PARAMETER (MPI_COMM_NULL=67108864)
       INTEGER MPI_WIN_NULL
       PARAMETER (MPI_WIN_NULL=536870912)
       INTEGER MPI_FILE_NULL
       PARAMETER (MPI_FILE_NULL=0)
       INTEGER MPI_GROUP_NULL
       PARAMETER (MPI_GROUP_NULL=134217728)
       INTEGER MPI_OP_NULL
       PARAMETER (MPI_OP_NULL=402653184)
       INTEGER MPI_DATATYPE_NULL
       PARAMETER (MPI_DATATYPE_NULL=201326592)
       INTEGER MPI_REQUEST_NULL
       PARAMETER (MPI_REQUEST_NULL=738197504)
       INTEGER MPI_ERRHANDLER_NULL
       PARAMETER (MPI_ERRHANDLER_NULL=335544320)
       INTEGER MPI_INFO_NULL
       PARAMETER (MPI_INFO_NULL=469762048)
       INTEGER MPI_TAG_UB
       PARAMETER (MPI_TAG_UB=1681915906)
       INTEGER MPI_HOST
       PARAMETER (MPI_HOST=1681915908)
       INTEGER MPI_IO
       PARAMETER (MPI_IO=1681915910)
       INTEGER MPI_WTIME_IS_GLOBAL
       PARAMETER (MPI_WTIME_IS_GLOBAL=1681915912)
       INTEGER MPI_UNIVERSE_SIZE
       PARAMETER (MPI_UNIVERSE_SIZE=1681915914)
       INTEGER MPI_LASTUSEDCODE
       PARAMETER (MPI_LASTUSEDCODE=1681915916)
       INTEGER MPI_APPNUM
       PARAMETER (MPI_APPNUM=1681915918)
       INTEGER MPI_WIN_BASE
       PARAMETER (MPI_WIN_BASE=1711276034)
       INTEGER MPI_WIN_SIZE
       PARAMETER (MPI_WIN_SIZE=1711276036)
       INTEGER MPI_WIN_DISP_UNIT
       PARAMETER (MPI_WIN_DISP_UNIT=1711276038)
       INTEGER MPI_MAX_ERROR_STRING
       PARAMETER (MPI_MAX_ERROR_STRING=512-1)
       INTEGER MPI_MAX_PORT_NAME
       PARAMETER (MPI_MAX_PORT_NAME=255)
       INTEGER MPI_MAX_OBJECT_NAME
       PARAMETER (MPI_MAX_OBJECT_NAME=127)
       INTEGER MPI_MAX_INFO_KEY
       PARAMETER (MPI_MAX_INFO_KEY=254)
       INTEGER MPI_MAX_INFO_VAL
       PARAMETER (MPI_MAX_INFO_VAL=1023)
       INTEGER MPI_MAX_PROCESSOR_NAME
       PARAMETER (MPI_MAX_PROCESSOR_NAME=128-1)
       INTEGER MPI_MAX_DATAREP_STRING
       PARAMETER (MPI_MAX_DATAREP_STRING=127)
       INTEGER MPI_UNDEFINED, MPI_UNDEFINED_RANK
       PARAMETER (MPI_UNDEFINED=(-32766))
       PARAMETER (MPI_UNDEFINED_RANK=(-32766))
       INTEGER MPI_KEYVAL_INVALID
       PARAMETER (MPI_KEYVAL_INVALID=603979776)
       INTEGER MPI_BSEND_OVERHEAD
       PARAMETER (MPI_BSEND_OVERHEAD=95)
       INTEGER MPI_PROC_NULL
       PARAMETER (MPI_PROC_NULL=-1)
       INTEGER MPI_ANY_SOURCE
       PARAMETER (MPI_ANY_SOURCE=-2)
       INTEGER MPI_ANY_TAG
       PARAMETER (MPI_ANY_TAG=-1)
       INTEGER MPI_ROOT
       PARAMETER (MPI_ROOT=-3)
       INTEGER MPI_GRAPH
       PARAMETER (MPI_GRAPH=1)
       INTEGER MPI_CART
       PARAMETER (MPI_CART=2)
       INTEGER MPI_DIST_GRAPH
       PARAMETER (MPI_DIST_GRAPH=3)
       INTEGER MPI_VERSION
       PARAMETER (MPI_VERSION=2)
       INTEGER MPI_SUBVERSION
       PARAMETER (MPI_SUBVERSION=2)
       INTEGER MPI_LOCK_EXCLUSIVE
       PARAMETER (MPI_LOCK_EXCLUSIVE=234)
       INTEGER MPI_LOCK_SHARED
       PARAMETER (MPI_LOCK_SHARED=235)
       INTEGER MPI_COMPLEX
       PARAMETER (MPI_COMPLEX=1275070494)
       INTEGER MPI_DOUBLE_COMPLEX
       PARAMETER (MPI_DOUBLE_COMPLEX=1275072546)
       INTEGER MPI_LOGICAL
       PARAMETER (MPI_LOGICAL=1275069469)
       INTEGER MPI_REAL
       PARAMETER (MPI_REAL=1275069468)
       INTEGER MPI_DOUBLE_PRECISION
       PARAMETER (MPI_DOUBLE_PRECISION=1275070495)
       INTEGER MPI_INTEGER
       PARAMETER (MPI_INTEGER=1275069467)
       INTEGER MPI_2INTEGER
       PARAMETER (MPI_2INTEGER=1275070496)
       INTEGER MPI_2COMPLEX
       PARAMETER (MPI_2COMPLEX=1275072548)
       INTEGER MPI_2DOUBLE_PRECISION
       PARAMETER (MPI_2DOUBLE_PRECISION=1275072547)
       INTEGER MPI_2REAL
       PARAMETER (MPI_2REAL=1275070497)
       INTEGER MPI_2DOUBLE_COMPLEX
       PARAMETER (MPI_2DOUBLE_COMPLEX=1275076645)
       INTEGER MPI_CHARACTER
       PARAMETER (MPI_CHARACTER=1275068698)
       INTEGER MPI_BYTE
       PARAMETER (MPI_BYTE=1275068685)
       INTEGER MPI_UB
       PARAMETER (MPI_UB=1275068433)
       INTEGER MPI_LB
       PARAMETER (MPI_LB=1275068432)
       INTEGER MPI_PACKED
       PARAMETER (MPI_PACKED=1275068687)
       INTEGER MPI_INTEGER1
       PARAMETER (MPI_INTEGER1=1275068717)
       INTEGER MPI_INTEGER2
       PARAMETER (MPI_INTEGER2=1275068975)
       INTEGER MPI_INTEGER4
       PARAMETER (MPI_INTEGER4=1275069488)
       INTEGER MPI_INTEGER8
       PARAMETER (MPI_INTEGER8=1275070513)
       INTEGER MPI_INTEGER16
       PARAMETER (MPI_INTEGER16=MPI_DATATYPE_NULL)
       INTEGER MPI_REAL4
       PARAMETER (MPI_REAL4=1275069479)
       INTEGER MPI_REAL8
       PARAMETER (MPI_REAL8=1275070505)
       INTEGER MPI_REAL16
       PARAMETER (MPI_REAL16=1275072555)
       INTEGER MPI_COMPLEX8
       PARAMETER (MPI_COMPLEX8=1275070504)
       INTEGER MPI_COMPLEX16
       PARAMETER (MPI_COMPLEX16=1275072554)
       INTEGER MPI_COMPLEX32
       PARAMETER (MPI_COMPLEX32=1275076652)
       INTEGER MPI_ADDRESS_KIND, MPI_OFFSET_KIND, MPI_INTEGER_KIND
       PARAMETER (MPI_ADDRESS_KIND=8)
       PARAMETER (MPI_OFFSET_KIND=8)
       PARAMETER (MPI_INTEGER_KIND=4)
       INTEGER MPI_CHAR
       PARAMETER (MPI_CHAR=1275068673)
       INTEGER MPI_SIGNED_CHAR
       PARAMETER (MPI_SIGNED_CHAR=1275068696)
       INTEGER MPI_UNSIGNED_CHAR
       PARAMETER (MPI_UNSIGNED_CHAR=1275068674)
       INTEGER MPI_WCHAR
       PARAMETER (MPI_WCHAR=1275069454)
       INTEGER MPI_SHORT
       PARAMETER (MPI_SHORT=1275068931)
       INTEGER MPI_UNSIGNED_SHORT
       PARAMETER (MPI_UNSIGNED_SHORT=1275068932)
       INTEGER MPI_INT
       PARAMETER (MPI_INT=1275069445)
       INTEGER MPI_UNSIGNED
       PARAMETER (MPI_UNSIGNED=1275069446)
       INTEGER MPI_LONG
       PARAMETER (MPI_LONG=1275070471)
       INTEGER MPI_UNSIGNED_LONG
       PARAMETER (MPI_UNSIGNED_LONG=1275070472)
       INTEGER MPI_FLOAT
       PARAMETER (MPI_FLOAT=1275069450)
       INTEGER MPI_DOUBLE
       PARAMETER (MPI_DOUBLE=1275070475)
       INTEGER MPI_LONG_DOUBLE
       PARAMETER (MPI_LONG_DOUBLE=1275072524)
       INTEGER MPI_LONG_LONG_INT
       PARAMETER (MPI_LONG_LONG_INT=1275070473)
       INTEGER MPI_UNSIGNED_LONG_LONG
       PARAMETER (MPI_UNSIGNED_LONG_LONG=1275070489)
       INTEGER MPI_LONG_LONG
       PARAMETER (MPI_LONG_LONG=1275070473)
       INTEGER MPI_FLOAT_INT
       PARAMETER (MPI_FLOAT_INT=-1946157056)
       INTEGER MPI_DOUBLE_INT
       PARAMETER (MPI_DOUBLE_INT=-1946157055)
       INTEGER MPI_LONG_INT
       PARAMETER (MPI_LONG_INT=-1946157054)
       INTEGER MPI_SHORT_INT
       PARAMETER (MPI_SHORT_INT=-1946157053)
       INTEGER MPI_2INT
       PARAMETER (MPI_2INT=1275070486)
       INTEGER MPI_LONG_DOUBLE_INT
       PARAMETER (MPI_LONG_DOUBLE_INT=-1946157052)
       INTEGER MPI_INT8_T
       PARAMETER (MPI_INT8_T=1275068727)
       INTEGER MPI_INT16_T
       PARAMETER (MPI_INT16_T=1275068984)
       INTEGER MPI_INT32_T
       PARAMETER (MPI_INT32_T=1275069497)
       INTEGER MPI_INT64_T
       PARAMETER (MPI_INT64_T=1275070522)
       INTEGER MPI_UINT8_T
       PARAMETER (MPI_UINT8_T=1275068731)
       INTEGER MPI_UINT16_T
       PARAMETER (MPI_UINT16_T=1275068988)
       INTEGER MPI_UINT32_T
       PARAMETER (MPI_UINT32_T=1275069501)
       INTEGER MPI_UINT64_T
       PARAMETER (MPI_UINT64_T=1275070526)
       INTEGER MPI_C_BOOL
       PARAMETER (MPI_C_BOOL=1275068735)
       INTEGER MPI_C_FLOAT_COMPLEX
       PARAMETER (MPI_C_FLOAT_COMPLEX=1275070528)
       INTEGER MPI_C_COMPLEX
       PARAMETER (MPI_C_COMPLEX=1275070528)
       INTEGER MPI_C_DOUBLE_COMPLEX
       PARAMETER (MPI_C_DOUBLE_COMPLEX=1275072577)
       INTEGER MPI_C_LONG_DOUBLE_COMPLEX
       PARAMETER (MPI_C_LONG_DOUBLE_COMPLEX=1275076674)
       INTEGER MPI_AINT
       PARAMETER (MPI_AINT=1275070531)
       INTEGER MPI_OFFSET
       PARAMETER (MPI_OFFSET=1275070532)
       INTEGER MPI_COMBINER_NAMED
       PARAMETER (MPI_COMBINER_NAMED=1)
       INTEGER MPI_COMBINER_DUP
       PARAMETER (MPI_COMBINER_DUP=2)
       INTEGER MPI_COMBINER_CONTIGUOUS
       PARAMETER (MPI_COMBINER_CONTIGUOUS=3)
       INTEGER MPI_COMBINER_VECTOR
       PARAMETER (MPI_COMBINER_VECTOR=4)
       INTEGER MPI_COMBINER_HVECTOR_INTEGER
       PARAMETER (MPI_COMBINER_HVECTOR_INTEGER=5)
       INTEGER MPI_COMBINER_HVECTOR
       PARAMETER (MPI_COMBINER_HVECTOR=6)
       INTEGER MPI_COMBINER_INDEXED
       PARAMETER (MPI_COMBINER_INDEXED=7)
       INTEGER MPI_COMBINER_HINDEXED_INTEGER
       PARAMETER (MPI_COMBINER_HINDEXED_INTEGER=8)
       INTEGER MPI_COMBINER_HINDEXED
       PARAMETER (MPI_COMBINER_HINDEXED=9)
       INTEGER MPI_COMBINER_INDEXED_BLOCK
       PARAMETER (MPI_COMBINER_INDEXED_BLOCK=10)
       INTEGER MPI_COMBINER_STRUCT_INTEGER
       PARAMETER (MPI_COMBINER_STRUCT_INTEGER=11)
       INTEGER MPI_COMBINER_STRUCT
       PARAMETER (MPI_COMBINER_STRUCT=12)
       INTEGER MPI_COMBINER_SUBARRAY
       PARAMETER (MPI_COMBINER_SUBARRAY=13)
       INTEGER MPI_COMBINER_DARRAY
       PARAMETER (MPI_COMBINER_DARRAY=14)
       INTEGER MPI_COMBINER_F90_REAL
       PARAMETER (MPI_COMBINER_F90_REAL=15)
       INTEGER MPI_COMBINER_F90_COMPLEX
       PARAMETER (MPI_COMBINER_F90_COMPLEX=16)
       INTEGER MPI_COMBINER_F90_INTEGER
       PARAMETER (MPI_COMBINER_F90_INTEGER=17)
       INTEGER MPI_COMBINER_RESIZED
       PARAMETER (MPI_COMBINER_RESIZED=18)
       INTEGER MPI_TYPECLASS_REAL
       PARAMETER (MPI_TYPECLASS_REAL=1)
       INTEGER MPI_TYPECLASS_INTEGER
       PARAMETER (MPI_TYPECLASS_INTEGER=2)
       INTEGER MPI_TYPECLASS_COMPLEX
       PARAMETER (MPI_TYPECLASS_COMPLEX=3)
       INTEGER MPI_MODE_NOCHECK
       PARAMETER (MPI_MODE_NOCHECK=1024)
       INTEGER MPI_MODE_NOSTORE
       PARAMETER (MPI_MODE_NOSTORE=2048)
       INTEGER MPI_MODE_NOPUT
       PARAMETER (MPI_MODE_NOPUT=4096)
       INTEGER MPI_MODE_NOPRECEDE
       PARAMETER (MPI_MODE_NOPRECEDE=8192)
       INTEGER MPI_MODE_NOSUCCEED
       PARAMETER (MPI_MODE_NOSUCCEED=16384)
       INTEGER MPI_THREAD_SINGLE
       PARAMETER (MPI_THREAD_SINGLE=0)
       INTEGER MPI_THREAD_FUNNELED
       PARAMETER (MPI_THREAD_FUNNELED=1)
       INTEGER MPI_THREAD_SERIALIZED
       PARAMETER (MPI_THREAD_SERIALIZED=2)
       INTEGER MPI_THREAD_MULTIPLE
       PARAMETER (MPI_THREAD_MULTIPLE=3)
       INTEGER MPI_MODE_RDONLY
       PARAMETER (MPI_MODE_RDONLY=2)
       INTEGER MPI_MODE_RDWR
       PARAMETER (MPI_MODE_RDWR=8)
       INTEGER MPI_MODE_WRONLY
       PARAMETER (MPI_MODE_WRONLY=4)
       INTEGER MPI_MODE_DELETE_ON_CLOSE
       PARAMETER (MPI_MODE_DELETE_ON_CLOSE=16)
       INTEGER MPI_MODE_UNIQUE_OPEN
       PARAMETER (MPI_MODE_UNIQUE_OPEN=32)
       INTEGER MPI_MODE_CREATE
       PARAMETER (MPI_MODE_CREATE=1)
       INTEGER MPI_MODE_EXCL
       PARAMETER (MPI_MODE_EXCL=64)
       INTEGER MPI_MODE_APPEND
       PARAMETER (MPI_MODE_APPEND=128)
       INTEGER MPI_MODE_SEQUENTIAL
       PARAMETER (MPI_MODE_SEQUENTIAL=256)
       INTEGER MPI_SEEK_SET
       PARAMETER (MPI_SEEK_SET=600)
       INTEGER MPI_SEEK_CUR
       PARAMETER (MPI_SEEK_CUR=602)
       INTEGER MPI_SEEK_END
       PARAMETER (MPI_SEEK_END=604)
       INTEGER MPI_ORDER_C
       PARAMETER (MPI_ORDER_C=56)
       INTEGER MPI_ORDER_FORTRAN
       PARAMETER (MPI_ORDER_FORTRAN=57)
       INTEGER MPI_DISTRIBUTE_BLOCK
       PARAMETER (MPI_DISTRIBUTE_BLOCK=121)
       INTEGER MPI_DISTRIBUTE_CYCLIC
       PARAMETER (MPI_DISTRIBUTE_CYCLIC=122)
       INTEGER MPI_DISTRIBUTE_NONE
       PARAMETER (MPI_DISTRIBUTE_NONE=123)
       INTEGER MPI_DISTRIBUTE_DFLT_DARG
       PARAMETER (MPI_DISTRIBUTE_DFLT_DARG=-49767)
       integer*8 MPI_DISPLACEMENT_CURRENT
       PARAMETER (MPI_DISPLACEMENT_CURRENT=-54278278)
       INTEGER*4 MPI_BOTTOM, MPI_IN_PLACE, MPI_UNWEIGHTED
       EXTERNAL MPI_DUP_FN, MPI_NULL_DELETE_FN, MPI_NULL_COPY_FN
       EXTERNAL MPI_WTIME, MPI_WTICK
       EXTERNAL PMPI_WTIME, PMPI_WTICK
       EXTERNAL MPI_COMM_DUP_FN, MPI_COMM_NULL_DELETE_FN
       EXTERNAL MPI_COMM_NULL_COPY_FN
       EXTERNAL MPI_WIN_DUP_FN, MPI_WIN_NULL_DELETE_FN
       EXTERNAL MPI_WIN_NULL_COPY_FN
       EXTERNAL MPI_TYPE_DUP_FN, MPI_TYPE_NULL_DELETE_FN
       EXTERNAL MPI_TYPE_NULL_COPY_FN
       EXTERNAL MPI_CONVERSION_FN_NULL
       DOUBLE PRECISION MPI_WTIME, MPI_WTICK
       DOUBLE PRECISION PMPI_WTIME, PMPI_WTICK


!      CHARACTER*1 PADS_A(3), PADS_B(3)
!      COMMON /MPIFCMB1/ MPI_STATUS_IGNORE
!      COMMON /MPIFCMB2/ MPI_STATUSES_IGNORE
!      COMMON /MPIFCMB3/ MPI_BOTTOM
!      COMMON /MPIFCMB4/ MPI_IN_PLACE
       COMMON /MPIFCMB5/ MPI_UNWEIGHTED
!      COMMON /MPIFCMB6/ MPI_ERRCODES_IGNORE
!      COMMON /MPIFCMB7/ MPI_ARGVS_NULL, PADS_A
!      COMMON /MPIFCMB8/ MPI_ARGV_NULL, PADS_B
!      SAVE /MPIFCMB1/,/MPIFCMB2/
!      SAVE /MPIFCMB3/,/MPIFCMB4/,/MPIFCMB5/,/MPIFCMB6/
       SAVE /MPIFCMB5/
!      SAVE /MPIFCMB7/,/MPIFCMB8/

       COMMON /MPIPRIV1/ MPI_BOTTOM, MPI_IN_PLACE, MPI_STATUS_IGNORE

       COMMON /MPIPRIV2/ MPI_STATUSES_IGNORE, MPI_ERRCODES_IGNORE
       SAVE /MPIPRIV1/,/MPIPRIV2/

       COMMON /MPIPRIVC/ MPI_ARGVS_NULL, MPI_ARGV_NULL
       SAVE   /MPIPRIVC/
!sgi_mipspro gets this from 'use mpi'


!--- public paramters  -----------------------------------------------
  public :: MPP_VERBOSE, MPP_DEBUG, ALL_PES, ANY_PE, NULL_PE, NOTE, WARNING, FATAL
  public :: MPP_CLOCK_SYNC, MPP_CLOCK_DETAILED, CLOCK_COMPONENT, CLOCK_SUBCOMPONENT
  public :: CLOCK_MODULE_DRIVER, CLOCK_MODULE, CLOCK_ROUTINE, CLOCK_LOOP, CLOCK_INFRA
  public :: MAXPES, EVENT_RECV, EVENT_SEND

!--- public data from mpp_data_mod ------------------------------
  public :: request

!--- public interface from mpp_util.h ------------------------------
  public :: stdin, stdout, stderr, stdlog, lowercase, uppercase, mpp_error, mpp_error_state
  public :: mpp_set_warn_level, mpp_sync, mpp_sync_self, mpp_set_stack_size, mpp_pe
  public :: mpp_node, mpp_npes, mpp_root_pe, mpp_set_root_pe, mpp_declare_pelist
  public :: mpp_get_current_pelist, mpp_set_current_pelist, mpp_clock_begin, mpp_clock_end
  public :: mpp_clock_id, mpp_clock_set_grain, mpp_record_timing_data, get_unit

!--- public interface from mpp_comm.h ------------------------------
  public :: mpp_chksum, mpp_max, mpp_min, mpp_sum, mpp_transmit, mpp_send, mpp_recv
  public :: mpp_broadcast, mpp_malloc, mpp_init, mpp_exit


!*********************************************************************
!
!    public data type
!
!*********************************************************************
!peset hold communicators as SHMEM-compatible triads (start, log2(stride), num)
  type :: communicator
     private
     character(len=32) :: name
     integer, pointer  :: list(:) =>NULL()
     integer           :: count
     integer           :: start, log2stride ! dummy variables when libMPI is defined.
     integer           :: id, group         ! MPI communicator and group id for this PE set.
! dummy variables when libSMA is defined.
  end type communicator

  type :: event
     private
     character(len=16)                         :: name
     integer(8), dimension(MAX_EVENTS) :: ticks, bytes
     integer                                   :: calls
  end type event

!a clock contains an array of event profiles for a region
  type :: clock
     private
     character(len=32)    :: name
     integer(8)   :: tick
     integer(8)   :: total_ticks
     integer              :: peset_num
     logical              :: sync_on_begin, detailed
     integer              :: grain
     type(event), pointer :: events(:) =>NULL() !if needed, allocate to MAX_EVENT_TYPES
     logical              :: is_on              !initialize to false. set true when calling mpp_clock_begin
! set false when calling mpp_clock_end
  end type clock

  type :: Clock_Data_Summary
     private
     character(len=16)  :: name
     real(8)  :: msg_size_sums(MAX_BINS)
     real(8)  :: msg_time_sums(MAX_BINS)
     real(8)  :: total_data
     real(8)  :: total_time
     integer(8) :: msg_size_cnts(MAX_BINS)
     integer(8) :: total_cnts
  end type Clock_Data_Summary

  type :: Summary_Struct
     private
     character(len=16)         :: name
     type (Clock_Data_Summary) :: event(MAX_EVENT_TYPES)
  end type Summary_Struct

!***********************************************************************
!
!     public interface from mpp_util.h
!
!***********************************************************************
! <INTERFACE NAME="mpp_error">
!  <OVERVIEW>
!    Error handler.
!  </OVERVIEW>
!  <DESCRIPTION>
!    It is strongly recommended that all error exits pass through
!    <TT>mpp_error</TT> to assure the program fails cleanly. An individual
!    PE encountering a <TT>STOP</TT> statement, for instance, can cause the
!    program to hang. The use of the <TT>STOP</TT> statement is strongly
!    discouraged.
!
!    Calling mpp_error with no arguments produces an immediate error
!    exit, i.e:
!    <PRE>
!    call mpp_error
!    call mpp_error(FATAL)
!    </PRE>
!    are equivalent.
!
!    The argument order
!    <PRE>
!    call mpp_error( routine, errormsg, errortype )
!    </PRE>
!    is also provided to support legacy code. In this version of the
!    call, none of the arguments may be omitted.
!
!    The behaviour of <TT>mpp_error</TT> for a <TT>WARNING</TT> can be
!    controlled with an additional call <TT>mpp_set_warn_level</TT>.
!    <PRE>
!    call mpp_set_warn_level(ERROR)
!    </PRE>
!    causes <TT>mpp_error</TT> to treat <TT>WARNING</TT>
!    exactly like <TT>FATAL</TT>.
!    <PRE>
!    call mpp_set_warn_level(WARNING)
!    </PRE>
!    resets to the default behaviour described above.
!
!    <TT>mpp_error</TT> also has an internal error state which
!    maintains knowledge of whether a warning has been issued. This can be
!    used at startup in a subroutine that checks if the model has been
!    properly configured. You can generate a series of warnings using
!    <TT>mpp_error</TT>, and then check at the end if any warnings has been
!    issued using the function <TT>mpp_error_state()</TT>. If the value of
!    this is <TT>WARNING</TT>, at least one warning has been issued, and
!    the user can take appropriate action:
!
!    <PRE>
!    if( ... )call mpp_error( WARNING, '...' )
!    if( ... )call mpp_error( WARNING, '...' )
!    if( ... )call mpp_error( WARNING, '...' )
!    ...
!    if( mpp_error_state().EQ.WARNING )call mpp_error( FATAL, '...' )
!    </PRE>
!  </DESCRIPTION>
!  <TEMPLATE>
!    call mpp_error( errortype, routine, errormsg )
!  </TEMPLATE>
!  <IN NAME="errortype">
!    One of <TT>NOTE</TT>, <TT>WARNING</TT> or <TT>FATAL</TT>
!    (these definitions are acquired by use association).
!    <TT>NOTE</TT> writes <TT>errormsg</TT> to <TT>STDOUT</TT>.
!    <TT>WARNING</TT> writes <TT>errormsg</TT> to <TT>STDERR</TT>.
!    <TT>FATAL</TT> writes <TT>errormsg</TT> to <TT>STDERR</TT>,
!    and induces a clean error exit with a call stack traceback.
!  </IN>
! </INTERFACE>
  interface mpp_error
     module procedure mpp_error_basic
     module procedure mpp_error_mesg
     module procedure mpp_error_noargs
     module procedure mpp_error_is
     module procedure mpp_error_rs
     module procedure mpp_error_ia
     module procedure mpp_error_ra
     module procedure mpp_error_ia_ia
     module procedure mpp_error_ia_ra
     module procedure mpp_error_ra_ia
     module procedure mpp_error_ra_ra
     module procedure mpp_error_ia_is
     module procedure mpp_error_ia_rs
     module procedure mpp_error_ra_is
     module procedure mpp_error_ra_rs
     module procedure mpp_error_is_ia
     module procedure mpp_error_is_ra
     module procedure mpp_error_rs_ia
     module procedure mpp_error_rs_ra
     module procedure mpp_error_is_is
     module procedure mpp_error_is_rs
     module procedure mpp_error_rs_is
     module procedure mpp_error_rs_rs
  end interface

  interface array_to_char
     module procedure iarray_to_char
     module procedure rarray_to_char
  end interface

!***********************************************************************
!
!    public interface from mpp_comm.h
!
!***********************************************************************


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!       ROUTINES TO INITIALIZE/FINALIZE MPP MODULE: mpp_init, mpp_exit        !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

! <SUBROUTINE NAME="mpp_init">
!  <OVERVIEW>
!   Initialize <TT>mpp_mod</TT>.
!  </OVERVIEW>
!  <DESCRIPTION>
!   Called to initialize the <TT>mpp_mod</TT> package. It is recommended
!   that this call be the first executed line in your program. It sets the
!   number of PEs assigned to this run (acquired from the command line, or
!   through the environment variable <TT>NPES</TT>), and associates an ID
!   number to each PE. These can be accessed by calling <LINK
!   SRC="#mpp_npes"><TT>mpp_npes</TT></LINK> and <LINK
!   SRC="#mpp_pe"><TT>mpp_pe</TT></LINK>.
!  </DESCRIPTION>
!  <TEMPLATE>
!   call mpp_init( flags )
!  </TEMPLATE>
!  <IN NAME="flags" TYPE="integer">
!   <TT>flags</TT> can be set to <TT>MPP_VERBOSE</TT> to
!   have <TT>mpp_mod</TT> keep you informed of what it's up to.
!  </IN>
! </SUBROUTINE>

! <SUBROUTINE NAME="mpp_exit">
!  <OVERVIEW>
!   Exit <TT>mpp_mod</TT>.
!  </OVERVIEW>
!  <DESCRIPTION>
!   Called at the end of the run, or to re-initialize <TT>mpp_mod</TT>,
!   should you require that for some odd reason.
!
!   This call implies synchronization across all PEs.
!  </DESCRIPTION>
!  <TEMPLATE>
!   call mpp_exit()
!  </TEMPLATE>
! </SUBROUTINE>

!#######################################################################
! <SUBROUTINE NAME="mpp_malloc">
!  <OVERVIEW>
!    Symmetric memory allocation.
!  </OVERVIEW>
!  <DESCRIPTION>
!    This routine is used on SGI systems when <TT>mpp_mod</TT> is
!    invoked in the SHMEM library. It ensures that dynamically allocated
!    memory can be used with <TT>shmem_get</TT> and
!    <TT>shmem_put</TT>. This is called <I>symmetric
!    allocation</I> and is described in the
!    <TT>intro_shmem</TT> man page. <TT>ptr</TT> is a <I>Cray
!    pointer</I> (see the section on <LINK
!    SRC="#PORTABILITY">portability</LINK>).  The operation can be expensive
!    (since it requires a global barrier). We therefore attempt to re-use
!    existing allocation whenever possible. Therefore <TT>len</TT>
!    and <TT>ptr</TT> must have the <TT>SAVE</TT> attribute
!    in the calling routine, and retain the information about the last call
!    to <TT>mpp_malloc</TT>. Additional memory is symmetrically
!    allocated if and only if <TT>newlen</TT> exceeds
!    <TT>len</TT>.
!
!    This is never required on Cray PVP or MPP systems. While the T3E
!    manpages do talk about symmetric allocation, <TT>mpp_mod</TT>
!    is coded to remove this restriction.
!
!    It is never required if <TT>mpp_mod</TT> is invoked in MPI.
!
!   This call implies synchronization across all PEs.
!  </DESCRIPTION>
!  <TEMPLATE>
!   call mpp_malloc( ptr, newlen, len )
!  </TEMPLATE>
!  <IN NAME="ptr">
!     a cray pointer, points to a dummy argument in this routine.
!  </IN>
!  <IN NAME="newlen" TYPE="integer">
!     the required allocation length for the pointer ptr
!  </IN>
!  <IN NAME="len" TYPE="integer">
!     the current allocation (0 if unallocated).
!  </IN>
! </SUBROUTINE>

!#####################################################################

! <SUBROUTINE NAME="mpp_set_stack_size">
!  <OVERVIEW>
!    Allocate module internal workspace.
!  </OVERVIEW>
!  <DESCRIPTION>
!    <TT>mpp_mod</TT> maintains a private internal array called
!    <TT>mpp_stack</TT> for private workspace. This call sets the length,
!    in words, of this array.
!
!    The <TT>mpp_init</TT> call sets this
!    workspace length to a default of 32768, and this call may be used if a
!    longer workspace is needed.
!
!    This call implies synchronization across all PEs.
!
!    This workspace is symmetrically allocated, as required for
!    efficient communication on SGI and Cray MPP systems. Since symmetric
!    allocation must be performed by <I>all</I> PEs in a job, this call
!    must also be called by all PEs, using the same value of
!    <TT>n</TT>. Calling <TT>mpp_set_stack_size</TT> from a subset of PEs,
!    or with unequal argument <TT>n</TT>, may cause the program to hang.
!
!    If any MPP call using <TT>mpp_stack</TT> overflows the declared
!    stack array, the program will abort with a message specifying the
!    stack length that is required. Many users wonder why, if the required
!    stack length can be computed, it cannot also be specified at that
!    point. This cannot be automated because there is no way for the
!    program to know if all PEs are present at that call, and with equal
!    values of <TT>n</TT>. The program must be rerun by the user with the
!    correct argument to <TT>mpp_set_stack_size</TT>, called at an
!    appropriate point in the code where all PEs are known to be present.
!  </DESCRIPTION>
!  <TEMPLATE>
!    call mpp_set_stack_size(n)
!  </TEMPLATE>
!  <IN NAME="n" TYPE="integer"></IN>
! </SUBROUTINE>

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!            GLOBAL REDUCTION ROUTINES: mpp_max, mpp_sum, mpp_min             !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

! <INTERFACE NAME="mpp_max">
!  <OVERVIEW>
!    Reduction operations.
!  </OVERVIEW>
!  <DESCRIPTION>
!    Find the max of scalar a the PEs in pelist
!    result is also automatically broadcast to all PEs
!  </DESCRIPTION>
!  <TEMPLATE>
!    call  mpp_max( a, pelist )
!  </TEMPLATE>
!  <IN NAME="a">
!    <TT>real</TT> or <TT>integer</TT>, of 4-byte of 8-byte kind.
!  </IN>
!  <IN NAME="pelist">
!    If <TT>pelist</TT> is omitted, the context is assumed to be the
!    current pelist. This call implies synchronization across the PEs in
!    <TT>pelist</TT>, or the current pelist if <TT>pelist</TT> is absent.
!  </IN>
! </INTERFACE>

  interface mpp_max
     module procedure mpp_max_real8

     module procedure mpp_max_int8


     module procedure mpp_max_int4
  end interface

  interface mpp_min
     module procedure mpp_min_real8

     module procedure mpp_min_int8


     module procedure mpp_min_int4
  end interface


! <INTERFACE NAME="mpp_sum">
!  <OVERVIEW>
!    Reduction operation.
!  </OVERVIEW>
!  <DESCRIPTION>
!    <TT>MPP_TYPE_</TT> corresponds to any 4-byte and 8-byte variant of
!    <TT>integer, real, complex</TT> variables, of rank 0 or 1. A
!    contiguous block from a multi-dimensional array may be passed by its
!    starting address and its length, as in <TT>f77</TT>.
!
!    Library reduction operators are not required or guaranteed to be
!    bit-reproducible. In any case, changing the processor count changes
!    the data layout, and thus very likely the order of operations. For
!    bit-reproducible sums of distributed arrays, consider using the
!    <TT>mpp_global_sum</TT> routine provided by the <LINK
!    SRC="mpp_domains.html"><TT>mpp_domains</TT></LINK> module.
!
!    The <TT>bit_reproducible</TT> flag provided in earlier versions of
!    this routine has been removed.
!
!
!    If <TT>pelist</TT> is omitted, the context is assumed to be the
!    current pelist. This call implies synchronization across the PEs in
!    <TT>pelist</TT>, or the current pelist if <TT>pelist</TT> is absent.
!  </DESCRIPTION>
!  <TEMPLATE>
!    call mpp_sum( a, length, pelist )
!  </TEMPLATE>
!  <IN NAME="length"></IN>
!  <IN NAME="pelist"></IN>
!  <INOUT NAME="a"></INOUT>
! </INTERFACE>

  interface mpp_sum

     module procedure mpp_sum_int8
     module procedure mpp_sum_int8_scalar
     module procedure mpp_sum_int8_2d
     module procedure mpp_sum_int8_3d
     module procedure mpp_sum_int8_4d
     module procedure mpp_sum_int8_5d

     module procedure mpp_sum_real8
     module procedure mpp_sum_real8_scalar
     module procedure mpp_sum_real8_2d
     module procedure mpp_sum_real8_3d
     module procedure mpp_sum_real8_4d
     module procedure mpp_sum_real8_5d

     module procedure mpp_sum_int4
     module procedure mpp_sum_int4_scalar
     module procedure mpp_sum_int4_2d
     module procedure mpp_sum_int4_3d
     module procedure mpp_sum_int4_4d
     module procedure mpp_sum_int4_5d


  end interface

!#####################################################################

! <INTERFACE NAME="mpp_transmit">
!  <OVERVIEW>
!    Basic message-passing call.
!  </OVERVIEW>
!  <DESCRIPTION>
!    <TT>MPP_TYPE_</TT> corresponds to any 4-byte and 8-byte variant of
!    <TT>integer, real, complex, logical</TT> variables, of rank 0 or 1. A
!    contiguous block from a multi-dimensional array may be passed by its
!    starting address and its length, as in <TT>f77</TT>.
!
!    <TT>mpp_transmit</TT> is currently implemented as asynchronous
!    outward transmission and synchronous inward transmission. This follows
!    the behaviour of <TT>shmem_put</TT> and <TT>shmem_get</TT>. In MPI, it
!    is implemented as <TT>mpi_isend</TT> and <TT>mpi_recv</TT>. For most
!    applications, transmissions occur in pairs, and are here accomplished
!    in a single call.
!
!    The special PE designations <TT>NULL_PE</TT>,
!    <TT>ANY_PE</TT> and <TT>ALL_PES</TT> are provided by use
!    association.
!
!    <TT>NULL_PE</TT>: is used to disable one of the pair of
!    transmissions.<BR/>
!    <TT>ANY_PE</TT>: is used for unspecific remote
!    destination. (Please note that <TT>put_pe=ANY_PE</TT> has no meaning
!    in the MPI context, though it is available in the SHMEM invocation. If
!    portability is a concern, it is best avoided).<BR/>
!    <TT>ALL_PES</TT>: is used for broadcast operations.
!
!    It is recommended that <LINK
!    SRC="#mpp_broadcast"><TT>mpp_broadcast</TT></LINK> be used for
!    broadcasts.
!
!    The following example illustrates the use of
!    <TT>NULL_PE</TT> and <TT>ALL_PES</TT>:
!
!    <PRE>
!    real, dimension(n) :: a
!    if( pe.EQ.0 )then
!        do p = 1,npes-1
!           call mpp_transmit( a, n, p, a, n, NULL_PE )
!        end do
!    else
!        call mpp_transmit( a, n, NULL_PE, a, n, 0 )
!    end if
!
!    call mpp_transmit( a, n, ALL_PES, a, n, 0 )
!    </PRE>
!
!    The do loop and the broadcast operation above are equivalent.
!
!    Two overloaded calls <TT>mpp_send</TT> and
!     <TT>mpp_recv</TT> have also been
!    provided. <TT>mpp_send</TT> calls <TT>mpp_transmit</TT>
!    with <TT>get_pe=NULL_PE</TT>. <TT>mpp_recv</TT> calls
!    <TT>mpp_transmit</TT> with <TT>put_pe=NULL_PE</TT>. Thus
!    the do loop above could be written more succinctly:
!
!    <PRE>
!    if( pe.EQ.0 )then
!        do p = 1,npes-1
!           call mpp_send( a, n, p )
!        end do
!    else
!        call mpp_recv( a, n, 0 )
!    end if
!    </PRE>
!  </DESCRIPTION>
!  <TEMPLATE>
!    call mpp_transmit( put_data, put_len, put_pe, get_data, get_len, get_pe )
!  </TEMPLATE>
! </INTERFACE>
  interface mpp_transmit
     module procedure mpp_transmit_real8
     module procedure mpp_transmit_real8_scalar
     module procedure mpp_transmit_real8_2d
     module procedure mpp_transmit_real8_3d
     module procedure mpp_transmit_real8_4d
     module procedure mpp_transmit_real8_5d


     module procedure mpp_transmit_int8
     module procedure mpp_transmit_int8_scalar
     module procedure mpp_transmit_int8_2d
     module procedure mpp_transmit_int8_3d
     module procedure mpp_transmit_int8_4d
     module procedure mpp_transmit_int8_5d
     module procedure mpp_transmit_logical8
     module procedure mpp_transmit_logical8_scalar
     module procedure mpp_transmit_logical8_2d
     module procedure mpp_transmit_logical8_3d
     module procedure mpp_transmit_logical8_4d
     module procedure mpp_transmit_logical8_5d



     module procedure mpp_transmit_int4
     module procedure mpp_transmit_int4_scalar
     module procedure mpp_transmit_int4_2d
     module procedure mpp_transmit_int4_3d
     module procedure mpp_transmit_int4_4d
     module procedure mpp_transmit_int4_5d
     module procedure mpp_transmit_logical4
     module procedure mpp_transmit_logical4_scalar
     module procedure mpp_transmit_logical4_2d
     module procedure mpp_transmit_logical4_3d
     module procedure mpp_transmit_logical4_4d
     module procedure mpp_transmit_logical4_5d
  end interface
  interface mpp_recv
     module procedure mpp_recv_real8
     module procedure mpp_recv_real8_scalar
     module procedure mpp_recv_real8_2d
     module procedure mpp_recv_real8_3d
     module procedure mpp_recv_real8_4d
     module procedure mpp_recv_real8_5d


     module procedure mpp_recv_int8
     module procedure mpp_recv_int8_scalar
     module procedure mpp_recv_int8_2d
     module procedure mpp_recv_int8_3d
     module procedure mpp_recv_int8_4d
     module procedure mpp_recv_int8_5d
     module procedure mpp_recv_logical8
     module procedure mpp_recv_logical8_scalar
     module procedure mpp_recv_logical8_2d
     module procedure mpp_recv_logical8_3d
     module procedure mpp_recv_logical8_4d
     module procedure mpp_recv_logical8_5d



     module procedure mpp_recv_int4
     module procedure mpp_recv_int4_scalar
     module procedure mpp_recv_int4_2d
     module procedure mpp_recv_int4_3d
     module procedure mpp_recv_int4_4d
     module procedure mpp_recv_int4_5d
     module procedure mpp_recv_logical4
     module procedure mpp_recv_logical4_scalar
     module procedure mpp_recv_logical4_2d
     module procedure mpp_recv_logical4_3d
     module procedure mpp_recv_logical4_4d
     module procedure mpp_recv_logical4_5d
  end interface
  interface mpp_send
     module procedure mpp_send_real8
     module procedure mpp_send_real8_scalar
     module procedure mpp_send_real8_2d
     module procedure mpp_send_real8_3d
     module procedure mpp_send_real8_4d
     module procedure mpp_send_real8_5d


     module procedure mpp_send_int8
     module procedure mpp_send_int8_scalar
     module procedure mpp_send_int8_2d
     module procedure mpp_send_int8_3d
     module procedure mpp_send_int8_4d
     module procedure mpp_send_int8_5d
     module procedure mpp_send_logical8
     module procedure mpp_send_logical8_scalar
     module procedure mpp_send_logical8_2d
     module procedure mpp_send_logical8_3d
     module procedure mpp_send_logical8_4d
     module procedure mpp_send_logical8_5d



     module procedure mpp_send_int4
     module procedure mpp_send_int4_scalar
     module procedure mpp_send_int4_2d
     module procedure mpp_send_int4_3d
     module procedure mpp_send_int4_4d
     module procedure mpp_send_int4_5d
     module procedure mpp_send_logical4
     module procedure mpp_send_logical4_scalar
     module procedure mpp_send_logical4_2d
     module procedure mpp_send_logical4_3d
     module procedure mpp_send_logical4_4d
     module procedure mpp_send_logical4_5d
  end interface

! <INTERFACE NAME="mpp_broadcast">

!   <OVERVIEW>
!     Parallel broadcasts.
!   </OVERVIEW>
!   <DESCRIPTION>
!     The <TT>mpp_broadcast</TT> call has been added because the original
!     syntax (using <TT>ALL_PES</TT> in <TT>mpp_transmit</TT>) did not
!     support a broadcast across a pelist.
!
!     <TT>MPP_TYPE_</TT> corresponds to any 4-byte and 8-byte variant of
!     <TT>integer, real, complex, logical</TT> variables, of rank 0 or 1. A
!     contiguous block from a multi-dimensional array may be passed by its
!     starting address and its length, as in <TT>f77</TT>.
!
!     Global broadcasts through the <TT>ALL_PES</TT> argument to <LINK
!     SRC="#mpp_transmit"><TT>mpp_transmit</TT></LINK> are still provided for
!     backward-compatibility.
!
!     If <TT>pelist</TT> is omitted, the context is assumed to be the
!     current pelist. <TT>from_pe</TT> must belong to the current
!     pelist. This call implies synchronization across the PEs in
!     <TT>pelist</TT>, or the current pelist if <TT>pelist</TT> is absent.
!   </DESCRIPTION>
!   <TEMPLATE>
!     call mpp_broadcast( data, length, from_pe, pelist )
!   </TEMPLATE>
!   <IN NAME="length"> </IN>
!   <IN NAME="from_pe"> </IN>
!   <IN NAME="pelist"> </IN>
!   <INOUT NAME="data(*)"> </INOUT>
! </INTERFACE>
  interface mpp_broadcast
     module procedure mpp_broadcast_real8
     module procedure mpp_broadcast_real8_scalar
     module procedure mpp_broadcast_real8_2d
     module procedure mpp_broadcast_real8_3d
     module procedure mpp_broadcast_real8_4d
     module procedure mpp_broadcast_real8_5d


     module procedure mpp_broadcast_int8
     module procedure mpp_broadcast_int8_scalar
     module procedure mpp_broadcast_int8_2d
     module procedure mpp_broadcast_int8_3d
     module procedure mpp_broadcast_int8_4d
     module procedure mpp_broadcast_int8_5d
     module procedure mpp_broadcast_logical8
     module procedure mpp_broadcast_logical8_scalar
     module procedure mpp_broadcast_logical8_2d
     module procedure mpp_broadcast_logical8_3d
     module procedure mpp_broadcast_logical8_4d
     module procedure mpp_broadcast_logical8_5d



     module procedure mpp_broadcast_int4
     module procedure mpp_broadcast_int4_scalar
     module procedure mpp_broadcast_int4_2d
     module procedure mpp_broadcast_int4_3d
     module procedure mpp_broadcast_int4_4d
     module procedure mpp_broadcast_int4_5d
     module procedure mpp_broadcast_logical4
     module procedure mpp_broadcast_logical4_scalar
     module procedure mpp_broadcast_logical4_2d
     module procedure mpp_broadcast_logical4_3d
     module procedure mpp_broadcast_logical4_4d
     module procedure mpp_broadcast_logical4_5d
  end interface

!#####################################################################
! <INTERFACE NAME="mpp_chksum">

!   <OVERVIEW>
!     Parallel checksums.
!   </OVERVIEW>
!   <DESCRIPTION>
!     <TT>mpp_chksum</TT> is a parallel checksum routine that returns an
!     identical answer for the same array irrespective of how it has been
!     partitioned across processors. <TT>8</TT>is the <TT>KIND</TT>
!     parameter corresponding to long integers (see discussion on
!     OS-dependent preprocessor directives) defined in
!     the header file <TT>fms_platform.h</TT>. <TT>MPP_TYPE_</TT> corresponds to any
!     4-byte and 8-byte variant of <TT>integer, real, complex, logical</TT>
!     variables, of rank 0 to 5.
!
!     Integer checksums on FP data use the F90 <TT>TRANSFER()</TT>
!     intrinsic.
!
!     The <LINK SRC="http://www.gfdl.noaa.gov/fms-cgi-bin/cvsweb.cgi/FMS/shared/chksum/chksum.html">serial checksum module</LINK> is superseded
!     by this function, and is no longer being actively maintained. This
!     provides identical results on a single-processor job, and to perform
!     serial checksums on a single processor of a parallel job, you only
!     need to use the optional <TT>pelist</TT> argument.
!     <PRE>
!     use mpp_mod
!     integer :: pe, chksum
!     real :: a(:)
!     pe = mpp_pe()
!     chksum = mpp_chksum( a, (/pe/) )
!     </PRE>
!
!     The additional functionality of <TT>mpp_chksum</TT> over
!     serial checksums is to compute the checksum across the PEs in
!     <TT>pelist</TT>. The answer is guaranteed to be the same for
!     the same distributed array irrespective of how it has been
!     partitioned.
!
!     If <TT>pelist</TT> is omitted, the context is assumed to be the
!     current pelist. This call implies synchronization across the PEs in
!     <TT>pelist</TT>, or the current pelist if <TT>pelist</TT> is absent.
!   </DESCRIPTION>
!   <TEMPLATE>
!     mpp_chksum( var, pelist )
!   </TEMPLATE>
!   <IN NAME="pelist" TYPE="integer" DIM="(:)"> </IN>
!   <IN NAME="var" TYPE="MPP_TYPE_"> </IN>
! </INTERFACE>
  interface mpp_chksum

     module procedure mpp_chksum_i8_1d
     module procedure mpp_chksum_i8_2d
     module procedure mpp_chksum_i8_3d
     module procedure mpp_chksum_i8_4d

     module procedure mpp_chksum_i4_1d
     module procedure mpp_chksum_i4_2d
     module procedure mpp_chksum_i4_3d
     module procedure mpp_chksum_i4_4d
     module procedure mpp_chksum_r8_0d
     module procedure mpp_chksum_r8_1d
     module procedure mpp_chksum_r8_2d
     module procedure mpp_chksum_r8_3d
     module procedure mpp_chksum_r8_4d
     module procedure mpp_chksum_r8_5d



  end interface

!***********************************************************************
!
!            module variables
!
!***********************************************************************
  type(communicator),save :: peset(0:PESET_MAX) !0 is a dummy used to hold single-PE "self" communicator
  logical              :: module_is_initialized = .false.
  logical              :: debug = .false.
  integer              :: npes=1, root_pe=0, pe=0
  integer(8)   :: tick, ticks_per_sec, max_ticks, start_tick, end_tick, tick0=0
  integer              :: mpp_comm_private, ocean_comm_private
  logical              :: first_call_system_clock_mpi=.TRUE.
  real(8)    :: mpi_count0=0  ! use to prevent integer overflow
  real(8)    :: mpi_tick_rate=0.d0  ! clock rate for mpi_wtick()
  logical              :: mpp_record_timing_data=.TRUE.
  type(clock),save     :: clocks(MAX_CLOCKS)
  integer              :: log_unit, etc_unit
  character(len=32)    :: configfile='logfile'
  integer              :: peset_num=0, current_peset_num=0
  integer              :: world_peset_num                  !the world communicator
  integer              :: ocean_peset_num                  !the ocean communicator
  integer              :: error
  integer              :: clock_num=0, num_clock_ids=0,current_clock=0, previous_clock(MAX_CLOCKS)=0
  real                 :: tick_rate
  integer, allocatable, target :: request(:), request_global(:)
  integer, allocatable, target :: request_recv(:), request_recv_global(:)
! if you want to save the non-root PE information uncomment out the following line
! and comment out the assigment of etcfile to '/dev/null'

  character(len=32)    :: etcfile='/dev/null'



  integer :: in_unit=5, out_unit=6, err_unit=0


!--- variables used in mpp_util.h
  type(Summary_Struct) :: clock_summary(MAX_CLOCKS)
  logical              :: warnings_are_fatal = .FALSE.
  integer              :: error_state=0
  integer              :: clock_grain=CLOCK_LOOP-1

!--- variables used in mpp_comm.h





  integer            :: clock0    !measures total runtime from mpp_init to mpp_exit
  integer            :: mpp_stack_size=0, mpp_stack_hwm=0
  integer            :: tag=1
  logical            :: verbose=.FALSE.



  character(len=128), public :: version= &
       '$Id mpp.F90 $'
  character(len=128), public :: tagname= &
       '$Name: mom4p1_pubrel_dec2009_nnz $'

  contains



!#######################################################################
subroutine system_clock_mpi( count, count_rate, count_max )
! There can be one ONE baseline count0 and this routine is
! included in multiple places.
!mimics F90 system_clock_mpi intrinsic
      integer(8), intent(out), optional :: count, count_rate, count_max
!count must return a number between 0 and count_max
      integer(8), parameter :: maxtick=HUGE(count_max)
      if(first_call_system_clock_mpi)then
         first_call_system_clock_mpi=.false.
         mpi_count0 = MPI_WTime()
         mpi_tick_rate = 1.d0/MPI_WTick()
      endif
      if( PRESENT(count) )then
          count = (MPI_WTime()-mpi_count0)*mpi_tick_rate
      end if
      if( PRESENT(count_rate) )then
          count_rate = mpi_tick_rate
      end if
      if( PRESENT(count_max) )then
          count_max = maxtick-1
      end if
      return
    end subroutine system_clock_mpi



! -*-f90-*-
! $Id: mpp_util.inc,v 17.0.2.1.2.2 2009/10/08 23:34:41 wfc Exp $



! -*-f90-*-
! $Id: mpp_util_mpi.inc,v 17.0.2.1.2.1 2009/09/08 14:20:56 nnz Exp $

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!         MISCELLANEOUS UTILITIES: mpp_error                                  !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

subroutine mpp_error_basic( errortype, errormsg )
!a very basic error handler
!uses ABORT and FLUSH calls, may need to use cpp to rename
  integer,                    intent(in) :: errortype
  character(len=*), intent(in), optional :: errormsg
  character(len=256)                     :: text
  logical                                :: opened
  integer                                :: istat, out_unit

  if( .NOT.module_is_initialized )call ABORT()

  select case( errortype )
  case(NOTE)
     text = 'NOTE'         !just FYI
  case(WARNING)
     text = 'WARNING'      !probable error
  case(FATAL)
     text = 'FATAL'        !fatal error
  case default
     text = 'WARNING: non-existent errortype (must be NOTE|WARNING|FATAL)'
  end select

  if( npes.GT.1 )write( text,'(a,i5)' )trim(text)//' from PE', pe   !this is the mpp part
  if( PRESENT(errormsg) )text = trim(text)//': '//trim(errormsg)

  out_unit = stdout()
  select case( errortype )
  case(NOTE)
     write( out_unit,'(a)' )trim(text)
  case default

     write( stderr(), '(/a/)' )trim(text)

     write( out_unit,'(/a/)' )trim(text)
     if( errortype.EQ.FATAL .OR. warnings_are_fatal )then
        call FLUSH(out_unit)

        call MPI_ABORT( MPI_COMM_WORLD, 1, error )
     end if
  end select

  error_state = errortype
  return
end subroutine mpp_error_basic

!#####################################################################
!--- makes a PE set out of a PE list. A PE list is an ordered list of PEs
!--- a PE set is a triad (start,log2stride,size) for SHMEM, an a communicator for MPI
!--- if stride is non-uniform or not a power of 2,
!--- will return error (not required for MPI but enforced for uniformity)
function get_peset(pelist)
  integer                       :: get_peset
  integer, intent(in), optional :: pelist(:)
  integer                       :: group
  integer                       :: i, n, stride, l
  integer,          allocatable :: sorted(:)
  character(len=128) :: text

  if( .NOT.PRESENT(pelist) )then !set it to current_peset_num
     get_peset = current_peset_num; return
  end if
  if( size(pelist(:)).EQ.1 .AND. npes.GT.1 )then    !collective ops on single PEs should return
     get_peset = 0; return
  end if

!--- first make sure pelist is monotonically increasing.
!print *, 'In mpp_inc'
!print *, pelist
  do n = 2, size(pelist(:))
     if(pelist(n) <= pelist(n-1)) call mpp_error(FATAL, "GET_PESET: pelist is not monotonically increasing")
  enddo

  allocate( sorted(size(pelist(:))) )
  sorted = pelist 
  if( debug )write( stderr(),* )'pelist=', pelist

!find if this array matches any existing peset
!  do i = 1,peset_num
  do i = 0,peset_num !<-- esm insertion
     if( debug )write( stderr(),'(a,3i6)' )'pe, i, peset_num=', pe, i, peset_num
     if( size(sorted(:)).EQ.size(peset(i)%list(:)) )then
        if( ALL(sorted.EQ.peset(i)%list) )then
           deallocate(sorted)
           get_peset = i; return
        end if
     end if
  end do
!not found, so create new peset
  peset_num = peset_num + 1
  if( peset_num.GE.PESET_MAX )call mpp_error( FATAL, 'GET_PESET: number of PE sets exceeds PESET_MAX.' )
  i = peset_num             !shorthand
!create list
  allocate( peset(i)%list(size(sorted(:))) )
  peset(i)%list(:) = sorted(:)
  peset(i)%count = size(sorted(:))

  call MPI_GROUP_INCL( peset(current_peset_num)%group, size(sorted(:)), sorted, peset(i)%group, error )
  call MPI_COMM_CREATE( peset(current_peset_num)%id, peset(i)%group, peset(i)%id, error )

  deallocate(sorted)
  get_peset = i

  return

end function get_peset

!#######################################################################
!synchronize PEs in list
subroutine mpp_sync( pelist, do_self )
  integer, intent(in), optional :: pelist(:)
  logical, intent(in), optional :: do_self
  logical                       :: dself
  integer                       :: n

  dself=.true.; if(PRESENT(do_self))dself=do_self
  if(dself)call mpp_sync_self(pelist)

  n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

  if( current_clock.NE.0 )call system_clock_mpi(start_tick)

  call MPI_BARRIER( peset(n)%id, error )


  if( current_clock.NE.0 )call increment_current_clock(EVENT_WAIT)

  return
end subroutine mpp_sync

!#######################################################################
!this is to check if current PE's outstanding puts are complete
!but we can't use shmem_fence because we are actually waiting for
!a remote PE to complete its get
subroutine mpp_sync_self( pelist, check )
  integer, intent(in), optional :: pelist(:)
  integer, intent(in), optional :: check

  integer                       :: i, m, n, stride, my_check

  n = get_peset(pelist)

  if( current_clock.NE.0 )call system_clock_mpi(start_tick)

  my_check = EVENT_SEND
  if(present(check)) my_check = check
  select case(my_check)
  case(EVENT_SEND)
     do m = 1,peset(n)%count
        i = peset(n)%list(m)
        if( request(i).NE.MPI_REQUEST_NULL )call MPI_WAIT( request(i), stat, error )
     end do
  case(EVENT_RECV)
     do m = 1,peset(n)%count
        i = peset(n)%list(m)
        if( request_recv(i).NE.MPI_REQUEST_NULL )call MPI_WAIT( request_recv(i), stat, error )
     end do    
  case default
     call mpp_error( FATAL, 'mpp_sync_self: The value of optional argument check should be EVENT_SEND or EVENT_RECV')
  end select 
  if( current_clock.NE.0 )call increment_current_clock(EVENT_WAIT)
  return
end subroutine mpp_sync_self



!#####################################################################
! <FUNCTION NAME="stdin">
!  <OVERVIEW>
!    Standard fortran unit numbers.
!  </OVERVIEW>
!  <DESCRIPTION>
!    This function returns the current standard fortran unit numbers for input.
!  </DESCRIPTION>
!  <TEMPLATE>
!   stdin()
!  </TEMPLATE>
! </FUNCTION>
  function stdin()
    integer :: stdin
    stdin = in_unit
    return
  end function stdin

!#####################################################################
! <FUNCTION NAME="stdout">
!  <OVERVIEW>
!    Standard fortran unit numbers.
!  </OVERVIEW>
!  <DESCRIPTION>
!    This function returns the current  standard fortran unit numbers for output.
!  </DESCRIPTION>
!  <TEMPLATE>
!   stdout()
!  </TEMPLATE>
! </FUNCTION>
  function stdout()
    integer :: stdout
    stdout = out_unit
    if( pe.NE.root_pe )stdout = stdlog()
    return
  end function stdout

!#####################################################################
! <FUNCTION NAME="stderr">
!  <OVERVIEW>
!    Standard fortran unit numbers.
!  </OVERVIEW>
!  <DESCRIPTION>
!    This function returns the current standard fortran unit numbers for error messages.
!  </DESCRIPTION>
!  <TEMPLATE>
!   stderr()
!  </TEMPLATE>
! </FUNCTION>
  function stderr()
    integer :: stderr
    stderr = err_unit
    return
  end function stderr

!#####################################################################
! <FUNCTION NAME="stdlog">
!  <OVERVIEW>
!    Standard fortran unit numbers.
!  </OVERVIEW>
!  <DESCRIPTION>
!    This function returns the current  standard fortran unit numbers for log messages.
!    Log messages, by convention, are written to the file <TT>logfile.out</TT>.
!  </DESCRIPTION>
!  <TEMPLATE>
!    stdlog()
!  </TEMPLATE>
! </FUNCTION>
  function stdlog()
    integer :: stdlog,istat
    logical :: opened
    character(len=11) :: this_pe
    if( pe.EQ.root_pe )then
       write(this_pe,'(a,i6.6,a)') '.',pe,'.out'
       inquire( file=trim(configfile)//this_pe, opened=opened )
       if( opened )then
          call FLUSH(log_unit)
       else
          log_unit=get_unit()
          open( unit=log_unit, status='UNKNOWN', file=trim(configfile)//this_pe, position='APPEND', err=10 )
       end if
       stdlog = log_unit
    else
       inquire( unit=etc_unit, opened=opened )
       if( opened )then
          call FLUSH(etc_unit)
       else
          open( unit=etc_unit, status='UNKNOWN', file=trim(etcfile), position='APPEND', err=11 )
       end if
       stdlog = etc_unit
    end if
    return
10  call mpp_error( FATAL, 'STDLOG: unable to open '//trim(configfile)//this_pe//'.' )
11  call mpp_error( FATAL, 'STDLOG: unable to open '//trim(etcfile)//'.' )
  end function stdlog

!#####################################################################
  subroutine mpp_init_logfile()
  integer :: p
  logical :: exist
  character(len=11) :: this_pe
  if( pe.EQ.root_pe )then
     log_unit = get_unit()
     do p=0,npes-1
       write(this_pe,'(a,i6.6,a)') '.',p,'.out'
       inquire( file=trim(configfile)//this_pe, exist=exist )
       if(exist)then
         open( unit=log_unit, file=trim(configfile)//this_pe, status='REPLACE' )
         close(log_unit)
       endif
     end do
  end if
  end subroutine mpp_init_logfile
!#####################################################################
  subroutine mpp_set_warn_level(flag)
    integer, intent(in) :: flag

    if( flag.EQ.WARNING )then
       warnings_are_fatal = .FALSE.
    else if( flag.EQ.FATAL )then
       warnings_are_fatal = .TRUE.
    else
       call mpp_error( FATAL, 'MPP_SET_WARN_LEVEL: warning flag must be set to WARNING or FATAL.' )
    end if
    return
  end subroutine mpp_set_warn_level

!#####################################################################
  function mpp_error_state()
    integer :: mpp_error_state
    mpp_error_state = error_state
    return
  end function mpp_error_state

!#####################################################################
!overloads to mpp_error_basic
!support for error_mesg routine in FMS
subroutine mpp_error_mesg( routine, errormsg, errortype )
  character(len=*), intent(in) :: routine, errormsg
  integer,          intent(in) :: errortype

  call mpp_error( errortype, trim(routine)//': '//trim(errormsg) )
  return
end subroutine mpp_error_mesg

!#####################################################################
subroutine mpp_error_noargs()
  call mpp_error(FATAL)
end subroutine mpp_error_noargs

!#####################################################################
subroutine mpp_error_Is(errortype, errormsg1, value, errormsg2)
  integer,          intent(in) :: errortype
  INTEGER,          intent(in) :: value
  character(len=*), intent(in) :: errormsg1
  character(len=*),      intent(in), optional :: errormsg2
  call mpp_error( errortype, errormsg1, (/value/), errormsg2)
end subroutine mpp_error_Is
!#####################################################################
subroutine mpp_error_Rs(errortype, errormsg1, value, errormsg2)
  integer,          intent(in) :: errortype
  REAL,             intent(in) :: value
  character(len=*), intent(in) :: errormsg1
  character(len=*),      intent(in), optional :: errormsg2
  call mpp_error( errortype, errormsg1, (/value/), errormsg2)
end subroutine mpp_error_Rs
!#####################################################################
subroutine mpp_error_Ia(errortype, errormsg1, array, errormsg2)
  integer,               intent(in) :: errortype
  INTEGER, dimension(:), intent(in) :: array
  character(len=*),      intent(in) :: errormsg1
  character(len=*),      intent(in), optional :: errormsg2
  character(len=512) :: string

  string = errormsg1//trim(array_to_char(array))
  if(present(errormsg2)) string = trim(string)//errormsg2
  call mpp_error_basic( errortype, trim(string))

end subroutine mpp_error_Ia

!#####################################################################
subroutine mpp_error_Ra(errortype, errormsg1, array, errormsg2)
  integer,            intent(in) :: errortype
  REAL, dimension(:), intent(in) :: array
  character(len=*),      intent(in) :: errormsg1
  character(len=*),   intent(in), optional :: errormsg2
  character(len=512) :: string

  string = errormsg1//trim(array_to_char(array))
  if(present(errormsg2)) string = trim(string)//errormsg2
  call mpp_error_basic( errortype, trim(string))

end subroutine mpp_error_Ra

!#####################################################################



subroutine mpp_error_ia_ia(errortype, errormsg1, array1, errormsg2, array2, errormsg3)
  integer,            intent(in) :: errortype
  integer, dimension(:), intent(in) :: array1
  integer, dimension(:), intent(in) :: array2
  character(len=*),      intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3
  character(len=512) :: string

  string = errormsg1//trim(array_to_char(array1)) 
  string = trim(string)//errormsg2//trim(array_to_char(array2)) 
  if(present(errormsg3)) string = trim(string)//errormsg3
  call mpp_error_basic( errortype, trim(string))

end subroutine mpp_error_ia_ia



!#####################################################################



subroutine mpp_error_ia_ra(errortype, errormsg1, array1, errormsg2, array2, errormsg3)
  integer,            intent(in) :: errortype
  integer, dimension(:), intent(in) :: array1
  real, dimension(:), intent(in) :: array2
  character(len=*),      intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3
  character(len=512) :: string

  string = errormsg1//trim(array_to_char(array1)) 
  string = trim(string)//errormsg2//trim(array_to_char(array2)) 
  if(present(errormsg3)) string = trim(string)//errormsg3
  call mpp_error_basic( errortype, trim(string))

end subroutine mpp_error_ia_ra



!#####################################################################



subroutine mpp_error_ra_ia(errortype, errormsg1, array1, errormsg2, array2, errormsg3)
  integer,            intent(in) :: errortype
  real, dimension(:), intent(in) :: array1
  integer, dimension(:), intent(in) :: array2
  character(len=*),      intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3
  character(len=512) :: string

  string = errormsg1//trim(array_to_char(array1)) 
  string = trim(string)//errormsg2//trim(array_to_char(array2)) 
  if(present(errormsg3)) string = trim(string)//errormsg3
  call mpp_error_basic( errortype, trim(string))

end subroutine mpp_error_ra_ia



!#####################################################################



subroutine mpp_error_ra_ra(errortype, errormsg1, array1, errormsg2, array2, errormsg3)
  integer,            intent(in) :: errortype
  real, dimension(:), intent(in) :: array1
  real, dimension(:), intent(in) :: array2
  character(len=*),      intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3
  character(len=512) :: string

  string = errormsg1//trim(array_to_char(array1)) 
  string = trim(string)//errormsg2//trim(array_to_char(array2)) 
  if(present(errormsg3)) string = trim(string)//errormsg3
  call mpp_error_basic( errortype, trim(string))

end subroutine mpp_error_ra_ra



!#####################################################################



subroutine mpp_error_ia_is(errortype, errormsg1, array, errormsg2, scalar, errormsg3)
  integer,            intent(in) :: errortype
  integer, dimension(:), intent(in) :: array
  integer,               intent(in) :: scalar
  character(len=*),   intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3

  call mpp_error( errortype, errormsg1, array, errormsg2, (/scalar/), errormsg3)

end subroutine mpp_error_ia_is



!#####################################################################



subroutine mpp_error_ia_rs(errortype, errormsg1, array, errormsg2, scalar, errormsg3)
  integer,            intent(in) :: errortype
  integer, dimension(:), intent(in) :: array
  real,               intent(in) :: scalar
  character(len=*),   intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3

  call mpp_error( errortype, errormsg1, array, errormsg2, (/scalar/), errormsg3)

end subroutine mpp_error_ia_rs



!#####################################################################



subroutine mpp_error_ra_is(errortype, errormsg1, array, errormsg2, scalar, errormsg3)
  integer,            intent(in) :: errortype
  real, dimension(:), intent(in) :: array
  integer,               intent(in) :: scalar
  character(len=*),   intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3

  call mpp_error( errortype, errormsg1, array, errormsg2, (/scalar/), errormsg3)

end subroutine mpp_error_ra_is



!#####################################################################



subroutine mpp_error_ra_rs(errortype, errormsg1, array, errormsg2, scalar, errormsg3)
  integer,            intent(in) :: errortype
  real, dimension(:), intent(in) :: array
  real,               intent(in) :: scalar
  character(len=*),   intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3

  call mpp_error( errortype, errormsg1, array, errormsg2, (/scalar/), errormsg3)

end subroutine mpp_error_ra_rs



!#####################################################################



subroutine mpp_error_is_ia(errortype, errormsg1, scalar2, errormsg2, array2, errormsg3)
  integer,            intent(in) :: errortype
  integer,               intent(in) :: scalar2
  integer, dimension(:), intent(in) :: array2
  character(len=*),   intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3

  call mpp_error( errortype, errormsg1, (/scalar2/), errormsg2, array2, errormsg3)

end subroutine mpp_error_is_ia



!#####################################################################



subroutine mpp_error_is_ra(errortype, errormsg1, scalar2, errormsg2, array2, errormsg3)
  integer,            intent(in) :: errortype
  integer,               intent(in) :: scalar2
  real, dimension(:), intent(in) :: array2
  character(len=*),   intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3

  call mpp_error( errortype, errormsg1, (/scalar2/), errormsg2, array2, errormsg3)

end subroutine mpp_error_is_ra



!#####################################################################



subroutine mpp_error_rs_ia(errortype, errormsg1, scalar2, errormsg2, array2, errormsg3)
  integer,            intent(in) :: errortype
  real,               intent(in) :: scalar2
  integer, dimension(:), intent(in) :: array2
  character(len=*),   intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3

  call mpp_error( errortype, errormsg1, (/scalar2/), errormsg2, array2, errormsg3)

end subroutine mpp_error_rs_ia



!#####################################################################



subroutine mpp_error_rs_ra(errortype, errormsg1, scalar2, errormsg2, array2, errormsg3)
  integer,            intent(in) :: errortype
  real,               intent(in) :: scalar2
  real, dimension(:), intent(in) :: array2
  character(len=*),   intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3

  call mpp_error( errortype, errormsg1, (/scalar2/), errormsg2, array2, errormsg3)

end subroutine mpp_error_rs_ra



!#####################################################################



subroutine mpp_error_is_is(errortype, errormsg1, scalar1, errormsg2, scalar2, errormsg3)
  integer,            intent(in) :: errortype
  integer, intent(in) :: scalar1
  integer, intent(in) :: scalar2
  character(len=*),   intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3

  call mpp_error( errortype, errormsg1, (/scalar1/), errormsg2, (/scalar2/), errormsg3)

end subroutine mpp_error_is_is



!#####################################################################



subroutine mpp_error_is_rs(errortype, errormsg1, scalar1, errormsg2, scalar2, errormsg3)
  integer,            intent(in) :: errortype
  integer, intent(in) :: scalar1
  real, intent(in) :: scalar2
  character(len=*),   intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3

  call mpp_error( errortype, errormsg1, (/scalar1/), errormsg2, (/scalar2/), errormsg3)

end subroutine mpp_error_is_rs



!#####################################################################



subroutine mpp_error_rs_is(errortype, errormsg1, scalar1, errormsg2, scalar2, errormsg3)
  integer,            intent(in) :: errortype
  real, intent(in) :: scalar1
  integer, intent(in) :: scalar2
  character(len=*),   intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3

  call mpp_error( errortype, errormsg1, (/scalar1/), errormsg2, (/scalar2/), errormsg3)

end subroutine mpp_error_rs_is



!#####################################################################



subroutine mpp_error_rs_rs(errortype, errormsg1, scalar1, errormsg2, scalar2, errormsg3)
  integer,            intent(in) :: errortype
  real, intent(in) :: scalar1
  real, intent(in) :: scalar2
  character(len=*),   intent(in) :: errormsg1, errormsg2
  character(len=*),   intent(in), optional :: errormsg3

  call mpp_error( errortype, errormsg1, (/scalar1/), errormsg2, (/scalar2/), errormsg3)

end subroutine mpp_error_rs_rs



!#####################################################################
function iarray_to_char(iarray) result(string)
integer, intent(in) :: iarray(:)
character(len=256) :: string
character(len=32)  :: chtmp
integer :: i, len_tmp, len_string

 string = ''
 do i=1,size(iarray)
   write(chtmp,'(i16)') iarray(i)
   chtmp = adjustl(chtmp)
   len_tmp = len_trim(chtmp)
   len_string  = len_trim(string)
   string(len_string+1:len_string+len_tmp) = trim(chtmp)
   string(len_string+len_tmp+1:len_string+len_tmp+1) = ','
 enddo
 len_string = len_trim(string)
 string(len_string:len_string) = ' ' ! remove trailing comma

end function iarray_to_char
!#####################################################################
function rarray_to_char(rarray) result(string)
real, intent(in) :: rarray(:)
character(len=256) :: string
character(len=32)  :: chtmp
integer :: i, len_tmp, len_string

 string = ''
 do i=1,size(rarray)
   write(chtmp,'(G16.9)') rarray(i)
   chtmp = adjustl(chtmp)
   len_tmp = len_trim(chtmp)
   len_string  = len_trim(string)
   string(len_string+1:len_string+len_tmp) = trim(chtmp)
   string(len_string+len_tmp+1:len_string+len_tmp+1) = ','
 enddo
 len_string = len_trim(string)
 string(len_string:len_string) = ' ' ! remove trailing comma

end function rarray_to_char

!#####################################################################
! <FUNCTION NAME="mpp_pe">
!  <OVERVIEW>
!    Returns processor ID.
!  </OVERVIEW>
!  <DESCRIPTION>
!    This returns the unique ID associated with a PE. This number runs
!    between 0 and <TT>npes-1</TT>, where <TT>npes</TT> is the total
!    processor count, returned by <TT>mpp_npes</TT>. For a uniprocessor
!    application this will always return 0.
!  </DESCRIPTION>
!  <TEMPLATE>
!    mpp_pe()
!  </TEMPLATE>
! </FUNCTION>
  function mpp_pe()
    integer :: mpp_pe

    if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_PE: You must first call mpp_init.' )
    mpp_pe = pe
    return
  end function mpp_pe

!#####################################################################
  function mpp_node()
!calls mld_id from threadloc.c on sgi, which returns the hardware node ID from /hw/nodenum/...
    integer :: mpp_node
    integer :: mld_id

    if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_NODE: You must first call mpp_init.' )
    mpp_node = mld_id()
    return
  end function mpp_node

!#####################################################################
! <FUNCTION NAME="mpp_npes">
!  <OVERVIEW>
!    Returns processor count for current pelist.
!  </OVERVIEW>
!  <DESCRIPTION>
!    This returns the number of PEs in the current pelist. For a
!    uniprocessor application, this will always return 1.
!  </DESCRIPTION>
!  <TEMPLATE>
!    mpp_npes()
!  </TEMPLATE>
! </FUNCTION>
  function mpp_npes()
    integer :: mpp_npes

    if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_NPES: You must first call mpp_init.' )
    mpp_npes = size(peset(current_peset_num)%list(:))
    return
  end function mpp_npes

!#####################################################################
  function mpp_root_pe()
    integer :: mpp_root_pe

    if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_ROOT_PE: You must first call mpp_init.' )
    mpp_root_pe = root_pe
    return
  end function mpp_root_pe

!#####################################################################
  subroutine mpp_set_root_pe(num)
    integer, intent(in) :: num
    logical             :: opened

    if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_SET_ROOT_PE: You must first call mpp_init.' )
    if( .NOT.(ANY(num.EQ.peset(current_peset_num)%list(:))) ) &
         call mpp_error( FATAL, 'MPP_SET_ROOT_PE: you cannot set a root PE outside the current pelist.' )
!actions to take if root_pe has changed:
! open log_unit on new root_pe, close it on old root_pe and point its log_unit to stdout.
!      if( num.NE.root_pe )then  !root_pe has changed
!          if( pe.EQ.num )then
!on the new root_pe
!              if( log_unit.NE.out_unit )then
!                  inquire( unit=log_unit, opened=opened )
!                  if( .NOT.opened )open( unit=log_unit, status='OLD', file=trim(configfile), position='APPEND' )
!              end if
!          else if( pe.EQ.root_pe )then
!on the old root_pe
!              if( log_unit.NE.out_unit )then
!                  inquire( unit=log_unit, opened=opened )
!                  if( opened )close(log_unit)
!                  log_unit = out_unit
!              end if
!          end if
!      end if
    root_pe = num
    return
  end subroutine mpp_set_root_pe

!#####################################################################
! <SUBROUTINE NAME="mpp_declare_pelist">
!  <OVERVIEW>
!    Declare a pelist.
!  </OVERVIEW>
!  <DESCRIPTION>
!    This call is written specifically to accommodate a MPI restriction
!    that requires a parent communicator to create a child communicator, In
!    other words: a pelist cannot go off and declare a communicator, but
!    every PE in the parent, including those not in pelist(:), must get
!    together for the <TT>MPI_COMM_CREATE</TT> call. The parent is
!    typically <TT>MPI_COMM_WORLD</TT>, though it could also be a subset
!    that includes all PEs in <TT>pelist</TT>.
!
!    The restriction does not apply to SMA but to have uniform code, you
!    may as well call it.
!
!    This call implies synchronization across the PEs in the current
!    pelist, of which <TT>pelist</TT> is a subset.
!  </DESCRIPTION>
!  <TEMPLATE>
!   call mpp_declare_pelist( pelist,name )
!  </TEMPLATE>
!  <IN NAME="pelist" DIM="(:)" TYPE="integer"></IN>
! </SUBROUTINE>

  subroutine mpp_declare_pelist( pelist, name )
    integer,                    intent(in) :: pelist(:)
    character(len=*), intent(in), optional :: name
    integer :: i

    if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_DECLARE_PELIST: You must first call mpp_init.' )
    i = get_peset(pelist)
    write( peset(i)%name,'(a,i2.2)' ) 'PElist', i !default name
    if( PRESENT(name) )peset(i)%name = name
    return
  end subroutine mpp_declare_pelist

!#####################################################################
! <SUBROUTINE NAME="mpp_set_current_pelist">
!  <OVERVIEW>
!    Set context pelist.
!  </OVERVIEW>
!  <DESCRIPTION>
!    This call sets the value of the current pelist, which is the
!    context for all subsequent "global" calls where the optional
!    <TT>pelist</TT> argument is omitted. All the PEs that are to be in the
!    current pelist must call it.
!
!    In MPI, this call may hang unless <TT>pelist</TT> has been previous
!    declared using <LINK
!    SRC="#mpp_declare_pelist"><TT>mpp_declare_pelist</TT></LINK>.
!
!    If the argument <TT>pelist</TT> is absent, the current pelist is
!    set to the "world" pelist, of all PEs in the job.
!  </DESCRIPTION>
!  <TEMPLATE>
!    call mpp_set_current_pelist( pelist )
!  </TEMPLATE>
!  <IN NAME="pliest" TYPE="integer"></IN>
! </SUBROUTINE>

!  subroutine mpp_set_current_pelist( pelist, no_sync )
  subroutine mpp_set_current_pelist( pelist, no_sync,pesetnum ) !<-- esm insertion
!Once we branch off into a PE subset, we want subsequent "global" calls to
!sync only across this subset. This is declared as the current pelist (peset(current_peset_num)%list)
!when current_peset all pelist ops with no pelist should apply the current pelist.
!also, we set the start PE in this pelist to be the root_pe.
!unlike mpp_declare_pelist, this is called by the PEs in the pelist only
!so if the PEset has not been previously declared, this will hang in MPI.
!if pelist is omitted, we reset pelist to the world pelist.
    integer, intent(in), optional :: pelist(:)
    logical, intent(in), optional :: no_sync
!-->esm insertion
    integer, intent(in), optional :: pesetnum
!<-- esm insertion

    if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_SET_CURRENT_PELIST: You must first call mpp_init.' )
    if( PRESENT(pelist) )then
    if( .NOT.ANY(pe.EQ.pelist) )call mpp_error( FATAL, 'MPP_SET_CURRENT_PELIST: pe must be in pelist.' )
       current_peset_num = get_peset(pelist)
    else
       current_peset_num=ocean_peset_num
!--> esm insertion never set pelist for comm_WORLD,set them to ocean_comm
       if( present(pesetnum)) current_peset_num = world_peset_num
!<-- esm insertion
    end if
    
    if(current_peset_num.ne.world_peset_num) &
    		call mpp_set_root_pe( MINVAL(peset(current_peset_num)%list(:)) )
    if(.not.PRESENT(no_sync))call mpp_sync()  !this is called to make sure everyone in the current pelist is here.
    return
  end subroutine mpp_set_current_pelist

!#####################################################################
!this is created for use by mpp_define_domains within a pelist
!will be published but not publicized
  subroutine mpp_get_current_pelist( pelist, name, commID )
    integer, intent(out) :: pelist(:)
    character(len=*), intent(out), optional :: name
    integer, intent(out), optional :: commID

    if( size(pelist(:)).NE.size(peset(current_peset_num)%list(:)) ) &
         call mpp_error( FATAL, 'MPP_GET_CURRENT_PELIST: size(pelist) is wrong.' )
    pelist(:) = peset(current_peset_num)%list(:)
    if( PRESENT(name) )name = peset(current_peset_num)%name

    if( PRESENT(commID) )commID = peset(current_peset_num)%id


    return
  end subroutine mpp_get_current_pelist

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                        PERFORMANCE PROFILING CALLS                          !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
! <SUBROUTINE NAME="mpp_clock_set_grain">
!  <OVERVIEW>
!    Set the level of granularity of timing measurements.
!  </OVERVIEW>
!  <DESCRIPTION>
!    This routine and three other routines, mpp_clock_id, mpp_clock_begin(id),
!    and mpp_clock_end(id) may be used to time parallel code sections, and
!    extract parallel statistics. Clocks are identified by names, which
!    should be unique in the first 32 characters. The <TT>mpp_clock_id</TT>
!    call initializes a clock of a given name and returns an integer
!    <TT>id</TT>. This <TT>id</TT> can be used by subsequent
!    <TT>mpp_clock_begin</TT> and <TT>mpp_clock_end</TT> calls set around a
!    code section to be timed. Example:
!    <PRE>
!    integer :: id
!    id = mpp_clock_id( 'Atmosphere' )
!    call mpp_clock_begin(id)
!    call atmos_model()
!    call mpp_clock_end()
!    </PRE>
!     Two flags may be used to alter the behaviour of
!     <TT>mpp_clock</TT>. If the flag <TT>MPP_CLOCK_SYNC</TT> is turned on
!     by <TT>mpp_clock_id</TT>, the clock calls <TT>mpp_sync</TT> across all
!     the PEs in the current pelist at the top of the timed code section,
!     but allows each PE to complete the code section (and reach
!     <TT>mpp_clock_end</TT>) at different times. This allows us to measure
!     load imbalance for a given code section. Statistics are written to
!     <TT>stdout</TT> by <TT>mpp_exit</TT>.
!
!     The flag <TT>MPP_CLOCK_DETAILED</TT> may be turned on by
!     <TT>mpp_clock_id</TT> to get detailed communication
!     profiles. Communication events of the types <TT>SEND, RECV, BROADCAST,
!     REDUCE</TT> and <TT>WAIT</TT> are separately measured for data volume
!     and time. Statistics are written to <TT>stdout</TT> by
!     <TT>mpp_exit</TT>, and individual PE info is also written to the file
!     <TT>mpp_clock.out.####</TT> where <TT>####</TT> is the PE id given by
!     <TT>mpp_pe</TT>.
!
!     The flags <TT>MPP_CLOCK_SYNC</TT> and <TT>MPP_CLOCK_DETAILED</TT> are
!     integer parameters available by use association, and may be summed to
!     turn them both on.
!
!     While the nesting of clocks is allowed, please note that turning on
!     the non-optional flags on inner clocks has certain subtle issues.
!     Turning on <TT>MPP_CLOCK_SYNC</TT> on an inner
!     clock may distort outer clock measurements of load imbalance. Turning
!     on <TT>MPP_CLOCK_DETAILED</TT> will stop detailed measurements on its
!     outer clock, since only one detailed clock may be active at one time.
!     Also, detailed clocks only time a certain number of events per clock
!     (currently 40000) to conserve memory. If this array overflows, a
!     warning message is printed, and subsequent events for this clock are
!     not timed.
!
!     Timings are done using the <TT>f90</TT> standard
!     <TT>system_clock_mpi</TT> intrinsic.
!
!     The resolution of system_clock_mpi is often too coarse for use except
!     across large swaths of code. On SGI systems this is transparently
!     overloaded with a higher resolution clock made available in a
!     non-portable fortran interface made available by
!     <TT>nsclock.c</TT>. This approach will eventually be extended to other
!     platforms.
!
!     New behaviour added at the Havana release allows the user to embed
!     profiling calls at varying levels of granularity all over the code,
!     and for any particular run, set a threshold of granularity so that
!     finer-grained clocks become dormant.
!
!     The threshold granularity is held in the private module variable
!     <TT>clock_grain</TT>. This value may be modified by the call
!     <TT>mpp_clock_set_grain</TT>, and affect clocks initiated by
!     subsequent calls to <TT>mpp_clock_id</TT>. The value of
!     <TT>clock_grain</TT> is set to an arbitrarily large number initially.
!
!     Clocks initialized by <TT>mpp_clock_id</TT> can set a new optional
!     argument <TT>grain</TT> setting their granularity level. Clocks check
!     this level against the current value of <TT>clock_grain</TT>, and are
!     only triggered if they are <I>at or below ("coarser than")</I> the
!     threshold. Finer-grained clocks are dormant for that run.
!
!The following grain levels are pre-defined:
!
!<pre>
!!predefined clock granularities, but you can use any integer
!!using CLOCK_LOOP and above may distort coarser-grain measurements
!  integer, parameter, public :: CLOCK_COMPONENT=1 !component level, e.g model, exchange
!  integer, parameter, public :: CLOCK_SUBCOMPONENT=11 !top level within a model component, e.g dynamics, physics
!  integer, parameter, public :: CLOCK_MODULE=21 !module level, e.g main subroutine of a physics module
!  integer, parameter, public :: CLOCK_ROUTINE=31 !level of individual subroutine or function
!  integer, parameter, public :: CLOCK_LOOP=41 !loops or blocks within a routine
!  integer, parameter, public :: CLOCK_INFRA=51 !infrastructure level, e.g halo update
!</pre>
!
!     Note that subsequent changes to <TT>clock_grain</TT> do not
!     change the status of already initiated clocks, and that if the
!     optional <TT>grain</TT> argument is absent, the clock is always
!     triggered. This guarantees backward compatibility.
!  </DESCRIPTION>
!  <TEMPLATE>
!     call mpp_clock_set_grain( grain )
!  </TEMPLATE>
!  <IN NAME="grain" TYPE="integer"></IN>
! </SUBROUTINE>

  subroutine mpp_clock_set_grain( grain )
    integer, intent(in) :: grain
!set the granularity of times: only clocks whose grain is lower than
!clock_grain are triggered, finer-grained clocks are dormant.
!clock_grain is initialized to CLOCK_LOOP, so all clocks above the loop level
!are triggered if this is never called.
    if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_CLOCK_SET_GRAIN: You must first call mpp_init.' )

    clock_grain = grain
    return
  end subroutine mpp_clock_set_grain

!#####################################################################
  subroutine clock_init( id, name, flags, grain )
    integer,           intent(in) :: id
    character(len=*),  intent(in) :: name
    integer, intent(in), optional :: flags, grain
    integer                       :: i

    clocks(id)%name = name
    clocks(id)%tick = 0
    clocks(id)%total_ticks = 0
    clocks(id)%sync_on_begin = .FALSE.
    clocks(id)%detailed      = .FALSE.
    clocks(id)%peset_num = current_peset_num
    if( PRESENT(flags) )then
       if( BTEST(flags,0) )clocks(id)%sync_on_begin = .TRUE.
       if( BTEST(flags,1) )clocks(id)%detailed      = .TRUE.
    end if
    clocks(id)%grain = 0
    if( PRESENT(grain) )clocks(id)%grain = grain
    if( clocks(id)%detailed )then
       allocate( clocks(id)%events(MAX_EVENT_TYPES) )
       clocks(id)%events(EVENT_ALLREDUCE)%name = 'ALLREDUCE'
       clocks(id)%events(EVENT_BROADCAST)%name = 'BROADCAST'
       clocks(id)%events(EVENT_RECV)%name = 'RECV'
       clocks(id)%events(EVENT_SEND)%name = 'SEND'
       clocks(id)%events(EVENT_WAIT)%name = 'WAIT'
       do i=1,MAX_EVENT_TYPES
          clocks(id)%events(i)%ticks(:) = 0
          clocks(id)%events(i)%bytes(:) = 0
          clocks(id)%events(i)%calls = 0
       end do
       clock_summary(id)%name = name
       clock_summary(id)%event(EVENT_ALLREDUCE)%name = 'ALLREDUCE'
       clock_summary(id)%event(EVENT_BROADCAST)%name = 'BROADCAST'
       clock_summary(id)%event(EVENT_RECV)%name = 'RECV'
       clock_summary(id)%event(EVENT_SEND)%name = 'SEND'
       clock_summary(id)%event(EVENT_WAIT)%name = 'WAIT'
       do i=1,MAX_EVENT_TYPES
          clock_summary(id)%event(i)%msg_size_sums(:) = 0.0
          clock_summary(id)%event(i)%msg_time_sums(:) = 0.0
          clock_summary(id)%event(i)%total_data = 0.0
          clock_summary(id)%event(i)%total_time = 0.0
          clock_summary(id)%event(i)%msg_size_cnts(:) = 0
          clock_summary(id)%event(i)%total_cnts = 0
       end do
    end if
    return
  end subroutine clock_init

!#####################################################################
!return an ID for a new or existing clock
  function mpp_clock_id( name, flags, grain )
    integer                       :: mpp_clock_id
    character(len=*),  intent(in) :: name
    integer, intent(in), optional :: flags, grain
    integer                       :: i

    if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_CLOCK_ID: You must first call mpp_init.')

!if grain is present, the clock is only triggered if it
!is low ("coarse") enough: compared to clock_grain
!finer-grained clocks are dormant.
!if grain is absent, clock is triggered.
    if( PRESENT(grain) )then
       if( grain.GT.clock_grain )then
          mpp_clock_id = 0
          return
       end if
    end if
    mpp_clock_id = 1

    if( clock_num.EQ.0 )then  !first
       clock_num = mpp_clock_id
       call clock_init(mpp_clock_id,name,flags)
    else
       FIND_CLOCK: do while( trim(name).NE.trim(clocks(mpp_clock_id)%name) )
          mpp_clock_id = mpp_clock_id + 1
          if( mpp_clock_id.GT.clock_num )then
             if( mpp_clock_id.GT.MAX_CLOCKS )then
                call mpp_error( FATAL, 'MPP_CLOCK_ID: too many clock requests, ' // &
                      'check your clock id request or increase MAX_CLOCKS.')
             else               !new clock: initialize
                clock_num = mpp_clock_id
                call clock_init(mpp_clock_id,name,flags,grain)
                exit FIND_CLOCK 
             end if
          end if
       end do FIND_CLOCK
    endif
    return
  end function mpp_clock_id

!#####################################################################
  subroutine mpp_clock_begin(id)
    integer, intent(in) :: id

    if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_CLOCK_BEGIN: You must first call mpp_init.' )
    if( .not. mpp_record_timing_data)return
    if( id.EQ.0 )return
    if( id.LT.0 .OR. id.GT.clock_num )call mpp_error( FATAL, 'MPP_CLOCK_BEGIN: invalid id.' )

!$OMP MASTER
    if( clocks(id)%peset_num.NE.current_peset_num ) &
         call mpp_error( FATAL, 'MPP_CLOCK_BEGIN: cannot change pelist context of a clock.' )
    if( clocks(id)%is_on) call mpp_error(FATAL, 'MPP_CLOCK_BEGIN: mpp_clock_begin is called again '// &
                'before calling mpp_clock_end for the clock '//trim(clocks(id)%name) )
    if( clocks(id)%sync_on_begin )then
!do an untimed sync at the beginning of the clock
!this puts all PEs in the current pelist on par, so that measurements begin together
!ending time will be different, thus measuring load imbalance for this clock.
       call mpp_sync()
    end if

    num_clock_ids = num_clock_ids+1
    if(num_clock_ids > MAX_CLOCKS)call mpp_error(FATAL,'MPP_CLOCK_BEGIN: max num previous_clock exceeded.' )
    previous_clock(num_clock_ids) = current_clock
    current_clock = id
    call system_clock_mpi( clocks(id)%tick )
    clocks(id)%is_on = .true.
!$OMP END MASTER
    return
  end subroutine mpp_clock_begin

!#####################################################################
  subroutine mpp_clock_end(id)
    integer, intent(in) :: id
    integer(8)  :: delta

    if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_CLOCK_END: You must first call mpp_init.' )
    if( .not. mpp_record_timing_data)return
    if( id.EQ.0 )return
    if( id.LT.0 .OR. id.GT.clock_num )call mpp_error( FATAL, 'MPP_CLOCK_BEGIN: invalid id.' )
!$OMP MASTER
    if( .NOT. clocks(id)%is_on) call mpp_error(FATAL, 'MPP_CLOCK_END: mpp_clock_end is called '// &
                'before calling mpp_clock_begin for the clock '//trim(clocks(id)%name) )

    call system_clock_mpi(end_tick)
    if( clocks(id)%peset_num.NE.current_peset_num ) &
         call mpp_error( FATAL, 'MPP_CLOCK_END: cannot change pelist context of a clock.' )
    delta = end_tick - clocks(id)%tick
    if( delta.LT.0 )then
       write( stderr(),* )'pe, id, start_tick, end_tick, delta, max_ticks=', pe, id, clocks(id)%tick, end_tick, delta, max_ticks
       delta = delta + max_ticks + 1
       call mpp_error( WARNING, 'MPP_CLOCK_END: Clock rollover, assumed single roll.' )
    end if
    clocks(id)%total_ticks = clocks(id)%total_ticks + delta
    if(num_clock_ids < 1)then
       call mpp_error(FATAL,'MPP_CLOCK_END: min num previous_clock < 1.' )
    endif
    current_clock = previous_clock(num_clock_ids)
    num_clock_ids = num_clock_ids-1
    clocks(id)%is_on = .false.
!$OMP END MASTER
    return
  end subroutine mpp_clock_end

!#####################################################################
  subroutine increment_current_clock( event_id, bytes )
    integer,           intent(in) :: event_id
    integer, intent(in), optional :: bytes
    integer                       :: n
    integer(8)            :: delta

    if( .not. mpp_record_timing_data )return
    if( current_clock.EQ.0 )return
    if( current_clock.LT.0 .OR. current_clock.GT.clock_num )call mpp_error( FATAL, 'MPP_CLOCK_BEGIN: invalid current_clock.' )
    if( .NOT.clocks(current_clock)%detailed )return
    call system_clock_mpi(end_tick)
    n = clocks(current_clock)%events(event_id)%calls + 1

    if( n.EQ.MAX_EVENTS )call mpp_error( WARNING, &
         'MPP_CLOCK: events exceed MAX_EVENTS, ignore detailed profiling data for clock '//trim(clocks(current_clock)%name) )
    if( n.GT.MAX_EVENTS )return

    clocks(current_clock)%events(event_id)%calls = n
    delta = end_tick - start_tick
    if( delta.LT.0 )then
       write( stderr(),* )'pe, event_id, start_tick, end_tick, delta, max_ticks=', &
                           pe, event_id, start_tick, end_tick, delta, max_ticks
       delta = delta + max_ticks + 1
       call mpp_error( WARNING, 'MPP_CLOCK_END: Clock rollover, assumed single roll.' )
    end if
    clocks(current_clock)%events(event_id)%ticks(n) = delta
    if( PRESENT(bytes) )clocks(current_clock)%events(event_id)%bytes(n) = bytes
    return
  end subroutine increment_current_clock

!#####################################################################

  subroutine dump_clock_summary()

    real              :: total_time,total_time_all,total_data
    real              :: msg_size,eff_BW,s
    integer           :: SD_UNIT, total_calls
    integer           :: i,j,k,ct, msg_cnt
    character(len=2)  :: u
    character(len=20) :: filename
    character(len=20),dimension(MAX_BINS),save :: bin

    data bin( 1)  /'  0   -    8    B:  '/
    data bin( 2)  /'  8   -   16    B:  '/
    data bin( 3)  /' 16   -   32    B:  '/
    data bin( 4)  /' 32   -   64    B:  '/
    data bin( 5)  /' 64   -  128    B:  '/
    data bin( 6)  /'128   -  256    B:  '/
    data bin( 7)  /'256   -  512    B:  '/
    data bin( 8)  /'512   - 1024    B:  '/
    data bin( 9)  /'  1.0 -    2.1 KB:  '/
    data bin(10)  /'  2.1 -    4.1 KB:  '/
    data bin(11)  /'  4.1 -    8.2 KB:  '/
    data bin(12)  /'  8.2 -   16.4 KB:  '/
    data bin(13)  /' 16.4 -   32.8 KB:  '/
    data bin(14)  /' 32.8 -   65.5 KB:  '/
    data bin(15)  /' 65.5 -  131.1 KB:  '/
    data bin(16)  /'131.1 -  262.1 KB:  '/
    data bin(17)  /'262.1 -  524.3 KB:  '/
    data bin(18)  /'524.3 - 1048.6 KB:  '/
    data bin(19)  /'  1.0 -    2.1 MB:  '/
    data bin(20)  /' >2.1          MB:  '/

    if( .NOT.ANY(clocks(1:clock_num)%detailed) )return
    write( filename,'(a,i6.6)' )'mpp_clock.out.', pe

    SD_UNIT = get_unit()
    open(SD_UNIT,file=trim(filename),form='formatted')

    COMM_TYPE: do ct = 1,clock_num

       if( .NOT.clocks(ct)%detailed )cycle
       write(SD_UNIT,*) &
            clock_summary(ct)%name(1:15),' Communication Data for PE ',pe

       write(SD_UNIT,*) ' '
       write(SD_UNIT,*) ' '

       total_time_all = 0.0
       EVENT_TYPE: do k = 1,MAX_EVENT_TYPES-1

          if(clock_summary(ct)%event(k)%total_time == 0.0)cycle

          total_time = clock_summary(ct)%event(k)%total_time
          total_time_all = total_time_all + total_time
          total_data = clock_summary(ct)%event(k)%total_data
          total_calls = clock_summary(ct)%event(k)%total_cnts

          write(SD_UNIT,1000) clock_summary(ct)%event(k)%name(1:9) // ':'

          write(SD_UNIT,1001) 'Total Data: ',total_data*1.0e-6, &
               'MB; Total Time: ', total_time, &
               'secs; Total Calls: ',total_calls

          write(SD_UNIT,*) ' '
          write(SD_UNIT,1002) '     Bin            Counts      Avg Size        Eff B/W'
          write(SD_UNIT,*) ' '

          BIN_LOOP: do j=1,MAX_BINS

             if(clock_summary(ct)%event(k)%msg_size_cnts(j)==0)cycle

             if(j<=8)then
                s = 1.0
                u = ' B'
             elseif(j<=18)then
                s = 1.0e-3
                u = 'KB'
             else
                s = 1.0e-6
                u = 'MB'
             endif

             msg_cnt = clock_summary(ct)%event(k)%msg_size_cnts(j)
             msg_size = &
                  s*(clock_summary(ct)%event(k)%msg_size_sums(j)/real(msg_cnt))
             eff_BW = (1.0e-6)*( clock_summary(ct)%event(k)%msg_size_sums(j) / &
                  clock_summary(ct)%event(k)%msg_time_sums(j) )

             write(SD_UNIT,1003) bin(j),msg_cnt,msg_size,u,eff_BW

          end do BIN_LOOP

          write(SD_UNIT,*) ' '
          write(SD_UNIT,*) ' '
       end do EVENT_TYPE

! "Data-less" WAIT

       if(clock_summary(ct)%event(MAX_EVENT_TYPES)%total_time>0.0)then

          total_time = clock_summary(ct)%event(MAX_EVENT_TYPES)%total_time
          total_time_all = total_time_all + total_time
          total_calls = clock_summary(ct)%event(MAX_EVENT_TYPES)%total_cnts

          write(SD_UNIT,1000) clock_summary(ct)%event(MAX_EVENT_TYPES)%name(1:9) // ':'

          write(SD_UNIT,1004) 'Total Calls: ',total_calls,'; Total Time: ', &
               total_time,'secs'

       endif

       write(SD_UNIT,*) ' '
       write(SD_UNIT,1005) 'Total communication time spent for ' // &
            clock_summary(ct)%name(1:9) // ': ',total_time_all,'secs'
       write(SD_UNIT,*) ' '
       write(SD_UNIT,*) ' '
       write(SD_UNIT,*) ' '

    end do COMM_TYPE

    close(SD_UNIT)

1000 format(a)
1001 format(a,f8.2,a,f8.2,a,i6)
1002 format(a)
1003 format(a,i6,'    ','  ',f6.1,a,'    ',f7.3,'MB/sec')
1004 format(a,i8,a,f9.2,a)
1005 format(a,f9.2,a)
    return
  end subroutine dump_clock_summary

!#####################################################################

  integer function get_unit()

    integer,save :: i
    logical      :: l_open

!  9 is reserved for etc_unit
    do i=10,99
       inquire(unit=i,opened=l_open)
       if(.not.l_open)exit
    end do

    if(i==100)then
       call mpp_error(FATAL,'Unable to get I/O unit')
    else
       get_unit = i
    endif

    return
  end function get_unit

!#####################################################################

  subroutine sum_clock_data()

    integer :: i,j,k,ct,event_size,event_cnt
    real    :: msg_time

    CLOCK_TYPE: do ct=1,clock_num
       if( .NOT.clocks(ct)%detailed )cycle
       EVENT_TYPE: do j=1,MAX_EVENT_TYPES-1
          event_cnt = clocks(ct)%events(j)%calls
          EVENT_SUMMARY: do i=1,event_cnt

             clock_summary(ct)%event(j)%total_cnts = &
                  clock_summary(ct)%event(j)%total_cnts + 1

             event_size = clocks(ct)%events(j)%bytes(i)

             k = find_bin(event_size)

             clock_summary(ct)%event(j)%msg_size_cnts(k) = &
                  clock_summary(ct)%event(j)%msg_size_cnts(k) + 1

             clock_summary(ct)%event(j)%msg_size_sums(k) = &
                  clock_summary(ct)%event(j)%msg_size_sums(k) &
                  + clocks(ct)%events(j)%bytes(i)

             clock_summary(ct)%event(j)%total_data = &
                  clock_summary(ct)%event(j)%total_data &
                  + clocks(ct)%events(j)%bytes(i)

             msg_time = clocks(ct)%events(j)%ticks(i)
             msg_time = tick_rate * real( clocks(ct)%events(j)%ticks(i) )

             clock_summary(ct)%event(j)%msg_time_sums(k) = &
                  clock_summary(ct)%event(j)%msg_time_sums(k) + msg_time

             clock_summary(ct)%event(j)%total_time = &
                  clock_summary(ct)%event(j)%total_time + msg_time

          end do EVENT_SUMMARY
       end do EVENT_TYPE

       j = MAX_EVENT_TYPES ! WAITs
! "msg_size_cnts" doesn't really mean anything for WAIT
! but position will be used to store number of counts for now.

       event_cnt = clocks(ct)%events(j)%calls
       clock_summary(ct)%event(j)%msg_size_cnts(1) = event_cnt
       clock_summary(ct)%event(j)%total_cnts       = event_cnt

       msg_time = tick_rate * real( sum ( clocks(ct)%events(j)%ticks(1:event_cnt) ) )
       clock_summary(ct)%event(j)%msg_time_sums(1) = &
            clock_summary(ct)%event(j)%msg_time_sums(1) + msg_time

       clock_summary(ct)%event(j)%total_time = clock_summary(ct)%event(j)%msg_time_sums(1)

    end do CLOCK_TYPE

    return
  contains
    integer function find_bin(event_size)

      integer,intent(in) :: event_size
      integer            :: k,msg_size

      msg_size = 8
      k = 1
      do while(event_size>msg_size .and. k<MAX_BINS)
         k = k+1
         msg_size = msg_size*2
      end do
      find_bin = k
      return
    end function find_bin

  end subroutine sum_clock_data

!#####################################################################

  function uppercase (cs) 
    character(len=*), intent(in) :: cs
    character(len=len(cs)),target       :: uppercase 
    integer                      :: k,tlen
    character, pointer :: ca
    integer, parameter :: co=iachar('A')-iachar('a') ! case offset
!The transfer function truncates the string with xlf90_r
    tlen = len_trim(cs)
    if(tlen <= 0) then      ! catch IBM compiler bug
       uppercase = cs  ! simply return input blank string
    else
    uppercase = cs(1:tlen)
! #etd

      do k=1, tlen
       ca => uppercase(k:k)
       if(ca >= "a" .and. ca <= "z") ca = achar(ichar(ca)+co)
    enddo

    endif
  end function uppercase

!#######################################################################

  function lowercase (cs) 
    character(len=*), intent(in) :: cs
    character(len=len(cs)),target       :: lowercase 
    integer, parameter :: co=iachar('a')-iachar('A') ! case offset
    integer                        :: k,tlen
    character, pointer :: ca
!  The transfer function truncates the string with xlf90_r
    tlen = len_trim(cs)
    if(tlen <= 0) then      ! catch IBM compiler bug
       lowercase = cs  ! simply return input blank string
    else
    lowercase = cs(1:tlen)
! #etd

    do k=1, tlen
       ca => lowercase(k:k)
       if(ca >= "A" .and. ca <= "Z") ca = achar(ichar(ca)+co)
    enddo

    endif
  end function lowercase


!#######################################################################


! -*-f90-*-
! $Id: mpp_comm.inc,v 16.0 2008/07/30 22:46:18 fms Exp $




! -*-f90-*-
! $Id: mpp_comm_mpi.inc,v 17.0.2.1 2009/09/08 14:20:55 nnz Exp $


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!       ROUTINES TO INITIALIZE/FINALIZE MPP MODULE: mpp_init, mpp_exit        !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!    subroutine mpp_init( flags, in, out, err, log )
!      integer, optional, intent(in) :: flags, in, out, err, log
!subroutine mpp_init( flags, localcomm )
  subroutine mpp_init( flags, localcomm,myid, cpl_id, cpl_rank,ocean_comm,ocean_npes_out,pelist ) !<-- esm insertion
  integer, optional, intent(in) :: flags
  integer, optional, intent(in) :: localcomm
!-->esm deletion integer                       :: my_pe, num_pes, len, i, iunit
  logical                       :: opened, existed
! --> esm insertion
  integer, optional, intent(in) :: myid,cpl_id
  integer, optional, intent(out) :: cpl_rank,ocean_comm,ocean_npes_out
  integer, dimension(256), optional, intent(out) :: pelist
  integer                       :: my_pe, num_pes, len, i, ocean_id=2,coupler_id=0,coupler_rank=0,ierr, iunit
  integer                       :: ibuf(2),dummy(2),istatus(MPI_STATUS_SIZE)
  integer                       :: color,key,ocean_npes
! <-- esm insertion

  character(len=5) :: this_pe

  if( module_is_initialized )return


  call MPI_INITIALIZED( opened, error ) !in case called from another MPI package
  if(opened .and. .NOT. PRESENT(localcomm)) call mpp_error( FATAL, 'MPP_INIT: communicator is required' )
  if( .NOT.opened ) then
    call MPI_INIT(error)
    mpp_comm_private = MPI_COMM_WORLD
!--> esm insertion
    if(present(myid)) ocean_id=myid
    color=Ocean_id
    key=1
    call MPI_COMM_SPLIT( MPI_COMM_WORLD, color,key, ocean_comm_private, error )
!<-- esm insertion

  else
    mpp_comm_private  = localcomm
  endif

  if(present(ocean_comm))ocean_comm=ocean_comm_private

  call MPI_COMM_SIZE( mpp_comm_private, npes, error )
  call MPI_COMM_SIZE( ocean_comm_private, ocean_npes, error ) ! <-- esm insertion
  call MPI_COMM_RANK( ocean_comm_private, pe, error )
  if(present(ocean_npes_out))ocean_npes_out=ocean_npes

!--> esm insertion
  if( .NOT.opened ) then
   if(ocean_npes .ne. npes) then
    if(present(cpl_id)) coupler_id=cpl_id
    if ( tag .ne.1) call mpp_error(FATAL,'in mpp_init, tag is not 1')
    call MPI_RECV(ibuf,2,MPI_INTEGER,MPI_ANY_SOURCE,tag, &
       MPI_COMM_WORLD,istatus,ierr)
    coupler_rank=ibuf(2)
    if (present(cpl_rank)) cpl_rank=coupler_rank
    if ( coupler_id .ne. ibuf(1)) call mpp_error(FATAL,'in mpp_init, coupler id is not right')

    ibuf(1)=Ocean_id ! Ocean_id
    call MPI_COMM_RANK( mpp_comm_private, ibuf(2), error )
    call MPI_GATHER(ibuf,2,MPI_INTEGER,dummy,2,MPI_INTEGER, &
       coupler_rank,MPI_COMM_WORLD,ierr)
   endif
  endif
!<-- esm insertion


  
	allocate( request_global(0:0) )
  allocate( request_recv_global(0:0) )
  request_global(:) = MPI_REQUEST_NULL
  request_recv_global(:) = MPI_REQUEST_NULL

  allocate( request(0:ocean_npes-1) )
  allocate( request_recv(0:ocean_npes-1) )
  request(:) = MPI_REQUEST_NULL
  request_recv(:) = MPI_REQUEST_NULL

  module_is_initialized = .TRUE.

!PEsets: make defaults illegal
  peset(:)%count = -1
  peset(:)%id = -1
  peset(:)%group = -1
!0=single-PE, initialized so that count returns 1
!<-- esm insertion
  if( npes.eq.1) then
    peset(0)%count = 1
    allocate( peset(0)%list(1) )
    peset(0)%list = pe
    current_peset_num = 0
    peset(0)%id = mpp_comm_private
  else if(npes.gt.1) then
    peset_num=0
    peset(0)%count = npes
    allocate( peset(0)%list(npes) )
    do i=0,npes-1
      peset(0)%list(i+1) = i
    enddo
    current_peset_num = 0
    peset(0)%id = mpp_comm_private
  endif
!<-- esm insertion
  
call MPI_COMM_GROUP( mpp_comm_private, peset(0)%group, error )
  world_peset_num = get_peset( (/(i,i=0,npes-1)/) )
  current_peset_num = world_peset_num !initialize current PEset to world

!<-- esm insertion
!set peset 1
  if ( npes.ne.ocean_npes) then      !not stand alone
    peset_num=1
    peset(1)%id=ocean_comm_private
    peset(1)%count=ocean_npes
    allocate(peset(1)%list(peset(1)%count))
    peset(1)%list=(/(i,i=0,ocean_npes-1)/)
    call MPI_COMM_GROUP( ocean_comm_private, peset(1)%group, error )
    if(present(pelist))pelist(1:peset(1)%count)=peset(1)%list(1:peset(1)%count)
    ocean_peset_num=1
    current_peset_num=ocean_peset_num
  endif
!<-- always set peset num to ocean
!<-- esm insertion
!initialize clocks
  call system_clock_mpi( count=tick0, count_rate=ticks_per_sec, count_max=max_ticks )
  tick_rate = 1./ticks_per_sec
  clock0 = mpp_clock_id( 'Total runtime', flags=MPP_CLOCK_SYNC )

  if( PRESENT(flags) )then
     debug   = flags.EQ.MPP_DEBUG
     verbose = flags.EQ.MPP_VERBOSE .OR. debug
  end if

! non-root pe messages written to other location than stdout()
! 9 is reserved for etc_unit
  etc_unit=9
  inquire(unit=etc_unit,opened=opened)
  if(opened) call mpp_error(FATAL,'Unit 9 is already in use (etc_unit) in mpp_comm_mpi')
  if (trim(etcfile) /= '/dev/null') then
    write( etcfile,'(a,i6.6)' )trim(etcfile)//'.', pe
  endif
  inquire(file=etcfile, exist=existed)
  if(existed) then
     open( unit=etc_unit, file=trim(etcfile), status='UNKNOWN' )
  else
     open( unit=etc_unit, file=trim(etcfile) )
  endif
!if optional argument logunit=stdout, write messages to stdout instead.
!if specifying non-defaults, you must specify units not yet in use.
!      if( PRESENT(in) )then
!          inquire( unit=in, opened=opened )
!          if( opened )call mpp_error( FATAL, 'MPP_INIT: unable to open stdin.' )
!          in_unit=in
!      end if
!      if( PRESENT(out) )then
!          inquire( unit=out, opened=opened )
!          if( opened )call mpp_error( FATAL, 'MPP_INIT: unable to open stdout.' )
!          out_unit=out
!      end if
!      if( PRESENT(err) )then
!          inquire( unit=err, opened=opened )
!          if( opened )call mpp_error( FATAL, 'MPP_INIT: unable to open stderr.' )
!          err_unit=err
!      end if
!      log_unit=get_unit()
!      if( PRESENT(log) )then
!          inquire( unit=log, opened=opened )
!          if( opened .AND. log.NE.out_unit )call mpp_error( FATAL, 'MPP_INIT: unable to open stdlog.' )
!          log_unit=log
!      end if
!!log_unit can be written to only from root_pe, all others write to stdout
!      if( log_unit.NE.out_unit )then
!          inquire( unit=log_unit, opened=opened )
!          if( opened )call mpp_error( FATAL, 'MPP_INIT: specified unit for stdlog already in use.' )
!          if( pe.EQ.root_pe )open( unit=log_unit, file=trim(configfile), status='REPLACE' )
!          call mpp_sync()
!          if( pe.NE.root_pe )open( unit=log_unit, file=trim(configfile), status='OLD' )
!      end if

  call mpp_init_logfile()

!messages
  iunit = stdlog()  ! workaround for lf95.
  if( verbose )call mpp_error( NOTE, 'MPP_INIT: initializing MPP module...' )
  if( pe.EQ.root_pe )then
     write( iunit,'(/a)' )'MPP module '//trim(version)//trim(tagname)
     write( iunit,'(a,i6)' )'MPP started with NPES=', npes
     write( iunit,'(a)' )'Using MPI library for message passing...'
     write( iunit, '(a,es12.4,a,i10,a)' ) &
          'Realtime clock resolution=', tick_rate, ' sec (', ticks_per_sec, ' ticks/sec)'
     write( iunit, '(a,es12.4,a,i20,a)' ) &
          'Clock rolls over after ', max_ticks*tick_rate, ' sec (', max_ticks, ' ticks)'
     write( iunit,'(/a)' )'MPP module '//trim(mpp_parameter_version)//trim(mpp_parameter_tagname)
     write( iunit,'(/a)' )'MPP module '//trim(mpp_data_version)//trim(mpp_data_tagname)
  end if
  call mpp_clock_begin(clock0)

  return
end subroutine mpp_init

!#######################################################################
!to be called at the end of a run
subroutine mpp_exit()
  integer :: i, j, k, n, nmax, istat, out_unit, log_unit
  real    :: t, tmin, tmax, tavg, tstd
  real    :: m, mmin, mmax, mavg, mstd, t_total
!-->esm insertion swapna
  integer :: worldpeset,penumm1,minpe,minpe1
  logical :: no_sync=.true.
!<--esm insertion swapna

  logical :: opened

  if( .NOT.module_is_initialized )return
  call mpp_set_current_pelist()
  call mpp_clock_end(clock0)
  t_total = clocks(clock0)%total_ticks*tick_rate
  out_unit = stdout()
  log_unit = stdlog()
  if( clock_num.GT.0 )then
     if( ANY(clocks(1:clock_num)%detailed) )then
        call sum_clock_data; call dump_clock_summary
     end if
     if( pe.EQ.root_pe )then
        write( out_unit,'(/a,i6,a)' ) 'Tabulating mpp_clock statistics across ', npes, ' PEs...'
        if( ANY(clocks(1:clock_num)%detailed) ) &
             write( out_unit,'(a)' )'   ... see mpp_clock.out.#### for details on individual PEs.'
        write( out_unit,'(/32x,a)' ) '          tmin          tmax          tavg          tstd  tfrac grain pemin pemax'
     end if
     write( log_unit,'(/37x,a)' ) 'time'

     call FLUSH( out_unit )
     call mpp_sync()
     do i = 1,clock_num
!!<--esm insertion swapna
!     if(i.eq.1) then
!         penumm1=1
!         minpe=peset(1)%list(1)    ! this pelist has been sorted
!       else
!         penumm1=clocks(i-1)%peset_num
!         minpe=clocks(i-1)%peset_num
!         minpe1=clocks(i)%peset_num
!     endif
!     if (clocks(i)%peset_num.eq.2 .and. penumm1.eq.1) then
!     		if( .NOT.ANY(peset(clocks(i)%peset_num)%list(:).EQ.pe-minpe) )cycle
!     else if (  clocks(i)%peset_num.eq.1 .and. penumm1.eq.2) then
!        if( .NOT.ANY(peset(clocks(i)%peset_num)%list(:).EQ.pe+minpe1) )cycle
!     else
!        if( .NOT.ANY(peset(clocks(i)%peset_num)%list(:).EQ.pe) )cycle
!     endif
!!<--esm insertion added swapna 29/07/12
        call mpp_set_current_pelist( peset(clocks(i)%peset_num)%list )
        out_unit = stdout()
        log_unit = stdlog()
!times between mpp_clock ticks
        t = clocks(i)%total_ticks*tick_rate
        tmin = t; call mpp_min(tmin)
        tmax = t; call mpp_max(tmax)
        tavg = t; call mpp_sum(tavg); tavg = tavg/mpp_npes()
        tstd = (t-tavg)**2; call mpp_sum(tstd); tstd = sqrt( tstd/mpp_npes() )
        if( pe.EQ.root_pe )write( out_unit,'(a32,4f14.6,f7.3,3i6)' ) &
             clocks(i)%name, tmin, tmax, tavg, tstd, tavg/t_total, &
             clocks(i)%grain, minval(peset(clocks(i)%peset_num)%list), &
             maxval(peset(clocks(i)%peset_num)%list)
        write(log_unit,'(a32,f14.6)') clocks(i)%name, clocks(i)%total_ticks*tick_rate
     end do

     if( ANY(clocks(1:clock_num)%detailed) .AND. pe.EQ.root_pe )write( out_unit,'(/32x,a)' ) &
          '       tmin       tmax       tavg       tstd       mmin       mmax       mavg       mstd  mavg/tavg'

     do i = 1,clock_num
!messages: bytelengths and times
        if( .NOT.clocks(i)%detailed )cycle
        if( .NOT.ANY(peset(clocks(i)%peset_num)%list(:).EQ.pe) )cycle
        call mpp_set_current_pelist( peset(clocks(i)%peset_num)%list )
        do j = 1,MAX_EVENT_TYPES
           n = clocks(i)%events(j)%calls; nmax = n
           call mpp_max(nmax)
           if( nmax.NE.0 )then
!don't divide by n because n might be 0
              m = 0
              if( n.GT.0 )m = sum(clocks(i)%events(j)%bytes(1:n))
              mmin = m; call mpp_min(mmin)
              mmax = m; call mpp_max(mmax)
              mavg = m; call mpp_sum(mavg); mavg = mavg/mpp_npes()
              mstd = (m-mavg)**2; call mpp_sum(mstd); mstd = sqrt( mstd/mpp_npes() )
              t = 0
              if( n.GT.0 )t = sum(clocks(i)%events(j)%ticks(1:n))*tick_rate
              tmin = t; call mpp_min(tmin)
              tmax = t; call mpp_max(tmax)
              tavg = t; call mpp_sum(tavg); tavg = tavg/mpp_npes()
              tstd = (t-tavg)**2; call mpp_sum(tstd); tstd = sqrt( tstd/mpp_npes() )
              if( pe.EQ.root_pe )write( out_unit,'(a32,4f11.3,5es11.3)' ) &
                   trim(clocks(i)%name)//' '//trim(clocks(i)%events(j)%name), &
                   tmin, tmax, tavg, tstd, mmin, mmax, mavg, mstd, mavg/tavg
           end if
        end do
     end do
  end if

! close down etc_unit: 9
  inquire(unit=etc_unit, opened=opened)
  if (opened) then
   call FLUSH (etc_unit)
   close(etc_unit)
  endif

  call mpp_set_current_pelist()
  call mpp_sync()
  call mpp_max(mpp_stack_hwm)
  if( pe.EQ.root_pe )write( out_unit,* )'MPP_STACK high water mark=', mpp_stack_hwm
  if(mpp_comm_private == MPI_COMM_WORLD ) call MPI_FINALIZE(error)



  return
end subroutine mpp_exit

!#######################################################################
    subroutine mpp_malloc( ptr, newlen, len )
      integer, intent(in)    :: newlen
      integer, intent(inout) :: len

      integer(8), intent(in) :: ptr
      call mpp_error( FATAL, 'mpp_malloc: requires use_MPI_SMA' )

      return
    end subroutine mpp_malloc



!#######################################################################
!set the mpp_stack variable to be at least n LONG words long
  subroutine mpp_set_stack_size(n)
    integer, intent(in) :: n
    character(len=8)    :: text

    if( n.GT.mpp_stack_size .AND. allocated(mpp_stack) )deallocate(mpp_stack)
    if( .NOT.allocated(mpp_stack) )then
       allocate( mpp_stack(n) )
       mpp_stack_size = n
    end if

    write( text,'(i8)' )n
    if( pe.EQ.root_pe )call mpp_error( NOTE, 'MPP_SET_STACK_SIZE: stack size set to '//text//'.' )

    return
  end subroutine mpp_set_stack_size

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                BASIC MESSAGE PASSING ROUTINE: mpp_transmit                  !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!























































!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                  MPP_TRANSMIT                               !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_transmit_real8( put_data, put_len, to_pe, get_data, get_len, from_pe, block )
!a message-passing routine intended to be reminiscent equally of both MPI and SHMEM

!put_data and get_data are contiguous real(8) arrays

!at each call, your put_data array is put to   to_pe's get_data
!              your get_data array is got from from_pe's put_data
!i.e we assume that typically (e.g updating halo regions) each PE performs a put _and_ a get

!special PE designations:
!      NULL_PE: to disable a put or a get (e.g at boundaries)
!      ANY_PE:  if remote PE for the put or get is to be unspecific
!      ALL_PES: broadcast and collect operations (collect not yet implemented)

!ideally we would not pass length, but this f77-style call performs better (arrays passed by address, not descriptor)
!further, this permits <length> contiguous words from an array of any rank to be passed (avoiding f90 rank conformance check)

!caller is responsible for completion checks (mpp_sync_self) before and after

      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      real(8), intent(in)  :: put_data(*)
      real(8), intent(out) :: get_data(*)
      logical, intent(in), optional :: block
      logical                       :: block_comm
      integer                       :: i, out_unit, n !<-- esm insertion
      integer, pointer 							:: irequest(:),irequest_recv(:)
      real(8), allocatable, save :: local_data(:) !local copy used by non-parallel code (no SHMEM or MPI)

!--> esm insertion
      n=get_peset(); if( peset(n)%count.EQ.1 )return
      if(n.gt.ocean_peset_num)n=ocean_peset_num

      if(associated(irequest)) then
				nullify(irequest)
        nullify(irequest_recv)
      endif
      if(n.eq.world_peset_num) then
				irequest => request_global
				irequest_recv => request_recv_global
      else
				irequest => request
        irequest_recv => request_recv
      endif
!<-- esm insertion

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_TRANSMIT: You must first call mpp_init.' )
      if( to_pe.EQ.NULL_PE .AND. from_pe.EQ.NULL_PE )return
      
      block_comm = .true.
      if(PRESENT(block)) block_comm = block

      out_unit = stdout()
      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_TRANSMIT begin: to_pe, from_pe, put_len, get_len=', to_pe, from_pe, put_len, get_len
      end if

!do put first and then get
!      if( to_pe.GE.0 .AND. to_pe.LT.npes )then
!use non-blocking sends
      if( to_pe.GE.0 .AND. to_pe.LT.peset(n)%count )then !<-- esm insertion
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
          if( irequest(to_pe).NE.MPI_REQUEST_NULL )then !only one message from pe->to_pe in queue
!              if( debug )write( stderr(),* )'PE waiting for sending', pe, to_pe
              call MPI_WAIT( irequest(to_pe), stat, error )
          end if
!          call MPI_ISEND( put_data, put_len, MPI_REAL8, to_pe, tag, mpp_comm_private, request(to_pe), error )
          call MPI_ISEND( put_data, put_len, MPI_REAL8, to_pe, tag, peset(n)%id, irequest(to_pe), error ) !<-- esm insertion
          if( current_clock.NE.0 )call increment_current_clock( EVENT_SEND, put_len*8 )
      else if( to_pe.EQ.ALL_PES )then !this is a broadcast from from_pe
          if( from_pe.LT.0 .OR. from_pe.GE.npes )call mpp_error( FATAL, 'MPP_TRANSMIT: broadcasting from invalid PE.' )
          if( put_len.GT.get_len )call mpp_error( FATAL, 'MPP_TRANSMIT: size mismatch between put_data and get_data.' )
          if( pe.EQ.from_pe )then
              if( LOC(get_data).NE.LOC(put_data) )then
!dir$ IVDEP
                  do i = 1,get_len
                     get_data(i) = put_data(i)
                  end do
              end if
          end if
          call mpp_broadcast( get_data, get_len, from_pe )
          return
      else if( to_pe.EQ.ANY_PE )then !we don't have a destination to do puts to, so only do gets
!...but you cannot have a pure get with MPI
          call mpp_error( FATAL, 'MPP_TRANSMIT: you cannot transmit to ANY_PE using MPI.' )
      else if( to_pe.NE.NULL_PE )then  !no other valid cases except NULL_PE
          call mpp_error( FATAL, 'MPP_TRANSMIT: invalid to_pe.' )
      end if

!do the get: for libSMA, a get means do a wait to ensure put on remote PE is complete
!      if( from_pe.GE.0 .AND. from_pe.LT.npes )then
!receive from from_pe
      if( from_pe.GE.0 .AND. from_pe.LT.peset(n)%count  )then !<-- esm insertion
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
          if( block_comm ) then
!             call MPI_RECV( get_data, get_len, MPI_REAL8, from_pe, MPI_ANY_TAG, mpp_comm_private, stat, error )
             call MPI_RECV( get_data, get_len, MPI_REAL8, from_pe, MPI_ANY_TAG,peset(n)%id , stat, error) !<-- esm insertion
          else
             if( irequest_recv(from_pe).NE.MPI_REQUEST_NULL )then !only one message from from_pe->pe in queue
!              if( debug )write( stderr(),* )'PE waiting for receiving', pe, from_pe
                call MPI_WAIT( irequest_recv(from_pe), stat, error )
             end if
!             call MPI_IRECV( get_data, get_len, MPI_REAL8, from_pe, MPI_ANY_TAG, mpp_comm_private, &
             call MPI_IRECV( get_data, get_len, MPI_REAL8, from_pe, MPI_ANY_TAG, peset(n)%id, &
                  irequest_recv(from_pe), error )!<-- esm insertion
          endif
          if( current_clock.NE.0 )call increment_current_clock( EVENT_RECV, get_len*8 )
      else if( from_pe.EQ.ANY_PE )then
!receive from MPI_ANY_SOURCE
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
!          call MPI_RECV( get_data, get_len, MPI_REAL8, MPI_ANY_SOURCE, MPI_ANY_TAG, mpp_comm_private, stat, error )
          call MPI_RECV( get_data, get_len, MPI_REAL8, MPI_ANY_SOURCE, MPI_ANY_TAG, peset(n)%id, stat, error )!<-- esm insertion
          if( current_clock.NE.0 )call increment_current_clock( EVENT_RECV, get_len*8 )
      else if( from_pe.EQ.ALL_PES )then
          call mpp_error( FATAL, 'MPP_TRANSMIT: from_pe=ALL_PES has ambiguous meaning, and hence is not implemented.' )
      else if( from_pe.NE.NULL_PE )then !only remaining valid choice is NULL_PE
          call mpp_error( FATAL, 'MPP_TRANSMIT: invalid from_pe.' )
      end if

      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_TRANSMIT end: to_pe, from_pe, put_len, get_len=', to_pe, from_pe, put_len, get_len
      end if
      return
    end subroutine mpp_transmit_real8

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                MPP_BROADCAST                                !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_broadcast_real8( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      real(8), intent(inout) :: data(*)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer :: n, i, from_rank, out_unit

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_BROADCAST: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      out_unit = stdout()
      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_BROADCAST begin: from_pe, length=', from_pe, length
      end if

      if( .NOT.ANY(from_pe.EQ.peset(current_peset_num)%list) ) &
           call mpp_error( FATAL, 'MPP_BROADCAST: broadcasting from invalid PE.' )

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
! find the rank of from_pe in the pelist.
      do i = 1, mpp_npes()
         if(peset(n)%list(i) == from_pe) then
             from_rank = i - 1
             exit
         endif
      enddo
      if( mpp_npes().GT.1 )call MPI_BCAST( data, length, MPI_REAL8, from_rank, peset(n)%id, error )
      if( current_clock.NE.0 )call increment_current_clock( EVENT_BROADCAST, length*8 )
      return
    end subroutine mpp_broadcast_real8

!####################################################################################
! -*-f90-*-
! $Id: mpp_transmit.inc,v 13.0.34.1.6.1 2009/07/16 18:25:06 z1l Exp $

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                  MPP_TRANSMIT                               !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_transmit_real8_scalar( put_data, to_pe, get_data, from_pe, plen, glen)
      integer, intent(in) :: to_pe, from_pe
      real(8), intent(in)  :: put_data
      real(8), intent(out) :: get_data
      integer, optional, intent(in) :: plen, glen
      integer                       :: put_len, get_len
      real(8) :: put_data1D(1), get_data1D(1)
      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )

      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      put_len=1; if(PRESENT(plen))put_len=plen
      get_len=1; if(PRESENT(glen))get_len=glen
      call mpp_transmit_real8 ( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_real8_scalar

    subroutine mpp_transmit_real8_2d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      real(8), intent(in)  :: put_data(:,:)
      real(8), intent(out) :: get_data(:,:)
      real(8) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_real8_2d

    subroutine mpp_transmit_real8_3d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      real(8), intent(in)  :: put_data(:,:,:)
      real(8), intent(out) :: get_data(:,:,:)
      real(8) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_real8_3d

    subroutine mpp_transmit_real8_4d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      real(8), intent(in)  :: put_data(:,:,:,:)
      real(8), intent(out) :: get_data(:,:,:,:)
      real(8) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_real8_4d

    subroutine mpp_transmit_real8_5d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      real(8), intent(in)  :: put_data(:,:,:,:,:)
      real(8), intent(out) :: get_data(:,:,:,:,:)
      real(8) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_real8_5d
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                              MPP_SEND and RECV                              !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_recv_real8( get_data, get_len, from_pe, block )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      real(8), intent(out) :: get_data(*)
      logical, intent(in), optional :: block

      real(8) :: dummy(1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe, block )
    end subroutine mpp_recv_real8

    subroutine mpp_send_real8( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      real(8), intent(in) :: put_data(*)
      real(8) :: dummy(1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_real8

    subroutine mpp_recv_real8_scalar( get_data, from_pe, glen, block )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: from_pe
      real(8), intent(out) :: get_data
      logical, intent(in), optional :: block

      integer, optional, intent(in) :: glen
      integer                       :: get_len
      real(8) :: get_data1D(1)
      real(8) :: dummy(1)

      pointer( ptr, get_data1D )
      ptr = LOC(get_data)
      get_len=1; if(PRESENT(glen))get_len=glen
      call mpp_transmit( dummy, 1, NULL_PE, get_data1D, get_len, from_pe , block)

    end subroutine mpp_recv_real8_scalar

    subroutine mpp_send_real8_scalar( put_data, to_pe, plen)
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: to_pe
      real(8), intent(in) :: put_data
      integer, optional, intent(in) :: plen
      integer                       :: put_len
      real(8) :: put_data1D(1)
      real(8) :: dummy(1)

      pointer( ptr, put_data1D )
      ptr = LOC(put_data)
      put_len=1; if(PRESENT(plen))put_len=plen
      call mpp_transmit( put_data1D, put_len, to_pe, dummy, 1, NULL_PE )

    end subroutine mpp_send_real8_scalar

    subroutine mpp_recv_real8_2d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      real(8), intent(out) :: get_data(:,:)
      real(8) :: dummy(1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_real8_2d

    subroutine mpp_send_real8_2d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      real(8), intent(in) :: put_data(:,:)
      real(8) :: dummy(1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_real8_2d

    subroutine mpp_recv_real8_3d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      real(8), intent(out) :: get_data(:,:,:)
      real(8) :: dummy(1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_real8_3d

    subroutine mpp_send_real8_3d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      real(8), intent(in) :: put_data(:,:,:)
      real(8) :: dummy(1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_real8_3d

    subroutine mpp_recv_real8_4d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      real(8), intent(out) :: get_data(:,:,:,:)
      real(8) :: dummy(1,1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_real8_4d

    subroutine mpp_send_real8_4d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      real(8), intent(in) :: put_data(:,:,:,:)
      real(8) :: dummy(1,1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_real8_4d

    subroutine mpp_recv_real8_5d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      real(8), intent(out) :: get_data(:,:,:,:,:)
      real(8) :: dummy(1,1,1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_real8_5d

    subroutine mpp_send_real8_5d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      real(8), intent(in) :: put_data(:,:,:,:,:)
      real(8) :: dummy(1,1,1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_real8_5d
    
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                MPP_BROADCAST                                !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_broadcast_real8_scalar( data, from_pe, pelist )
      real(8), intent(inout) :: data
      integer, intent(in) :: from_pe
      integer, intent(in), optional :: pelist(:)
      real(8) :: data1D(1)

      pointer( ptr, data1D )

      ptr = LOC(data)
      call mpp_broadcast_real8( data1D, 1, from_pe, pelist )

      return
    end subroutine mpp_broadcast_real8_scalar
    
    subroutine mpp_broadcast_real8_2d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      real(8), intent(inout) :: data(:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      real(8) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_real8_2d
    
    subroutine mpp_broadcast_real8_3d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      real(8), intent(inout) :: data(:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      real(8) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
   end subroutine mpp_broadcast_real8_3d
    
    subroutine mpp_broadcast_real8_4d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      real(8), intent(inout) :: data(:,:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      real(8) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_real8_4d
    
    subroutine mpp_broadcast_real8_5d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      real(8), intent(inout) :: data(:,:,:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      real(8) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_real8_5d






























































!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                  MPP_TRANSMIT                               !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_transmit_int8( put_data, put_len, to_pe, get_data, get_len, from_pe, block )
!a message-passing routine intended to be reminiscent equally of both MPI and SHMEM

!put_data and get_data are contiguous integer(8) arrays

!at each call, your put_data array is put to   to_pe's get_data
!              your get_data array is got from from_pe's put_data
!i.e we assume that typically (e.g updating halo regions) each PE performs a put _and_ a get

!special PE designations:
!      NULL_PE: to disable a put or a get (e.g at boundaries)
!      ANY_PE:  if remote PE for the put or get is to be unspecific
!      ALL_PES: broadcast and collect operations (collect not yet implemented)

!ideally we would not pass length, but this f77-style call performs better (arrays passed by address, not descriptor)
!further, this permits <length> contiguous words from an array of any rank to be passed (avoiding f90 rank conformance check)

!caller is responsible for completion checks (mpp_sync_self) before and after

      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      integer(8), intent(in)  :: put_data(*)
      integer(8), intent(out) :: get_data(*)
      logical, intent(in), optional :: block
      logical                       :: block_comm
      integer                       :: i, out_unit, n !<-- esm insertion
      integer, pointer 							:: irequest(:),irequest_recv(:)
      integer(8), allocatable, save :: local_data(:) !local copy used by non-parallel code (no SHMEM or MPI)

!--> esm insertion
      n=get_peset(); if( peset(n)%count.EQ.1 )return
      if(n.gt.ocean_peset_num)n=ocean_peset_num

      if(associated(irequest)) then
				nullify(irequest)
        nullify(irequest_recv)
      endif
      if(n.eq.world_peset_num) then
				irequest => request_global
				irequest_recv => request_recv_global
      else
				irequest => request
        irequest_recv => request_recv
      endif
!<-- esm insertion

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_TRANSMIT: You must first call mpp_init.' )
      if( to_pe.EQ.NULL_PE .AND. from_pe.EQ.NULL_PE )return
      
      block_comm = .true.
      if(PRESENT(block)) block_comm = block

      out_unit = stdout()
      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_TRANSMIT begin: to_pe, from_pe, put_len, get_len=', to_pe, from_pe, put_len, get_len
      end if

!do put first and then get
!      if( to_pe.GE.0 .AND. to_pe.LT.npes )then
!use non-blocking sends
      if( to_pe.GE.0 .AND. to_pe.LT.peset(n)%count )then !<-- esm insertion
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
          if( irequest(to_pe).NE.MPI_REQUEST_NULL )then !only one message from pe->to_pe in queue
!              if( debug )write( stderr(),* )'PE waiting for sending', pe, to_pe
              call MPI_WAIT( irequest(to_pe), stat, error )
          end if
!          call MPI_ISEND( put_data, put_len, MPI_INTEGER8, to_pe, tag, mpp_comm_private, request(to_pe), error )
          call MPI_ISEND( put_data, put_len, MPI_INTEGER8, to_pe, tag, peset(n)%id, irequest(to_pe), error ) !<-- esm insertion
          if( current_clock.NE.0 )call increment_current_clock( EVENT_SEND, put_len*8 )
      else if( to_pe.EQ.ALL_PES )then !this is a broadcast from from_pe
          if( from_pe.LT.0 .OR. from_pe.GE.npes )call mpp_error( FATAL, 'MPP_TRANSMIT: broadcasting from invalid PE.' )
          if( put_len.GT.get_len )call mpp_error( FATAL, 'MPP_TRANSMIT: size mismatch between put_data and get_data.' )
          if( pe.EQ.from_pe )then
              if( LOC(get_data).NE.LOC(put_data) )then
!dir$ IVDEP
                  do i = 1,get_len
                     get_data(i) = put_data(i)
                  end do
              end if
          end if
          call mpp_broadcast( get_data, get_len, from_pe )
          return
      else if( to_pe.EQ.ANY_PE )then !we don't have a destination to do puts to, so only do gets
!...but you cannot have a pure get with MPI
          call mpp_error( FATAL, 'MPP_TRANSMIT: you cannot transmit to ANY_PE using MPI.' )
      else if( to_pe.NE.NULL_PE )then  !no other valid cases except NULL_PE
          call mpp_error( FATAL, 'MPP_TRANSMIT: invalid to_pe.' )
      end if

!do the get: for libSMA, a get means do a wait to ensure put on remote PE is complete
!      if( from_pe.GE.0 .AND. from_pe.LT.npes )then
!receive from from_pe
      if( from_pe.GE.0 .AND. from_pe.LT.peset(n)%count  )then !<-- esm insertion
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
          if( block_comm ) then
!             call MPI_RECV( get_data, get_len, MPI_INTEGER8, from_pe, MPI_ANY_TAG, mpp_comm_private, stat, error )
             call MPI_RECV( get_data, get_len, MPI_INTEGER8, from_pe, MPI_ANY_TAG,peset(n)%id , stat, error) !<-- esm insertion
          else
             if( irequest_recv(from_pe).NE.MPI_REQUEST_NULL )then !only one message from from_pe->pe in queue
!              if( debug )write( stderr(),* )'PE waiting for receiving', pe, from_pe
                call MPI_WAIT( irequest_recv(from_pe), stat, error )
             end if
!             call MPI_IRECV( get_data, get_len, MPI_INTEGER8, from_pe, MPI_ANY_TAG, mpp_comm_private, &
             call MPI_IRECV( get_data, get_len, MPI_INTEGER8, from_pe, MPI_ANY_TAG, peset(n)%id, &
                  irequest_recv(from_pe), error )!<-- esm insertion
          endif
          if( current_clock.NE.0 )call increment_current_clock( EVENT_RECV, get_len*8 )
      else if( from_pe.EQ.ANY_PE )then
!receive from MPI_ANY_SOURCE
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
!          call MPI_RECV( get_data, get_len, MPI_INTEGER8, MPI_ANY_SOURCE, MPI_ANY_TAG, mpp_comm_private, stat, error )
          call MPI_RECV( get_data, get_len, MPI_INTEGER8, MPI_ANY_SOURCE, MPI_ANY_TAG, peset(n)%id, stat, error )!<-- esm insertion
          if( current_clock.NE.0 )call increment_current_clock( EVENT_RECV, get_len*8 )
      else if( from_pe.EQ.ALL_PES )then
          call mpp_error( FATAL, 'MPP_TRANSMIT: from_pe=ALL_PES has ambiguous meaning, and hence is not implemented.' )
      else if( from_pe.NE.NULL_PE )then !only remaining valid choice is NULL_PE
          call mpp_error( FATAL, 'MPP_TRANSMIT: invalid from_pe.' )
      end if

      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_TRANSMIT end: to_pe, from_pe, put_len, get_len=', to_pe, from_pe, put_len, get_len
      end if
      return
    end subroutine mpp_transmit_int8

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                MPP_BROADCAST                                !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_broadcast_int8( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      integer(8), intent(inout) :: data(*)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer :: n, i, from_rank, out_unit

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_BROADCAST: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      out_unit = stdout()
      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_BROADCAST begin: from_pe, length=', from_pe, length
      end if

      if( .NOT.ANY(from_pe.EQ.peset(current_peset_num)%list) ) &
           call mpp_error( FATAL, 'MPP_BROADCAST: broadcasting from invalid PE.' )

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
! find the rank of from_pe in the pelist.
      do i = 1, mpp_npes()
         if(peset(n)%list(i) == from_pe) then
             from_rank = i - 1
             exit
         endif
      enddo
      if( mpp_npes().GT.1 )call MPI_BCAST( data, length, MPI_INTEGER8, from_rank, peset(n)%id, error )
      if( current_clock.NE.0 )call increment_current_clock( EVENT_BROADCAST, length*8 )
      return
    end subroutine mpp_broadcast_int8

!####################################################################################
! -*-f90-*-
! $Id: mpp_transmit.inc,v 13.0.34.1.6.1 2009/07/16 18:25:06 z1l Exp $

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                  MPP_TRANSMIT                               !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_transmit_int8_scalar( put_data, to_pe, get_data, from_pe, plen, glen)
      integer, intent(in) :: to_pe, from_pe
      integer(8), intent(in)  :: put_data
      integer(8), intent(out) :: get_data
      integer, optional, intent(in) :: plen, glen
      integer                       :: put_len, get_len
      integer(8) :: put_data1D(1), get_data1D(1)
      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )

      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      put_len=1; if(PRESENT(plen))put_len=plen
      get_len=1; if(PRESENT(glen))get_len=glen
      call mpp_transmit_int8 ( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_int8_scalar

    subroutine mpp_transmit_int8_2d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      integer(8), intent(in)  :: put_data(:,:)
      integer(8), intent(out) :: get_data(:,:)
      integer(8) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_int8_2d

    subroutine mpp_transmit_int8_3d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      integer(8), intent(in)  :: put_data(:,:,:)
      integer(8), intent(out) :: get_data(:,:,:)
      integer(8) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_int8_3d

    subroutine mpp_transmit_int8_4d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      integer(8), intent(in)  :: put_data(:,:,:,:)
      integer(8), intent(out) :: get_data(:,:,:,:)
      integer(8) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_int8_4d

    subroutine mpp_transmit_int8_5d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      integer(8), intent(in)  :: put_data(:,:,:,:,:)
      integer(8), intent(out) :: get_data(:,:,:,:,:)
      integer(8) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_int8_5d
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                              MPP_SEND and RECV                              !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_recv_int8( get_data, get_len, from_pe, block )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      integer(8), intent(out) :: get_data(*)
      logical, intent(in), optional :: block

      integer(8) :: dummy(1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe, block )
    end subroutine mpp_recv_int8

    subroutine mpp_send_int8( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      integer(8), intent(in) :: put_data(*)
      integer(8) :: dummy(1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_int8

    subroutine mpp_recv_int8_scalar( get_data, from_pe, glen, block )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: from_pe
      integer(8), intent(out) :: get_data
      logical, intent(in), optional :: block

      integer, optional, intent(in) :: glen
      integer                       :: get_len
      integer(8) :: get_data1D(1)
      integer(8) :: dummy(1)

      pointer( ptr, get_data1D )
      ptr = LOC(get_data)
      get_len=1; if(PRESENT(glen))get_len=glen
      call mpp_transmit( dummy, 1, NULL_PE, get_data1D, get_len, from_pe , block)

    end subroutine mpp_recv_int8_scalar

    subroutine mpp_send_int8_scalar( put_data, to_pe, plen)
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: to_pe
      integer(8), intent(in) :: put_data
      integer, optional, intent(in) :: plen
      integer                       :: put_len
      integer(8) :: put_data1D(1)
      integer(8) :: dummy(1)

      pointer( ptr, put_data1D )
      ptr = LOC(put_data)
      put_len=1; if(PRESENT(plen))put_len=plen
      call mpp_transmit( put_data1D, put_len, to_pe, dummy, 1, NULL_PE )

    end subroutine mpp_send_int8_scalar

    subroutine mpp_recv_int8_2d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      integer(8), intent(out) :: get_data(:,:)
      integer(8) :: dummy(1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_int8_2d

    subroutine mpp_send_int8_2d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      integer(8), intent(in) :: put_data(:,:)
      integer(8) :: dummy(1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_int8_2d

    subroutine mpp_recv_int8_3d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      integer(8), intent(out) :: get_data(:,:,:)
      integer(8) :: dummy(1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_int8_3d

    subroutine mpp_send_int8_3d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      integer(8), intent(in) :: put_data(:,:,:)
      integer(8) :: dummy(1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_int8_3d

    subroutine mpp_recv_int8_4d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      integer(8), intent(out) :: get_data(:,:,:,:)
      integer(8) :: dummy(1,1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_int8_4d

    subroutine mpp_send_int8_4d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      integer(8), intent(in) :: put_data(:,:,:,:)
      integer(8) :: dummy(1,1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_int8_4d

    subroutine mpp_recv_int8_5d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      integer(8), intent(out) :: get_data(:,:,:,:,:)
      integer(8) :: dummy(1,1,1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_int8_5d

    subroutine mpp_send_int8_5d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      integer(8), intent(in) :: put_data(:,:,:,:,:)
      integer(8) :: dummy(1,1,1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_int8_5d
    
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                MPP_BROADCAST                                !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_broadcast_int8_scalar( data, from_pe, pelist )
      integer(8), intent(inout) :: data
      integer, intent(in) :: from_pe
      integer, intent(in), optional :: pelist(:)
      integer(8) :: data1D(1)

      pointer( ptr, data1D )

      ptr = LOC(data)
      call mpp_broadcast_int8( data1D, 1, from_pe, pelist )

      return
    end subroutine mpp_broadcast_int8_scalar
    
    subroutine mpp_broadcast_int8_2d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      integer(8), intent(inout) :: data(:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer(8) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_int8_2d
    
    subroutine mpp_broadcast_int8_3d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      integer(8), intent(inout) :: data(:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer(8) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
   end subroutine mpp_broadcast_int8_3d
    
    subroutine mpp_broadcast_int8_4d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      integer(8), intent(inout) :: data(:,:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer(8) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_int8_4d
    
    subroutine mpp_broadcast_int8_5d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      integer(8), intent(inout) :: data(:,:,:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer(8) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_int8_5d
























































!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                  MPP_TRANSMIT                               !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_transmit_int4( put_data, put_len, to_pe, get_data, get_len, from_pe, block )
!a message-passing routine intended to be reminiscent equally of both MPI and SHMEM

!put_data and get_data are contiguous integer(4) arrays

!at each call, your put_data array is put to   to_pe's get_data
!              your get_data array is got from from_pe's put_data
!i.e we assume that typically (e.g updating halo regions) each PE performs a put _and_ a get

!special PE designations:
!      NULL_PE: to disable a put or a get (e.g at boundaries)
!      ANY_PE:  if remote PE for the put or get is to be unspecific
!      ALL_PES: broadcast and collect operations (collect not yet implemented)

!ideally we would not pass length, but this f77-style call performs better (arrays passed by address, not descriptor)
!further, this permits <length> contiguous words from an array of any rank to be passed (avoiding f90 rank conformance check)

!caller is responsible for completion checks (mpp_sync_self) before and after

      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      integer(4), intent(in)  :: put_data(*)
      integer(4), intent(out) :: get_data(*)
      logical, intent(in), optional :: block
      logical                       :: block_comm
      integer                       :: i, out_unit, n !<-- esm insertion
      integer, pointer 							:: irequest(:),irequest_recv(:)
      integer(4), allocatable, save :: local_data(:) !local copy used by non-parallel code (no SHMEM or MPI)

!--> esm insertion
      n=get_peset(); if( peset(n)%count.EQ.1 )return
      if(n.gt.ocean_peset_num)n=ocean_peset_num

      if(associated(irequest)) then
				nullify(irequest)
        nullify(irequest_recv)
      endif
      if(n.eq.world_peset_num) then
				irequest => request_global
				irequest_recv => request_recv_global
      else
				irequest => request
        irequest_recv => request_recv
      endif
!<-- esm insertion

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_TRANSMIT: You must first call mpp_init.' )
      if( to_pe.EQ.NULL_PE .AND. from_pe.EQ.NULL_PE )return
      
      block_comm = .true.
      if(PRESENT(block)) block_comm = block

      out_unit = stdout()
      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_TRANSMIT begin: to_pe, from_pe, put_len, get_len=', to_pe, from_pe, put_len, get_len
      end if

!do put first and then get
!      if( to_pe.GE.0 .AND. to_pe.LT.npes )then
!use non-blocking sends
      if( to_pe.GE.0 .AND. to_pe.LT.peset(n)%count )then !<-- esm insertion
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
          if( irequest(to_pe).NE.MPI_REQUEST_NULL )then !only one message from pe->to_pe in queue
!              if( debug )write( stderr(),* )'PE waiting for sending', pe, to_pe
              call MPI_WAIT( irequest(to_pe), stat, error )
          end if
!          call MPI_ISEND( put_data, put_len, MPI_INTEGER4, to_pe, tag, mpp_comm_private, request(to_pe), error )
          call MPI_ISEND( put_data, put_len, MPI_INTEGER4, to_pe, tag, peset(n)%id, irequest(to_pe), error ) !<-- esm insertion
          if( current_clock.NE.0 )call increment_current_clock( EVENT_SEND, put_len*4 )
      else if( to_pe.EQ.ALL_PES )then !this is a broadcast from from_pe
          if( from_pe.LT.0 .OR. from_pe.GE.npes )call mpp_error( FATAL, 'MPP_TRANSMIT: broadcasting from invalid PE.' )
          if( put_len.GT.get_len )call mpp_error( FATAL, 'MPP_TRANSMIT: size mismatch between put_data and get_data.' )
          if( pe.EQ.from_pe )then
              if( LOC(get_data).NE.LOC(put_data) )then
!dir$ IVDEP
                  do i = 1,get_len
                     get_data(i) = put_data(i)
                  end do
              end if
          end if
          call mpp_broadcast( get_data, get_len, from_pe )
          return
      else if( to_pe.EQ.ANY_PE )then !we don't have a destination to do puts to, so only do gets
!...but you cannot have a pure get with MPI
          call mpp_error( FATAL, 'MPP_TRANSMIT: you cannot transmit to ANY_PE using MPI.' )
      else if( to_pe.NE.NULL_PE )then  !no other valid cases except NULL_PE
          call mpp_error( FATAL, 'MPP_TRANSMIT: invalid to_pe.' )
      end if

!do the get: for libSMA, a get means do a wait to ensure put on remote PE is complete
!      if( from_pe.GE.0 .AND. from_pe.LT.npes )then
!receive from from_pe
      if( from_pe.GE.0 .AND. from_pe.LT.peset(n)%count  )then !<-- esm insertion
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
          if( block_comm ) then
!             call MPI_RECV( get_data, get_len, MPI_INTEGER4, from_pe, MPI_ANY_TAG, mpp_comm_private, stat, error )
             call MPI_RECV( get_data, get_len, MPI_INTEGER4, from_pe, MPI_ANY_TAG,peset(n)%id , stat, error) !<-- esm insertion
          else
             if( irequest_recv(from_pe).NE.MPI_REQUEST_NULL )then !only one message from from_pe->pe in queue
!              if( debug )write( stderr(),* )'PE waiting for receiving', pe, from_pe
                call MPI_WAIT( irequest_recv(from_pe), stat, error )
             end if
!             call MPI_IRECV( get_data, get_len, MPI_INTEGER4, from_pe, MPI_ANY_TAG, mpp_comm_private, &
             call MPI_IRECV( get_data, get_len, MPI_INTEGER4, from_pe, MPI_ANY_TAG, peset(n)%id, &
                  irequest_recv(from_pe), error )!<-- esm insertion
          endif
          if( current_clock.NE.0 )call increment_current_clock( EVENT_RECV, get_len*4 )
      else if( from_pe.EQ.ANY_PE )then
!receive from MPI_ANY_SOURCE
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
!          call MPI_RECV( get_data, get_len, MPI_INTEGER4, MPI_ANY_SOURCE, MPI_ANY_TAG, mpp_comm_private, stat, error )
          call MPI_RECV( get_data, get_len, MPI_INTEGER4, MPI_ANY_SOURCE, MPI_ANY_TAG, peset(n)%id, stat, error )!<-- esm insertion
          if( current_clock.NE.0 )call increment_current_clock( EVENT_RECV, get_len*4 )
      else if( from_pe.EQ.ALL_PES )then
          call mpp_error( FATAL, 'MPP_TRANSMIT: from_pe=ALL_PES has ambiguous meaning, and hence is not implemented.' )
      else if( from_pe.NE.NULL_PE )then !only remaining valid choice is NULL_PE
          call mpp_error( FATAL, 'MPP_TRANSMIT: invalid from_pe.' )
      end if

      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_TRANSMIT end: to_pe, from_pe, put_len, get_len=', to_pe, from_pe, put_len, get_len
      end if
      return
    end subroutine mpp_transmit_int4

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                MPP_BROADCAST                                !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_broadcast_int4( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      integer(4), intent(inout) :: data(*)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer :: n, i, from_rank, out_unit

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_BROADCAST: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      out_unit = stdout()
      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_BROADCAST begin: from_pe, length=', from_pe, length
      end if

      if( .NOT.ANY(from_pe.EQ.peset(current_peset_num)%list) ) &
           call mpp_error( FATAL, 'MPP_BROADCAST: broadcasting from invalid PE.' )

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
! find the rank of from_pe in the pelist.
      do i = 1, mpp_npes()
         if(peset(n)%list(i) == from_pe) then
             from_rank = i - 1
             exit
         endif
      enddo
      if( mpp_npes().GT.1 )call MPI_BCAST( data, length, MPI_INTEGER4, from_rank, peset(n)%id, error )
      if( current_clock.NE.0 )call increment_current_clock( EVENT_BROADCAST, length*4 )
      return
    end subroutine mpp_broadcast_int4

!####################################################################################
! -*-f90-*-
! $Id: mpp_transmit.inc,v 13.0.34.1.6.1 2009/07/16 18:25:06 z1l Exp $

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                  MPP_TRANSMIT                               !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_transmit_int4_scalar( put_data, to_pe, get_data, from_pe, plen, glen)
      integer, intent(in) :: to_pe, from_pe
      integer(4), intent(in)  :: put_data
      integer(4), intent(out) :: get_data
      integer, optional, intent(in) :: plen, glen
      integer                       :: put_len, get_len
      integer(4) :: put_data1D(1), get_data1D(1)
      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )

      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      put_len=1; if(PRESENT(plen))put_len=plen
      get_len=1; if(PRESENT(glen))get_len=glen
      call mpp_transmit_int4 ( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_int4_scalar

    subroutine mpp_transmit_int4_2d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      integer(4), intent(in)  :: put_data(:,:)
      integer(4), intent(out) :: get_data(:,:)
      integer(4) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_int4_2d

    subroutine mpp_transmit_int4_3d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      integer(4), intent(in)  :: put_data(:,:,:)
      integer(4), intent(out) :: get_data(:,:,:)
      integer(4) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_int4_3d

    subroutine mpp_transmit_int4_4d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      integer(4), intent(in)  :: put_data(:,:,:,:)
      integer(4), intent(out) :: get_data(:,:,:,:)
      integer(4) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_int4_4d

    subroutine mpp_transmit_int4_5d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      integer(4), intent(in)  :: put_data(:,:,:,:,:)
      integer(4), intent(out) :: get_data(:,:,:,:,:)
      integer(4) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_int4_5d
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                              MPP_SEND and RECV                              !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_recv_int4( get_data, get_len, from_pe, block )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      integer(4), intent(out) :: get_data(*)
      logical, intent(in), optional :: block

      integer(4) :: dummy(1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe, block )
    end subroutine mpp_recv_int4

    subroutine mpp_send_int4( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      integer(4), intent(in) :: put_data(*)
      integer(4) :: dummy(1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_int4

    subroutine mpp_recv_int4_scalar( get_data, from_pe, glen, block )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: from_pe
      integer(4), intent(out) :: get_data
      logical, intent(in), optional :: block

      integer, optional, intent(in) :: glen
      integer                       :: get_len
      integer(4) :: get_data1D(1)
      integer(4) :: dummy(1)

      pointer( ptr, get_data1D )
      ptr = LOC(get_data)
      get_len=1; if(PRESENT(glen))get_len=glen
      call mpp_transmit( dummy, 1, NULL_PE, get_data1D, get_len, from_pe , block)

    end subroutine mpp_recv_int4_scalar

    subroutine mpp_send_int4_scalar( put_data, to_pe, plen)
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: to_pe
      integer(4), intent(in) :: put_data
      integer, optional, intent(in) :: plen
      integer                       :: put_len
      integer(4) :: put_data1D(1)
      integer(4) :: dummy(1)

      pointer( ptr, put_data1D )
      ptr = LOC(put_data)
      put_len=1; if(PRESENT(plen))put_len=plen
      call mpp_transmit( put_data1D, put_len, to_pe, dummy, 1, NULL_PE )

    end subroutine mpp_send_int4_scalar

    subroutine mpp_recv_int4_2d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      integer(4), intent(out) :: get_data(:,:)
      integer(4) :: dummy(1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_int4_2d

    subroutine mpp_send_int4_2d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      integer(4), intent(in) :: put_data(:,:)
      integer(4) :: dummy(1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_int4_2d

    subroutine mpp_recv_int4_3d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      integer(4), intent(out) :: get_data(:,:,:)
      integer(4) :: dummy(1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_int4_3d

    subroutine mpp_send_int4_3d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      integer(4), intent(in) :: put_data(:,:,:)
      integer(4) :: dummy(1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_int4_3d

    subroutine mpp_recv_int4_4d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      integer(4), intent(out) :: get_data(:,:,:,:)
      integer(4) :: dummy(1,1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_int4_4d

    subroutine mpp_send_int4_4d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      integer(4), intent(in) :: put_data(:,:,:,:)
      integer(4) :: dummy(1,1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_int4_4d

    subroutine mpp_recv_int4_5d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      integer(4), intent(out) :: get_data(:,:,:,:,:)
      integer(4) :: dummy(1,1,1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_int4_5d

    subroutine mpp_send_int4_5d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      integer(4), intent(in) :: put_data(:,:,:,:,:)
      integer(4) :: dummy(1,1,1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_int4_5d
    
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                MPP_BROADCAST                                !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_broadcast_int4_scalar( data, from_pe, pelist )
      integer(4), intent(inout) :: data
      integer, intent(in) :: from_pe
      integer, intent(in), optional :: pelist(:)
      integer(4) :: data1D(1)

      pointer( ptr, data1D )

      ptr = LOC(data)
      call mpp_broadcast_int4( data1D, 1, from_pe, pelist )

      return
    end subroutine mpp_broadcast_int4_scalar
    
    subroutine mpp_broadcast_int4_2d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      integer(4), intent(inout) :: data(:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer(4) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_int4_2d
    
    subroutine mpp_broadcast_int4_3d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      integer(4), intent(inout) :: data(:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer(4) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
   end subroutine mpp_broadcast_int4_3d
    
    subroutine mpp_broadcast_int4_4d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      integer(4), intent(inout) :: data(:,:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer(4) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_int4_4d
    
    subroutine mpp_broadcast_int4_5d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      integer(4), intent(inout) :: data(:,:,:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer(4) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_int4_5d
























































!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                  MPP_TRANSMIT                               !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_transmit_logical8( put_data, put_len, to_pe, get_data, get_len, from_pe, block )
!a message-passing routine intended to be reminiscent equally of both MPI and SHMEM

!put_data and get_data are contiguous logical(8) arrays

!at each call, your put_data array is put to   to_pe's get_data
!              your get_data array is got from from_pe's put_data
!i.e we assume that typically (e.g updating halo regions) each PE performs a put _and_ a get

!special PE designations:
!      NULL_PE: to disable a put or a get (e.g at boundaries)
!      ANY_PE:  if remote PE for the put or get is to be unspecific
!      ALL_PES: broadcast and collect operations (collect not yet implemented)

!ideally we would not pass length, but this f77-style call performs better (arrays passed by address, not descriptor)
!further, this permits <length> contiguous words from an array of any rank to be passed (avoiding f90 rank conformance check)

!caller is responsible for completion checks (mpp_sync_self) before and after

      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      logical(8), intent(in)  :: put_data(*)
      logical(8), intent(out) :: get_data(*)
      logical, intent(in), optional :: block
      logical                       :: block_comm
      integer                       :: i, out_unit, n !<-- esm insertion
      integer, pointer 							:: irequest(:),irequest_recv(:)
      logical(8), allocatable, save :: local_data(:) !local copy used by non-parallel code (no SHMEM or MPI)

!--> esm insertion
      n=get_peset(); if( peset(n)%count.EQ.1 )return
      if(n.gt.ocean_peset_num)n=ocean_peset_num

      if(associated(irequest)) then
				nullify(irequest)
        nullify(irequest_recv)
      endif
      if(n.eq.world_peset_num) then
				irequest => request_global
				irequest_recv => request_recv_global
      else
				irequest => request
        irequest_recv => request_recv
      endif
!<-- esm insertion

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_TRANSMIT: You must first call mpp_init.' )
      if( to_pe.EQ.NULL_PE .AND. from_pe.EQ.NULL_PE )return
      
      block_comm = .true.
      if(PRESENT(block)) block_comm = block

      out_unit = stdout()
      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_TRANSMIT begin: to_pe, from_pe, put_len, get_len=', to_pe, from_pe, put_len, get_len
      end if

!do put first and then get
!      if( to_pe.GE.0 .AND. to_pe.LT.npes )then
!use non-blocking sends
      if( to_pe.GE.0 .AND. to_pe.LT.peset(n)%count )then !<-- esm insertion
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
          if( irequest(to_pe).NE.MPI_REQUEST_NULL )then !only one message from pe->to_pe in queue
!              if( debug )write( stderr(),* )'PE waiting for sending', pe, to_pe
              call MPI_WAIT( irequest(to_pe), stat, error )
          end if
!          call MPI_ISEND( put_data, put_len, MPI_INTEGER8, to_pe, tag, mpp_comm_private, request(to_pe), error )
          call MPI_ISEND( put_data, put_len, MPI_INTEGER8, to_pe, tag, peset(n)%id, irequest(to_pe), error ) !<-- esm insertion
          if( current_clock.NE.0 )call increment_current_clock( EVENT_SEND, put_len*8 )
      else if( to_pe.EQ.ALL_PES )then !this is a broadcast from from_pe
          if( from_pe.LT.0 .OR. from_pe.GE.npes )call mpp_error( FATAL, 'MPP_TRANSMIT: broadcasting from invalid PE.' )
          if( put_len.GT.get_len )call mpp_error( FATAL, 'MPP_TRANSMIT: size mismatch between put_data and get_data.' )
          if( pe.EQ.from_pe )then
              if( LOC(get_data).NE.LOC(put_data) )then
!dir$ IVDEP
                  do i = 1,get_len
                     get_data(i) = put_data(i)
                  end do
              end if
          end if
          call mpp_broadcast( get_data, get_len, from_pe )
          return
      else if( to_pe.EQ.ANY_PE )then !we don't have a destination to do puts to, so only do gets
!...but you cannot have a pure get with MPI
          call mpp_error( FATAL, 'MPP_TRANSMIT: you cannot transmit to ANY_PE using MPI.' )
      else if( to_pe.NE.NULL_PE )then  !no other valid cases except NULL_PE
          call mpp_error( FATAL, 'MPP_TRANSMIT: invalid to_pe.' )
      end if

!do the get: for libSMA, a get means do a wait to ensure put on remote PE is complete
!      if( from_pe.GE.0 .AND. from_pe.LT.npes )then
!receive from from_pe
      if( from_pe.GE.0 .AND. from_pe.LT.peset(n)%count  )then !<-- esm insertion
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
          if( block_comm ) then
!             call MPI_RECV( get_data, get_len, MPI_INTEGER8, from_pe, MPI_ANY_TAG, mpp_comm_private, stat, error )
             call MPI_RECV( get_data, get_len, MPI_INTEGER8, from_pe, MPI_ANY_TAG,peset(n)%id , stat, error) !<-- esm insertion
          else
             if( irequest_recv(from_pe).NE.MPI_REQUEST_NULL )then !only one message from from_pe->pe in queue
!              if( debug )write( stderr(),* )'PE waiting for receiving', pe, from_pe
                call MPI_WAIT( irequest_recv(from_pe), stat, error )
             end if
!             call MPI_IRECV( get_data, get_len, MPI_INTEGER8, from_pe, MPI_ANY_TAG, mpp_comm_private, &
             call MPI_IRECV( get_data, get_len, MPI_INTEGER8, from_pe, MPI_ANY_TAG, peset(n)%id, &
                  irequest_recv(from_pe), error )!<-- esm insertion
          endif
          if( current_clock.NE.0 )call increment_current_clock( EVENT_RECV, get_len*8 )
      else if( from_pe.EQ.ANY_PE )then
!receive from MPI_ANY_SOURCE
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
!          call MPI_RECV( get_data, get_len, MPI_INTEGER8, MPI_ANY_SOURCE, MPI_ANY_TAG, mpp_comm_private, stat, error )
          call MPI_RECV( get_data, get_len, MPI_INTEGER8, MPI_ANY_SOURCE, MPI_ANY_TAG, peset(n)%id, stat, error )!<-- esm insertion
          if( current_clock.NE.0 )call increment_current_clock( EVENT_RECV, get_len*8 )
      else if( from_pe.EQ.ALL_PES )then
          call mpp_error( FATAL, 'MPP_TRANSMIT: from_pe=ALL_PES has ambiguous meaning, and hence is not implemented.' )
      else if( from_pe.NE.NULL_PE )then !only remaining valid choice is NULL_PE
          call mpp_error( FATAL, 'MPP_TRANSMIT: invalid from_pe.' )
      end if

      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_TRANSMIT end: to_pe, from_pe, put_len, get_len=', to_pe, from_pe, put_len, get_len
      end if
      return
    end subroutine mpp_transmit_logical8

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                MPP_BROADCAST                                !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_broadcast_logical8( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      logical(8), intent(inout) :: data(*)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer :: n, i, from_rank, out_unit

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_BROADCAST: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      out_unit = stdout()
      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_BROADCAST begin: from_pe, length=', from_pe, length
      end if

      if( .NOT.ANY(from_pe.EQ.peset(current_peset_num)%list) ) &
           call mpp_error( FATAL, 'MPP_BROADCAST: broadcasting from invalid PE.' )

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
! find the rank of from_pe in the pelist.
      do i = 1, mpp_npes()
         if(peset(n)%list(i) == from_pe) then
             from_rank = i - 1
             exit
         endif
      enddo
      if( mpp_npes().GT.1 )call MPI_BCAST( data, length, MPI_INTEGER8, from_rank, peset(n)%id, error )
      if( current_clock.NE.0 )call increment_current_clock( EVENT_BROADCAST, length*8 )
      return
    end subroutine mpp_broadcast_logical8

!####################################################################################
! -*-f90-*-
! $Id: mpp_transmit.inc,v 13.0.34.1.6.1 2009/07/16 18:25:06 z1l Exp $

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                  MPP_TRANSMIT                               !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_transmit_logical8_scalar( put_data, to_pe, get_data, from_pe, plen, glen)
      integer, intent(in) :: to_pe, from_pe
      logical(8), intent(in)  :: put_data
      logical(8), intent(out) :: get_data
      integer, optional, intent(in) :: plen, glen
      integer                       :: put_len, get_len
      logical(8) :: put_data1D(1), get_data1D(1)
      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )

      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      put_len=1; if(PRESENT(plen))put_len=plen
      get_len=1; if(PRESENT(glen))get_len=glen
      call mpp_transmit_logical8 ( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_logical8_scalar

    subroutine mpp_transmit_logical8_2d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      logical(8), intent(in)  :: put_data(:,:)
      logical(8), intent(out) :: get_data(:,:)
      logical(8) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_logical8_2d

    subroutine mpp_transmit_logical8_3d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      logical(8), intent(in)  :: put_data(:,:,:)
      logical(8), intent(out) :: get_data(:,:,:)
      logical(8) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_logical8_3d

    subroutine mpp_transmit_logical8_4d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      logical(8), intent(in)  :: put_data(:,:,:,:)
      logical(8), intent(out) :: get_data(:,:,:,:)
      logical(8) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_logical8_4d

    subroutine mpp_transmit_logical8_5d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      logical(8), intent(in)  :: put_data(:,:,:,:,:)
      logical(8), intent(out) :: get_data(:,:,:,:,:)
      logical(8) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_logical8_5d
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                              MPP_SEND and RECV                              !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_recv_logical8( get_data, get_len, from_pe, block )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      logical(8), intent(out) :: get_data(*)
      logical, intent(in), optional :: block

      logical(8) :: dummy(1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe, block )
    end subroutine mpp_recv_logical8

    subroutine mpp_send_logical8( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      logical(8), intent(in) :: put_data(*)
      logical(8) :: dummy(1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_logical8

    subroutine mpp_recv_logical8_scalar( get_data, from_pe, glen, block )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: from_pe
      logical(8), intent(out) :: get_data
      logical, intent(in), optional :: block

      integer, optional, intent(in) :: glen
      integer                       :: get_len
      logical(8) :: get_data1D(1)
      logical(8) :: dummy(1)

      pointer( ptr, get_data1D )
      ptr = LOC(get_data)
      get_len=1; if(PRESENT(glen))get_len=glen
      call mpp_transmit( dummy, 1, NULL_PE, get_data1D, get_len, from_pe , block)

    end subroutine mpp_recv_logical8_scalar

    subroutine mpp_send_logical8_scalar( put_data, to_pe, plen)
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: to_pe
      logical(8), intent(in) :: put_data
      integer, optional, intent(in) :: plen
      integer                       :: put_len
      logical(8) :: put_data1D(1)
      logical(8) :: dummy(1)

      pointer( ptr, put_data1D )
      ptr = LOC(put_data)
      put_len=1; if(PRESENT(plen))put_len=plen
      call mpp_transmit( put_data1D, put_len, to_pe, dummy, 1, NULL_PE )

    end subroutine mpp_send_logical8_scalar

    subroutine mpp_recv_logical8_2d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      logical(8), intent(out) :: get_data(:,:)
      logical(8) :: dummy(1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_logical8_2d

    subroutine mpp_send_logical8_2d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      logical(8), intent(in) :: put_data(:,:)
      logical(8) :: dummy(1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_logical8_2d

    subroutine mpp_recv_logical8_3d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      logical(8), intent(out) :: get_data(:,:,:)
      logical(8) :: dummy(1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_logical8_3d

    subroutine mpp_send_logical8_3d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      logical(8), intent(in) :: put_data(:,:,:)
      logical(8) :: dummy(1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_logical8_3d

    subroutine mpp_recv_logical8_4d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      logical(8), intent(out) :: get_data(:,:,:,:)
      logical(8) :: dummy(1,1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_logical8_4d

    subroutine mpp_send_logical8_4d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      logical(8), intent(in) :: put_data(:,:,:,:)
      logical(8) :: dummy(1,1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_logical8_4d

    subroutine mpp_recv_logical8_5d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      logical(8), intent(out) :: get_data(:,:,:,:,:)
      logical(8) :: dummy(1,1,1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_logical8_5d

    subroutine mpp_send_logical8_5d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      logical(8), intent(in) :: put_data(:,:,:,:,:)
      logical(8) :: dummy(1,1,1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_logical8_5d
    
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                MPP_BROADCAST                                !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_broadcast_logical8_scalar( data, from_pe, pelist )
      logical(8), intent(inout) :: data
      integer, intent(in) :: from_pe
      integer, intent(in), optional :: pelist(:)
      logical(8) :: data1D(1)

      pointer( ptr, data1D )

      ptr = LOC(data)
      call mpp_broadcast_logical8( data1D, 1, from_pe, pelist )

      return
    end subroutine mpp_broadcast_logical8_scalar
    
    subroutine mpp_broadcast_logical8_2d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      logical(8), intent(inout) :: data(:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      logical(8) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_logical8_2d
    
    subroutine mpp_broadcast_logical8_3d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      logical(8), intent(inout) :: data(:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      logical(8) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
   end subroutine mpp_broadcast_logical8_3d
    
    subroutine mpp_broadcast_logical8_4d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      logical(8), intent(inout) :: data(:,:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      logical(8) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_logical8_4d
    
    subroutine mpp_broadcast_logical8_5d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      logical(8), intent(inout) :: data(:,:,:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      logical(8) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_logical8_5d
























































!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                  MPP_TRANSMIT                               !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_transmit_logical4( put_data, put_len, to_pe, get_data, get_len, from_pe, block )
!a message-passing routine intended to be reminiscent equally of both MPI and SHMEM

!put_data and get_data are contiguous logical(4) arrays

!at each call, your put_data array is put to   to_pe's get_data
!              your get_data array is got from from_pe's put_data
!i.e we assume that typically (e.g updating halo regions) each PE performs a put _and_ a get

!special PE designations:
!      NULL_PE: to disable a put or a get (e.g at boundaries)
!      ANY_PE:  if remote PE for the put or get is to be unspecific
!      ALL_PES: broadcast and collect operations (collect not yet implemented)

!ideally we would not pass length, but this f77-style call performs better (arrays passed by address, not descriptor)
!further, this permits <length> contiguous words from an array of any rank to be passed (avoiding f90 rank conformance check)

!caller is responsible for completion checks (mpp_sync_self) before and after

      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      logical(4), intent(in)  :: put_data(*)
      logical(4), intent(out) :: get_data(*)
      logical, intent(in), optional :: block
      logical                       :: block_comm
      integer                       :: i, out_unit, n !<-- esm insertion
      integer, pointer 							:: irequest(:),irequest_recv(:)
      logical(4), allocatable, save :: local_data(:) !local copy used by non-parallel code (no SHMEM or MPI)

!--> esm insertion
      n=get_peset(); if( peset(n)%count.EQ.1 )return
      if(n.gt.ocean_peset_num)n=ocean_peset_num

      if(associated(irequest)) then
				nullify(irequest)
        nullify(irequest_recv)
      endif
      if(n.eq.world_peset_num) then
				irequest => request_global
				irequest_recv => request_recv_global
      else
				irequest => request
        irequest_recv => request_recv
      endif
!<-- esm insertion

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_TRANSMIT: You must first call mpp_init.' )
      if( to_pe.EQ.NULL_PE .AND. from_pe.EQ.NULL_PE )return
      
      block_comm = .true.
      if(PRESENT(block)) block_comm = block

      out_unit = stdout()
      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_TRANSMIT begin: to_pe, from_pe, put_len, get_len=', to_pe, from_pe, put_len, get_len
      end if

!do put first and then get
!      if( to_pe.GE.0 .AND. to_pe.LT.npes )then
!use non-blocking sends
      if( to_pe.GE.0 .AND. to_pe.LT.peset(n)%count )then !<-- esm insertion
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
          if( irequest(to_pe).NE.MPI_REQUEST_NULL )then !only one message from pe->to_pe in queue
!              if( debug )write( stderr(),* )'PE waiting for sending', pe, to_pe
              call MPI_WAIT( irequest(to_pe), stat, error )
          end if
!          call MPI_ISEND( put_data, put_len, MPI_INTEGER4, to_pe, tag, mpp_comm_private, request(to_pe), error )
          call MPI_ISEND( put_data, put_len, MPI_INTEGER4, to_pe, tag, peset(n)%id, irequest(to_pe), error ) !<-- esm insertion
          if( current_clock.NE.0 )call increment_current_clock( EVENT_SEND, put_len*4 )
      else if( to_pe.EQ.ALL_PES )then !this is a broadcast from from_pe
          if( from_pe.LT.0 .OR. from_pe.GE.npes )call mpp_error( FATAL, 'MPP_TRANSMIT: broadcasting from invalid PE.' )
          if( put_len.GT.get_len )call mpp_error( FATAL, 'MPP_TRANSMIT: size mismatch between put_data and get_data.' )
          if( pe.EQ.from_pe )then
              if( LOC(get_data).NE.LOC(put_data) )then
!dir$ IVDEP
                  do i = 1,get_len
                     get_data(i) = put_data(i)
                  end do
              end if
          end if
          call mpp_broadcast( get_data, get_len, from_pe )
          return
      else if( to_pe.EQ.ANY_PE )then !we don't have a destination to do puts to, so only do gets
!...but you cannot have a pure get with MPI
          call mpp_error( FATAL, 'MPP_TRANSMIT: you cannot transmit to ANY_PE using MPI.' )
      else if( to_pe.NE.NULL_PE )then  !no other valid cases except NULL_PE
          call mpp_error( FATAL, 'MPP_TRANSMIT: invalid to_pe.' )
      end if

!do the get: for libSMA, a get means do a wait to ensure put on remote PE is complete
!      if( from_pe.GE.0 .AND. from_pe.LT.npes )then
!receive from from_pe
      if( from_pe.GE.0 .AND. from_pe.LT.peset(n)%count  )then !<-- esm insertion
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
          if( block_comm ) then
!             call MPI_RECV( get_data, get_len, MPI_INTEGER4, from_pe, MPI_ANY_TAG, mpp_comm_private, stat, error )
             call MPI_RECV( get_data, get_len, MPI_INTEGER4, from_pe, MPI_ANY_TAG,peset(n)%id , stat, error) !<-- esm insertion
          else
             if( irequest_recv(from_pe).NE.MPI_REQUEST_NULL )then !only one message from from_pe->pe in queue
!              if( debug )write( stderr(),* )'PE waiting for receiving', pe, from_pe
                call MPI_WAIT( irequest_recv(from_pe), stat, error )
             end if
!             call MPI_IRECV( get_data, get_len, MPI_INTEGER4, from_pe, MPI_ANY_TAG, mpp_comm_private, &
             call MPI_IRECV( get_data, get_len, MPI_INTEGER4, from_pe, MPI_ANY_TAG, peset(n)%id, &
                  irequest_recv(from_pe), error )!<-- esm insertion
          endif
          if( current_clock.NE.0 )call increment_current_clock( EVENT_RECV, get_len*4 )
      else if( from_pe.EQ.ANY_PE )then
!receive from MPI_ANY_SOURCE
          if( current_clock.NE.0 )call system_clock_mpi(start_tick)
!          call MPI_RECV( get_data, get_len, MPI_INTEGER4, MPI_ANY_SOURCE, MPI_ANY_TAG, mpp_comm_private, stat, error )
          call MPI_RECV( get_data, get_len, MPI_INTEGER4, MPI_ANY_SOURCE, MPI_ANY_TAG, peset(n)%id, stat, error )!<-- esm insertion
          if( current_clock.NE.0 )call increment_current_clock( EVENT_RECV, get_len*4 )
      else if( from_pe.EQ.ALL_PES )then
          call mpp_error( FATAL, 'MPP_TRANSMIT: from_pe=ALL_PES has ambiguous meaning, and hence is not implemented.' )
      else if( from_pe.NE.NULL_PE )then !only remaining valid choice is NULL_PE
          call mpp_error( FATAL, 'MPP_TRANSMIT: invalid from_pe.' )
      end if

      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_TRANSMIT end: to_pe, from_pe, put_len, get_len=', to_pe, from_pe, put_len, get_len
      end if
      return
    end subroutine mpp_transmit_logical4

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                MPP_BROADCAST                                !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_broadcast_logical4( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      logical(4), intent(inout) :: data(*)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      integer :: n, i, from_rank, out_unit

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_BROADCAST: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      out_unit = stdout()
      if( debug )then
          call system_clock_mpi(tick)
          write( out_unit,'(a,i18,a,i5,a,2i5,2i8)' )&
               'T=',tick, ' PE=',pe, ' MPP_BROADCAST begin: from_pe, length=', from_pe, length
      end if

      if( .NOT.ANY(from_pe.EQ.peset(current_peset_num)%list) ) &
           call mpp_error( FATAL, 'MPP_BROADCAST: broadcasting from invalid PE.' )

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
! find the rank of from_pe in the pelist.
      do i = 1, mpp_npes()
         if(peset(n)%list(i) == from_pe) then
             from_rank = i - 1
             exit
         endif
      enddo
      if( mpp_npes().GT.1 )call MPI_BCAST( data, length, MPI_INTEGER4, from_rank, peset(n)%id, error )
      if( current_clock.NE.0 )call increment_current_clock( EVENT_BROADCAST, length*4 )
      return
    end subroutine mpp_broadcast_logical4

!####################################################################################
! -*-f90-*-
! $Id: mpp_transmit.inc,v 13.0.34.1.6.1 2009/07/16 18:25:06 z1l Exp $

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                  MPP_TRANSMIT                               !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_transmit_logical4_scalar( put_data, to_pe, get_data, from_pe, plen, glen)
      integer, intent(in) :: to_pe, from_pe
      logical(4), intent(in)  :: put_data
      logical(4), intent(out) :: get_data
      integer, optional, intent(in) :: plen, glen
      integer                       :: put_len, get_len
      logical(4) :: put_data1D(1), get_data1D(1)
      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )

      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      put_len=1; if(PRESENT(plen))put_len=plen
      get_len=1; if(PRESENT(glen))get_len=glen
      call mpp_transmit_logical4 ( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_logical4_scalar

    subroutine mpp_transmit_logical4_2d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      logical(4), intent(in)  :: put_data(:,:)
      logical(4), intent(out) :: get_data(:,:)
      logical(4) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_logical4_2d

    subroutine mpp_transmit_logical4_3d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      logical(4), intent(in)  :: put_data(:,:,:)
      logical(4), intent(out) :: get_data(:,:,:)
      logical(4) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_logical4_3d

    subroutine mpp_transmit_logical4_4d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      logical(4), intent(in)  :: put_data(:,:,:,:)
      logical(4), intent(out) :: get_data(:,:,:,:)
      logical(4) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_logical4_4d

    subroutine mpp_transmit_logical4_5d( put_data, put_len, to_pe, get_data, get_len, from_pe )
      integer, intent(in) :: put_len, to_pe, get_len, from_pe
      logical(4), intent(in)  :: put_data(:,:,:,:,:)
      logical(4), intent(out) :: get_data(:,:,:,:,:)
      logical(4) :: put_data1D(put_len), get_data1D(get_len)

      pointer( ptrp, put_data1D )
      pointer( ptrg, get_data1D )
      ptrp = LOC(put_data)
      ptrg = LOC(get_data)
      call mpp_transmit( put_data1D, put_len, to_pe, get_data1D, get_len, from_pe )

      return
    end subroutine mpp_transmit_logical4_5d
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                              MPP_SEND and RECV                              !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_recv_logical4( get_data, get_len, from_pe, block )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      logical(4), intent(out) :: get_data(*)
      logical, intent(in), optional :: block

      logical(4) :: dummy(1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe, block )
    end subroutine mpp_recv_logical4

    subroutine mpp_send_logical4( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      logical(4), intent(in) :: put_data(*)
      logical(4) :: dummy(1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_logical4

    subroutine mpp_recv_logical4_scalar( get_data, from_pe, glen, block )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: from_pe
      logical(4), intent(out) :: get_data
      logical, intent(in), optional :: block

      integer, optional, intent(in) :: glen
      integer                       :: get_len
      logical(4) :: get_data1D(1)
      logical(4) :: dummy(1)

      pointer( ptr, get_data1D )
      ptr = LOC(get_data)
      get_len=1; if(PRESENT(glen))get_len=glen
      call mpp_transmit( dummy, 1, NULL_PE, get_data1D, get_len, from_pe , block)

    end subroutine mpp_recv_logical4_scalar

    subroutine mpp_send_logical4_scalar( put_data, to_pe, plen)
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: to_pe
      logical(4), intent(in) :: put_data
      integer, optional, intent(in) :: plen
      integer                       :: put_len
      logical(4) :: put_data1D(1)
      logical(4) :: dummy(1)

      pointer( ptr, put_data1D )
      ptr = LOC(put_data)
      put_len=1; if(PRESENT(plen))put_len=plen
      call mpp_transmit( put_data1D, put_len, to_pe, dummy, 1, NULL_PE )

    end subroutine mpp_send_logical4_scalar

    subroutine mpp_recv_logical4_2d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      logical(4), intent(out) :: get_data(:,:)
      logical(4) :: dummy(1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_logical4_2d

    subroutine mpp_send_logical4_2d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      logical(4), intent(in) :: put_data(:,:)
      logical(4) :: dummy(1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_logical4_2d

    subroutine mpp_recv_logical4_3d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      logical(4), intent(out) :: get_data(:,:,:)
      logical(4) :: dummy(1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_logical4_3d

    subroutine mpp_send_logical4_3d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      logical(4), intent(in) :: put_data(:,:,:)
      logical(4) :: dummy(1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_logical4_3d

    subroutine mpp_recv_logical4_4d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      logical(4), intent(out) :: get_data(:,:,:,:)
      logical(4) :: dummy(1,1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_logical4_4d

    subroutine mpp_send_logical4_4d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      logical(4), intent(in) :: put_data(:,:,:,:)
      logical(4) :: dummy(1,1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_logical4_4d

    subroutine mpp_recv_logical4_5d( get_data, get_len, from_pe )
!a mpp_transmit with null arguments on the put side
      integer, intent(in) :: get_len, from_pe
      logical(4), intent(out) :: get_data(:,:,:,:,:)
      logical(4) :: dummy(1,1,1,1,1)
      call mpp_transmit( dummy, 1, NULL_PE, get_data, get_len, from_pe )
    end subroutine mpp_recv_logical4_5d

    subroutine mpp_send_logical4_5d( put_data, put_len, to_pe )
!a mpp_transmit with null arguments on the get side
      integer, intent(in) :: put_len, to_pe
      logical(4), intent(in) :: put_data(:,:,:,:,:)
      logical(4) :: dummy(1,1,1,1,1)
      call mpp_transmit( put_data, put_len, to_pe, dummy, 1, NULL_PE )
    end subroutine mpp_send_logical4_5d
    
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!                                MPP_BROADCAST                                !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

    subroutine mpp_broadcast_logical4_scalar( data, from_pe, pelist )
      logical(4), intent(inout) :: data
      integer, intent(in) :: from_pe
      integer, intent(in), optional :: pelist(:)
      logical(4) :: data1D(1)

      pointer( ptr, data1D )

      ptr = LOC(data)
      call mpp_broadcast_logical4( data1D, 1, from_pe, pelist )

      return
    end subroutine mpp_broadcast_logical4_scalar
    
    subroutine mpp_broadcast_logical4_2d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      logical(4), intent(inout) :: data(:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      logical(4) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_logical4_2d
    
    subroutine mpp_broadcast_logical4_3d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      logical(4), intent(inout) :: data(:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      logical(4) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
   end subroutine mpp_broadcast_logical4_3d
    
    subroutine mpp_broadcast_logical4_4d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      logical(4), intent(inout) :: data(:,:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      logical(4) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_logical4_4d
    
    subroutine mpp_broadcast_logical4_5d( data, length, from_pe, pelist )
!this call was originally bundled in with mpp_transmit, but that doesn't allow
!broadcast to a subset of PEs. This version will, and mpp_transmit will remain
!backward compatible.
      logical(4), intent(inout) :: data(:,:,:,:,:)
      integer, intent(in) :: length, from_pe
      integer, intent(in), optional :: pelist(:)
      logical(4) :: data1D(length)

      pointer( ptr, data1D )
      ptr = LOC(data)
      call mpp_broadcast( data1D, length, from_pe, pelist )

      return
    end subroutine mpp_broadcast_logical4_5d

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!                                                                             !
!            GLOBAL REDUCTION ROUTINES: mpp_max, mpp_sum, mpp_min             !
!                                                                             !
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!










    subroutine mpp_max_real8( a, pelist )
!find the max of scalar a the PEs in pelist (all PEs if this argument is omitted)
!result is also automatically broadcast to all PEs
      real(8), intent(inout) :: a
      integer, intent(in), optional :: pelist(0:)
      integer :: n
      real(8) :: work

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_REDUCE: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
      if( verbose )call mpp_error( NOTE, 'MPP_REDUCE_: using MPI_ALLREDUCE...' )
      call MPI_ALLREDUCE( a, work, 1, MPI_REAL8, MPI_MAX, peset(n)%id, error )
      a = work
      if( current_clock.NE.0 )call increment_current_clock( EVENT_ALLREDUCE, 8 )
      return
    end subroutine mpp_max_real8














    subroutine mpp_max_int8( a, pelist )
!find the max of scalar a the PEs in pelist (all PEs if this argument is omitted)
!result is also automatically broadcast to all PEs
      integer(8), intent(inout) :: a
      integer, intent(in), optional :: pelist(0:)
      integer :: n
      integer(8) :: work

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_REDUCE: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
      if( verbose )call mpp_error( NOTE, 'MPP_REDUCE_: using MPI_ALLREDUCE...' )
      call MPI_ALLREDUCE( a, work, 1, MPI_INTEGER8, MPI_MAX, peset(n)%id, error )
      a = work
      if( current_clock.NE.0 )call increment_current_clock( EVENT_ALLREDUCE, 8 )
      return
    end subroutine mpp_max_int8












    subroutine mpp_max_int4( a, pelist )
!find the max of scalar a the PEs in pelist (all PEs if this argument is omitted)
!result is also automatically broadcast to all PEs
      integer(4), intent(inout) :: a
      integer, intent(in), optional :: pelist(0:)
      integer :: n
      integer(4) :: work

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_REDUCE: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
      if( verbose )call mpp_error( NOTE, 'MPP_REDUCE_: using MPI_ALLREDUCE...' )
      call MPI_ALLREDUCE( a, work, 1, MPI_INTEGER4, MPI_MAX, peset(n)%id, error )
      a = work
      if( current_clock.NE.0 )call increment_current_clock( EVENT_ALLREDUCE, 4 )
      return
    end subroutine mpp_max_int4











    subroutine mpp_min_real8( a, pelist )
!find the max of scalar a the PEs in pelist (all PEs if this argument is omitted)
!result is also automatically broadcast to all PEs
      real(8), intent(inout) :: a
      integer, intent(in), optional :: pelist(0:)
      integer :: n
      real(8) :: work

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_REDUCE: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
      if( verbose )call mpp_error( NOTE, 'MPP_REDUCE_: using MPI_ALLREDUCE...' )
      call MPI_ALLREDUCE( a, work, 1, MPI_REAL8, MPI_MIN, peset(n)%id, error )
      a = work
      if( current_clock.NE.0 )call increment_current_clock( EVENT_ALLREDUCE, 8 )
      return
    end subroutine mpp_min_real8














    subroutine mpp_min_int8( a, pelist )
!find the max of scalar a the PEs in pelist (all PEs if this argument is omitted)
!result is also automatically broadcast to all PEs
      integer(8), intent(inout) :: a
      integer, intent(in), optional :: pelist(0:)
      integer :: n
      integer(8) :: work

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_REDUCE: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
      if( verbose )call mpp_error( NOTE, 'MPP_REDUCE_: using MPI_ALLREDUCE...' )
      call MPI_ALLREDUCE( a, work, 1, MPI_INTEGER8, MPI_MIN, peset(n)%id, error )
      a = work
      if( current_clock.NE.0 )call increment_current_clock( EVENT_ALLREDUCE, 8 )
      return
    end subroutine mpp_min_int8

             










    subroutine mpp_min_int4( a, pelist )
!find the max of scalar a the PEs in pelist (all PEs if this argument is omitted)
!result is also automatically broadcast to all PEs
      integer(4), intent(inout) :: a
      integer, intent(in), optional :: pelist(0:)
      integer :: n
      integer(4) :: work

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_REDUCE: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
      if( verbose )call mpp_error( NOTE, 'MPP_REDUCE_: using MPI_ALLREDUCE...' )
      call MPI_ALLREDUCE( a, work, 1, MPI_INTEGER4, MPI_MIN, peset(n)%id, error )
      a = work
      if( current_clock.NE.0 )call increment_current_clock( EVENT_ALLREDUCE, 4 )
      return
    end subroutine mpp_min_int4



















    subroutine mpp_sum_real8( a, length, pelist )
!sums array a over the PEs in pelist (all PEs if this argument is omitted)
!result is also automatically broadcast: all PEs have the sum in a at the end
!we are using f77-style call: array passed by address and not descriptor; further,
!the f90 conformance check is avoided.
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      real(8), intent(inout) :: a(*)
      integer :: n
      real(8) :: work(length)

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_SUM: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
      if( verbose )call mpp_error( NOTE, 'MPP_SUM: using MPI_ALLREDUCE...' )
      if( debug )write( stderr(),* )'pe, n, peset(n)%id=', pe, n, peset(n)%id
      call MPI_ALLREDUCE( a, work, length, MPI_REAL8, MPI_SUM, peset(n)%id, error )
      a(1:length) = work(1:length)
      if( current_clock.NE.0 )call increment_current_clock( EVENT_ALLREDUCE, length*8 )
      return
    end subroutine mpp_sum_real8

!#######################################################################

! -*-f90-*-
! $Id: mpp_sum.inc,v 13.0.38.1 2009/07/16 18:25:06 z1l Exp $

!#######################################################################

    subroutine mpp_sum_real8_scalar( a, pelist )
!sums array a when only first element is passed: this routine just converts to a call to mpp_sum_real8
      real(8), intent(inout) :: a
      integer, intent(in), optional :: pelist(:)
      real(8) :: b(1)

      b(1) = a
      if( debug )call mpp_error( NOTE, 'MPP_SUM_SCALAR_: calling MPP_SUM_ ...' )
      call mpp_sum_real8( b, 1, pelist )
      a = b(1)
      return
    end subroutine mpp_sum_real8_scalar

!#######################################################################
    subroutine mpp_sum_real8_2d( a, length, pelist )
      real(8), intent(inout) :: a(:,:)
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      real(8) :: a1D(length)

      pointer( ptr, a1D )
      ptr = LOC(a)
      call mpp_sum( a1D, length, pelist )

      return
    end subroutine mpp_sum_real8_2d

!#######################################################################
    subroutine mpp_sum_real8_3d( a, length, pelist )
      real(8), intent(inout) :: a(:,:,:)
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      real(8) :: a1D(length)

      pointer( ptr, a1D )
      ptr = LOC(a)
      call mpp_sum( a1D, length, pelist )

      return
    end subroutine mpp_sum_real8_3d

!#######################################################################
    subroutine mpp_sum_real8_4d( a, length, pelist )
      real(8), intent(inout) :: a(:,:,:,:)
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      real(8) :: a1D(length)

      pointer( ptr, a1D )
      ptr = LOC(a)
      call mpp_sum( a1D, length, pelist )

      return
    end subroutine mpp_sum_real8_4d

!#######################################################################
    subroutine mpp_sum_real8_5d( a, length, pelist )
      real(8), intent(inout) :: a(:,:,:,:,:)
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      real(8) :: a1D(length)

      pointer( ptr, a1D )
      ptr = LOC(a)
      call mpp_sum( a1D, length, pelist )

      return
    end subroutine mpp_sum_real8_5d


























    subroutine mpp_sum_int8( a, length, pelist )
!sums array a over the PEs in pelist (all PEs if this argument is omitted)
!result is also automatically broadcast: all PEs have the sum in a at the end
!we are using f77-style call: array passed by address and not descriptor; further,
!the f90 conformance check is avoided.
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      integer(8), intent(inout) :: a(*)
      integer :: n
      integer(8) :: work(length)

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_SUM: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
      if( verbose )call mpp_error( NOTE, 'MPP_SUM: using MPI_ALLREDUCE...' )
      if( debug )write( stderr(),* )'pe, n, peset(n)%id=', pe, n, peset(n)%id
      call MPI_ALLREDUCE( a, work, length, MPI_INTEGER8, MPI_SUM, peset(n)%id, error )
      a(1:length) = work(1:length)
      if( current_clock.NE.0 )call increment_current_clock( EVENT_ALLREDUCE, length*8 )
      return
    end subroutine mpp_sum_int8

!#######################################################################

! -*-f90-*-
! $Id: mpp_sum.inc,v 13.0.38.1 2009/07/16 18:25:06 z1l Exp $

!#######################################################################

    subroutine mpp_sum_int8_scalar( a, pelist )
!sums array a when only first element is passed: this routine just converts to a call to mpp_sum_int8
      integer(8), intent(inout) :: a
      integer, intent(in), optional :: pelist(:)
      integer(8) :: b(1)

      b(1) = a
      if( debug )call mpp_error( NOTE, 'MPP_SUM_SCALAR_: calling MPP_SUM_ ...' )
      call mpp_sum_int8( b, 1, pelist )
      a = b(1)
      return
    end subroutine mpp_sum_int8_scalar

!#######################################################################
    subroutine mpp_sum_int8_2d( a, length, pelist )
      integer(8), intent(inout) :: a(:,:)
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      integer(8) :: a1D(length)

      pointer( ptr, a1D )
      ptr = LOC(a)
      call mpp_sum( a1D, length, pelist )

      return
    end subroutine mpp_sum_int8_2d

!#######################################################################
    subroutine mpp_sum_int8_3d( a, length, pelist )
      integer(8), intent(inout) :: a(:,:,:)
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      integer(8) :: a1D(length)

      pointer( ptr, a1D )
      ptr = LOC(a)
      call mpp_sum( a1D, length, pelist )

      return
    end subroutine mpp_sum_int8_3d

!#######################################################################
    subroutine mpp_sum_int8_4d( a, length, pelist )
      integer(8), intent(inout) :: a(:,:,:,:)
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      integer(8) :: a1D(length)

      pointer( ptr, a1D )
      ptr = LOC(a)
      call mpp_sum( a1D, length, pelist )

      return
    end subroutine mpp_sum_int8_4d

!#######################################################################
    subroutine mpp_sum_int8_5d( a, length, pelist )
      integer(8), intent(inout) :: a(:,:,:,:,:)
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      integer(8) :: a1D(length)

      pointer( ptr, a1D )
      ptr = LOC(a)
      call mpp_sum( a1D, length, pelist )

      return
    end subroutine mpp_sum_int8_5d




















    subroutine mpp_sum_int4( a, length, pelist )
!sums array a over the PEs in pelist (all PEs if this argument is omitted)
!result is also automatically broadcast: all PEs have the sum in a at the end
!we are using f77-style call: array passed by address and not descriptor; further,
!the f90 conformance check is avoided.
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      integer(4), intent(inout) :: a(*)
      integer :: n
      integer(4) :: work(length)

      if( .NOT.module_is_initialized )call mpp_error( FATAL, 'MPP_SUM: You must first call mpp_init.' )
      n = get_peset(pelist); if( peset(n)%count.EQ.1 )return

      if( current_clock.NE.0 )call system_clock_mpi(start_tick)
      if( verbose )call mpp_error( NOTE, 'MPP_SUM: using MPI_ALLREDUCE...' )
      if( debug )write( stderr(),* )'pe, n, peset(n)%id=', pe, n, peset(n)%id
      call MPI_ALLREDUCE( a, work, length, MPI_INTEGER4, MPI_SUM, peset(n)%id, error )
      a(1:length) = work(1:length)
      if( current_clock.NE.0 )call increment_current_clock( EVENT_ALLREDUCE, length*4 )
      return
    end subroutine mpp_sum_int4

!#######################################################################

! -*-f90-*-
! $Id: mpp_sum.inc,v 13.0.38.1 2009/07/16 18:25:06 z1l Exp $

!#######################################################################

    subroutine mpp_sum_int4_scalar( a, pelist )
!sums array a when only first element is passed: this routine just converts to a call to mpp_sum_int4
      integer(4), intent(inout) :: a
      integer, intent(in), optional :: pelist(:)
      integer(4) :: b(1)

      b(1) = a
      if( debug )call mpp_error( NOTE, 'MPP_SUM_SCALAR_: calling MPP_SUM_ ...' )
      call mpp_sum_int4( b, 1, pelist )
      a = b(1)
      return
    end subroutine mpp_sum_int4_scalar

!#######################################################################
    subroutine mpp_sum_int4_2d( a, length, pelist )
      integer(4), intent(inout) :: a(:,:)
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      integer(4) :: a1D(length)

      pointer( ptr, a1D )
      ptr = LOC(a)
      call mpp_sum( a1D, length, pelist )

      return
    end subroutine mpp_sum_int4_2d

!#######################################################################
    subroutine mpp_sum_int4_3d( a, length, pelist )
      integer(4), intent(inout) :: a(:,:,:)
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      integer(4) :: a1D(length)

      pointer( ptr, a1D )
      ptr = LOC(a)
      call mpp_sum( a1D, length, pelist )

      return
    end subroutine mpp_sum_int4_3d

!#######################################################################
    subroutine mpp_sum_int4_4d( a, length, pelist )
      integer(4), intent(inout) :: a(:,:,:,:)
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      integer(4) :: a1D(length)

      pointer( ptr, a1D )
      ptr = LOC(a)
      call mpp_sum( a1D, length, pelist )

      return
    end subroutine mpp_sum_int4_4d

!#######################################################################
    subroutine mpp_sum_int4_5d( a, length, pelist )
      integer(4), intent(inout) :: a(:,:,:,:,:)
      integer, intent(in) :: length
      integer, intent(in), optional :: pelist(:)
      integer(4) :: a1D(length)

      pointer( ptr, a1D )
      ptr = LOC(a)
      call mpp_sum( a1D, length, pelist )

      return
    end subroutine mpp_sum_int4_5d









    function mpp_chksum_i8_1d( var, pelist )
      integer(8) :: mpp_chksum_i8_1d
      integer(8), intent(in) :: var (:)
      integer, optional :: pelist(:)
      mpp_chksum_i8_1d = sum(var)
      call mpp_sum( mpp_chksum_i8_1d, pelist )
      return
    end function mpp_chksum_i8_1d







    function mpp_chksum_i8_2d( var, pelist )
      integer(8) :: mpp_chksum_i8_2d
      integer(8), intent(in) :: var (:,:)
      integer, optional :: pelist(:)
      mpp_chksum_i8_2d = sum(var)
      call mpp_sum( mpp_chksum_i8_2d, pelist )
      return
    end function mpp_chksum_i8_2d







    function mpp_chksum_i8_3d( var, pelist )
      integer(8) :: mpp_chksum_i8_3d
      integer(8), intent(in) :: var (:,:,:)
      integer, optional :: pelist(:)
      mpp_chksum_i8_3d = sum(var)
      call mpp_sum( mpp_chksum_i8_3d, pelist )
      return
    end function mpp_chksum_i8_3d







    function mpp_chksum_i8_4d( var, pelist )
      integer(8) :: mpp_chksum_i8_4d
      integer(8), intent(in) :: var (:,:,:,:)
      integer, optional :: pelist(:)
      mpp_chksum_i8_4d = sum(var)
      call mpp_sum( mpp_chksum_i8_4d, pelist )
      return
    end function mpp_chksum_i8_4d







    function mpp_chksum_i8_5d( var, pelist )
      integer(8) :: mpp_chksum_i8_5d
      integer(8), intent(in) :: var (:,:,:,:,:)
      integer, optional :: pelist(:)
      mpp_chksum_i8_5d = sum(var)
      call mpp_sum( mpp_chksum_i8_5d, pelist )
      return
    end function mpp_chksum_i8_5d








    function mpp_chksum_i4_1d( var, pelist )
      integer(4) :: mpp_chksum_i4_1d
      integer(4), intent(in) :: var (:)
      integer, optional :: pelist(:)
      mpp_chksum_i4_1d = sum(var)
      call mpp_sum( mpp_chksum_i4_1d, pelist )
      return
    end function mpp_chksum_i4_1d







    function mpp_chksum_i4_2d( var, pelist )
      integer(4) :: mpp_chksum_i4_2d
      integer(4), intent(in) :: var (:,:)
      integer, optional :: pelist(:)
      mpp_chksum_i4_2d = sum(var)
      call mpp_sum( mpp_chksum_i4_2d, pelist )
      return
    end function mpp_chksum_i4_2d







    function mpp_chksum_i4_3d( var, pelist )
      integer(4) :: mpp_chksum_i4_3d
      integer(4), intent(in) :: var (:,:,:)
      integer, optional :: pelist(:)
      mpp_chksum_i4_3d = sum(var)
      call mpp_sum( mpp_chksum_i4_3d, pelist )
      return
    end function mpp_chksum_i4_3d







    function mpp_chksum_i4_4d( var, pelist )
      integer(4) :: mpp_chksum_i4_4d
      integer(4), intent(in) :: var (:,:,:,:)
      integer, optional :: pelist(:)
      mpp_chksum_i4_4d = sum(var)
      call mpp_sum( mpp_chksum_i4_4d, pelist )
      return
    end function mpp_chksum_i4_4d







    function mpp_chksum_i4_5d( var, pelist )
      integer(4) :: mpp_chksum_i4_5d
      integer(4), intent(in) :: var (:,:,:,:,:)
      integer, optional :: pelist(:)
      mpp_chksum_i4_5d = sum(var)
      call mpp_sum( mpp_chksum_i4_5d, pelist )
      return
    end function mpp_chksum_i4_5d







    function mpp_chksum_r8_0d( var, pelist )
!mold is a dummy array to be used by TRANSFER()
!must be same TYPE as result
!result is 8, which will actually be int ifdef no_8byte_integers
      integer(8) :: mpp_chksum_r8_0d
      real(8), intent(in) :: var
      integer, intent(in), optional :: pelist(:)
      integer(8) :: mold(1)
      pointer( p, mold )

      p = LOC(var)
      mpp_chksum_r8_0d = mpp_chksum( mold, pelist )
      return
    end function mpp_chksum_r8_0d







    function mpp_chksum_r8_1d( var, pelist )
!mold is a dummy array to be used by TRANSFER()
!must be same TYPE as result
!result is 8, which will actually be int ifdef no_8byte_integers
      integer(8) :: mpp_chksum_r8_1d, mold(1)
      real(8), intent(in) :: var (:)
      integer, intent(in), optional :: pelist(:)

      mpp_chksum_r8_1d = mpp_chksum( TRANSFER(var,mold), pelist )
      return
    end function mpp_chksum_r8_1d







    function mpp_chksum_r8_2d( var, pelist )
!mold is a dummy array to be used by TRANSFER()
!must be same TYPE as result
!result is 8, which will actually be int ifdef no_8byte_integers
      integer(8) :: mpp_chksum_r8_2d, mold(1)
      real(8), intent(in) :: var (:,:)
      integer, intent(in), optional :: pelist(:)

      mpp_chksum_r8_2d = mpp_chksum( TRANSFER(var,mold), pelist )
      return
    end function mpp_chksum_r8_2d







    function mpp_chksum_r8_3d( var, pelist )
!mold is a dummy array to be used by TRANSFER()
!must be same TYPE as result
!result is 8, which will actually be int ifdef no_8byte_integers
      integer(8) :: mpp_chksum_r8_3d, mold(1)
      real(8), intent(in) :: var (:,:,:)
      integer, intent(in), optional :: pelist(:)

      mpp_chksum_r8_3d = mpp_chksum( TRANSFER(var,mold), pelist )
      return
    end function mpp_chksum_r8_3d







    function mpp_chksum_r8_4d( var, pelist )
!mold is a dummy array to be used by TRANSFER()
!must be same TYPE as result
!result is 8, which will actually be int ifdef no_8byte_integers
      integer(8) :: mpp_chksum_r8_4d, mold(1)
      real(8), intent(in) :: var (:,:,:,:)
      integer, intent(in), optional :: pelist(:)

      mpp_chksum_r8_4d = mpp_chksum( TRANSFER(var,mold), pelist )
      return
    end function mpp_chksum_r8_4d







    function mpp_chksum_r8_5d( var, pelist )
!mold is a dummy array to be used by TRANSFER()
!must be same TYPE as result
!result is 8, which will actually be int ifdef no_8byte_integers
      integer(8) :: mpp_chksum_r8_5d, mold(1)
      real(8), intent(in) :: var (:,:,:,:,:)
      integer, intent(in), optional :: pelist(:)

      mpp_chksum_r8_5d = mpp_chksum( TRANSFER(var,mold), pelist )
      return
    end function mpp_chksum_r8_5d









  end module mpp_mod




